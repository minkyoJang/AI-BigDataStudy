{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/getChan/data_campus/blob/master/NLP/tensorflow_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNbZ2ou_oKUI",
        "colab_type": "text"
      },
      "source": [
        "# Tensorflow Concepts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLCTUvCun5e-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1hlFcpaoe99",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "3f233e73-1dfa-461b-da9b-f100fa055b82"
      },
      "source": [
        "a = tf.placeholder(tf.float32, shape=[3,4])\n",
        "b = tf.placeholder(tf.float32, shape=[4,6])\n",
        "\n",
        "\n",
        "op = tf.matmul(a, b)\n",
        "\n",
        "a_mat = np.random.randn(3,4)\n",
        "b_mat = np.random.randn(4,6)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  print(sess.run(op, feed_dict={a:a_mat, b:b_mat}))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.5456903  -0.8009943  -0.6600466   0.27159917 -0.12338307 -0.71377426]\n",
            " [ 1.890008    1.060418    0.55130184 -0.46573135 -0.7733471  -0.49385256]\n",
            " [-1.0697331   3.002978    0.03745753 -1.3840796  -0.02584723  1.3906353 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvZo4CZLpZSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21d8cd31-b5a0-4909-fc10-2fab4f1739b2"
      },
      "source": [
        "\n",
        "\n",
        "W = tf.Variable([0.3], dtype=tf.float32)\n",
        "b = tf.Variable([-0.3], dtype=tf.float32)\n",
        "x = tf.placeholder(tf.float32)\n",
        "\n",
        "linear_model = x*W + b\n",
        "\n",
        "init_op = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init_op)\n",
        "  print(sess.run(linear_model, feed_dict={x:5.0}))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vglslBrlsU77",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDcIqiyRqbS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = [[1, 2],\n",
        "          [2, 3],\n",
        "          [3, 1],\n",
        "          [4, 3],\n",
        "          [5, 3],\n",
        "          [6, 2]]\n",
        "y_data = [[0],\n",
        "          [0],\n",
        "          [0],\n",
        "          [1],\n",
        "          [1],\n",
        "          [1]]\n",
        "\n",
        "x_test_data = [[1,1], [5,4], [2,1], [6,3]]\n",
        "y_test_data = [[0], [1], [0], [1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV3BNNjwsvIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# placeholders for a tensor that will be always fed.\n",
        "x = tf.placeholder(tf.float32, shape=[None, 2], name=\"x_ph\")\n",
        "y = tf.placeholder(tf.float32, shape=[None, 1], name=\"y_ph\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twzfimABswWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaSP2PFosxhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hypothesis using sigmoid\n",
        "hypothesis = tf.sigmoid(tf.matmul(x, W) + b)\n",
        "\n",
        "# cost/loss function\n",
        "cost = -tf.reduce_mean(y * tf.log(hypothesis) + (1 - y) *\n",
        "                       tf.log(1 - hypothesis))\n",
        "\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "# Accuracy computation\n",
        "# True if hypothesis>0.5 else False\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32) # boolean to float32\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y), dtype=tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUxjvFtxszVa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d58c4a43-a7b2-4dfc-a5f3-8b77095a24c6"
      },
      "source": [
        "# Launch graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for step in range(10001):\n",
        "        cost_val, _ = sess.run([cost, train], feed_dict={x: x_data, y: y_data})\n",
        "        if step % 200 == 0:\n",
        "            print(step, cost_val)\n",
        "\n",
        "    # Accuracy report\n",
        "    hypothesis_val, pred_val, acc_val = sess.run([hypothesis, predicted, accuracy],\n",
        "                       feed_dict={x: x_test_data, y: y_test_data})\n",
        "    print(\"\\nHypothesis: \", hypothesis_val, \"\\nCorrect : \", pred_val, \"\\nAccuracy: \", acc_val)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1.3485712\n",
            "200 0.58159083\n",
            "400 0.5103544\n",
            "600 0.47224438\n",
            "800 0.44703197\n",
            "1000 0.42762554\n",
            "1200 0.41122702\n",
            "1400 0.39661315\n",
            "1600 0.38320163\n",
            "1800 0.3706942\n",
            "2000 0.35892662\n",
            "2200 0.34780017\n",
            "2400 0.3372502\n",
            "2600 0.32722977\n",
            "2800 0.31770208\n",
            "3000 0.30863592\n",
            "3200 0.3000035\n",
            "3400 0.29177985\n",
            "3600 0.28394145\n",
            "3800 0.2764664\n",
            "4000 0.269334\n",
            "4200 0.26252502\n",
            "4400 0.25602093\n",
            "4600 0.24980448\n",
            "4800 0.24385971\n",
            "5000 0.23817115\n",
            "5200 0.23272456\n",
            "5400 0.22750641\n",
            "5600 0.22250421\n",
            "5800 0.21770601\n",
            "6000 0.21310067\n",
            "6200 0.20867777\n",
            "6400 0.20442764\n",
            "6600 0.20034109\n",
            "6800 0.19640946\n",
            "7000 0.19262482\n",
            "7200 0.18897943\n",
            "7400 0.18546633\n",
            "7600 0.18207884\n",
            "7800 0.17881067\n",
            "8000 0.17565607\n",
            "8200 0.17260928\n",
            "8400 0.16966529\n",
            "8600 0.16681916\n",
            "8800 0.16406633\n",
            "9000 0.16140242\n",
            "9200 0.15882336\n",
            "9400 0.15632527\n",
            "9600 0.15390454\n",
            "9800 0.15155776\n",
            "10000 0.14928168\n",
            "\n",
            "Hypothesis:  [[0.02255697]\n",
            " [0.9552605 ]\n",
            " [0.09132387]\n",
            " [0.985479  ]] \n",
            "Correct :  [[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]] \n",
            "Accuracy:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpDE0E5vzF99",
        "colab_type": "text"
      },
      "source": [
        "# MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzuf9suOs1j9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UPYDmR0zDA1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "c0af9f8a-09f3-4185-b489-a413654ac4ce"
      },
      "source": [
        "mnist = input_data.read_data_sets(\"./data\", one_hot=True)\n",
        "r = random.randint(0, mnist.train.num_examples - 1)\n",
        "plt.imshow(mnist.train.images[r:r+1].reshape(28, 28),\n",
        "           cmap='Greys', interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0731 02:11:45.268595 140584050575232 deprecation.py:323] From <ipython-input-37-7950cd9991ba>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0731 02:11:45.271201 140584050575232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0731 02:11:45.273234 140584050575232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "W0731 02:11:50.379818 140584050575232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting ./data/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0731 02:11:50.694501 140584050575232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0731 02:11:50.697831 140584050575232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "W0731 02:11:50.807501 140584050575232 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting ./data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZUzjyzqzEJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PLACEHOLDERS\n",
        "model_inputs = tf.placeholder(dtype=tf.float32, shape=[None, 784])\n",
        "labels = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n",
        "global_step = tf.Variable(0, name='global_step', trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyVd1u1ozIob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5481628f-e12b-4f39-af82-c4003fdd5d1f"
      },
      "source": [
        "# VARIABLES\n",
        "w = tf.Variable(tf.random_normal(shape=[784, 10]))\n",
        "b = tf.Variable(tf.random_normal(shape=[10]))\n",
        "\n",
        "tf.summary.histogram('weights', w)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'weights:0' shape=() dtype=string>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p2u6NgxzJ22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits = tf.matmul(model_inputs, w) + b\n",
        "predictions = tf.nn.softmax(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP4ueVH61rh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PREDICTION\n",
        "compare_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "# ACCURACY\n",
        "accuracy = tf.reduce_mean(tf.cast(compare_pred, \"float\"))\n",
        "# COST FUNCTION\n",
        "loss = tf.reduce_mean(-tf.reduce_sum(labels*tf.log(predictions), reduction_indices=1))\n",
        "# OPTIMIZER, TRAIN OPERATION\n",
        "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss, global_step=global_step)\n",
        "\n",
        "merged = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awP0m6-t6fj-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0cc65fb0-96f4-4e4c-af01-a42e53fd715d"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  writer = tf.summary.FileWriter('./logs/%s' % timestamp, sess.graph)\n",
        "  saver = tf.train.Saver()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  for i in range(50):\n",
        "      avg_loss = 0.\n",
        "      for step in range(10000):\n",
        "        batch_images, batch_labels = mnist.train.next_batch(100)\n",
        "        feeds_train = {model_inputs: batch_images, labels: batch_labels}\n",
        "        _, loss_val, global_step_val, summary_val = sess.run([train_op, loss, global_step, merged], feed_dict=feeds_train)\n",
        "        avg_loss += loss_val\n",
        "\n",
        "        if (step+1) % 1000 == 0:\n",
        "          print (\"step {} | loss : {}\".format(step+1, avg_loss/(step+1)))\n",
        "          writer.add_summary(summary_val, global_step=global_step_val)\n",
        "\n",
        "\n",
        "      feeds_test = {model_inputs: mnist.test.images, labels: mnist.test.labels}\n",
        "      train_acc = sess.run(accuracy, feed_dict=feeds_train)\n",
        "      test_acc = sess.run(accuracy, feed_dict=feeds_test)\n",
        "      print(\"idx: %02d/50 cost: %.3f train_acc: %.3f test_acc: %.3f\"\n",
        "            % (i+1, avg_loss/(step+1), train_acc, test_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 1000 | loss : 5.984980409383774\n",
            "step 2000 | loss : 4.453757867872715\n",
            "step 3000 | loss : 3.661552770058314\n",
            "step 4000 | loss : 3.1686693378984927\n",
            "step 5000 | loss : 2.8301997478961947\n",
            "step 6000 | loss : 2.578509818583727\n",
            "step 7000 | loss : 2.3845245963931085\n",
            "step 8000 | loss : 2.2294184336550535\n",
            "step 9000 | loss : 2.1012576860520573\n",
            "step 10000 | loss : 1.9939224125683308\n",
            "idx: 01/50 cost: 1.994 train_acc: 0.800 test_acc: 0.805\n",
            "step 1000 | loss : 0.9743457005023957\n",
            "step 2000 | loss : 0.9616040303111076\n",
            "step 3000 | loss : 0.943552336126566\n",
            "step 4000 | loss : 0.9285133202672005\n",
            "step 5000 | loss : 0.9140960409462452\n",
            "step 6000 | loss : 0.9005217824156085\n",
            "step 7000 | loss : 0.8882751457584755\n",
            "step 8000 | loss : 0.8758806449361145\n",
            "step 9000 | loss : 0.8655075064897537\n",
            "step 10000 | loss : 0.8551829385772347\n",
            "idx: 02/50 cost: 0.855 train_acc: 0.790 test_acc: 0.846\n",
            "step 1000 | loss : 0.7442027658820153\n",
            "step 2000 | loss : 0.7400814957097173\n",
            "step 3000 | loss : 0.7321368755201498\n",
            "step 4000 | loss : 0.7270737058892847\n",
            "step 5000 | loss : 0.7217553924888372\n",
            "step 6000 | loss : 0.7160122738045951\n",
            "step 7000 | loss : 0.7106897597451295\n",
            "step 8000 | loss : 0.7054555240096524\n",
            "step 9000 | loss : 0.7003526510041621\n",
            "step 10000 | loss : 0.6959900973264128\n",
            "idx: 03/50 cost: 0.696 train_acc: 0.860 test_acc: 0.861\n",
            "step 1000 | loss : 0.6413672187179327\n",
            "step 2000 | loss : 0.6402565053701401\n",
            "step 3000 | loss : 0.6379141134520372\n",
            "step 4000 | loss : 0.6325437098406256\n",
            "step 5000 | loss : 0.6301506002902985\n",
            "step 6000 | loss : 0.6268814683308204\n",
            "step 7000 | loss : 0.6238705958064114\n",
            "step 8000 | loss : 0.6208070020005106\n",
            "step 9000 | loss : 0.6172749698095852\n",
            "step 10000 | loss : 0.6150057669892908\n",
            "idx: 04/50 cost: 0.615 train_acc: 0.880 test_acc: 0.869\n",
            "step 1000 | loss : 0.5788406197279692\n",
            "step 2000 | loss : 0.5783312389925123\n",
            "step 3000 | loss : 0.5771891983300448\n",
            "step 4000 | loss : 0.5752106276862323\n",
            "step 5000 | loss : 0.5730473453193903\n",
            "step 6000 | loss : 0.5706332614980638\n",
            "step 7000 | loss : 0.5689569191773023\n",
            "step 8000 | loss : 0.5666632264684885\n",
            "step 9000 | loss : 0.564932206157181\n",
            "step 10000 | loss : 0.5627451970085502\n",
            "idx: 05/50 cost: 0.563 train_acc: 0.900 test_acc: 0.876\n",
            "step 1000 | loss : 0.5404184439629316\n",
            "step 2000 | loss : 0.5402383239790797\n",
            "step 3000 | loss : 0.538578012401859\n",
            "step 4000 | loss : 0.5358676803410053\n",
            "step 5000 | loss : 0.5343453312411904\n",
            "step 6000 | loss : 0.5329346663989126\n",
            "step 7000 | loss : 0.5316900920804057\n",
            "step 8000 | loss : 0.5295471147978679\n",
            "step 9000 | loss : 0.5280084356955356\n",
            "step 10000 | loss : 0.5268125666156411\n",
            "idx: 06/50 cost: 0.527 train_acc: 0.910 test_acc: 0.879\n",
            "step 1000 | loss : 0.508889416962862\n",
            "step 2000 | loss : 0.5068463577851653\n",
            "step 3000 | loss : 0.5061825917959213\n",
            "step 4000 | loss : 0.5054444656297564\n",
            "step 5000 | loss : 0.5036338657498359\n",
            "step 6000 | loss : 0.5022622400696078\n",
            "step 7000 | loss : 0.5012335486859083\n",
            "step 8000 | loss : 0.5004552720887586\n",
            "step 9000 | loss : 0.4992289143378536\n",
            "step 10000 | loss : 0.497930351408571\n",
            "idx: 07/50 cost: 0.498 train_acc: 0.850 test_acc: 0.882\n",
            "step 1000 | loss : 0.4877228815406561\n",
            "step 2000 | loss : 0.48399435663595797\n",
            "step 3000 | loss : 0.48363739721228677\n",
            "step 4000 | loss : 0.4821404933240265\n",
            "step 5000 | loss : 0.48149363194704053\n",
            "step 6000 | loss : 0.4799732937676211\n",
            "step 7000 | loss : 0.4788015734702349\n",
            "step 8000 | loss : 0.47805282346904276\n",
            "step 9000 | loss : 0.47703435653862025\n",
            "step 10000 | loss : 0.4758788055069745\n",
            "idx: 08/50 cost: 0.476 train_acc: 0.900 test_acc: 0.885\n",
            "step 1000 | loss : 0.46468018739670514\n",
            "step 2000 | loss : 0.4633028006590903\n",
            "step 3000 | loss : 0.4640114976565043\n",
            "step 4000 | loss : 0.46278157125227154\n",
            "step 5000 | loss : 0.46144528748244046\n",
            "step 6000 | loss : 0.4610207772391538\n",
            "step 7000 | loss : 0.460218538181058\n",
            "step 8000 | loss : 0.45927880103420465\n",
            "step 9000 | loss : 0.4582853362717562\n",
            "step 10000 | loss : 0.45752597492039204\n",
            "idx: 09/50 cost: 0.458 train_acc: 0.890 test_acc: 0.888\n",
            "step 1000 | loss : 0.4467311700582504\n",
            "step 2000 | loss : 0.44625964187830686\n",
            "step 3000 | loss : 0.4459169699686269\n",
            "step 4000 | loss : 0.44597145401220767\n",
            "step 5000 | loss : 0.44525115543082355\n",
            "step 6000 | loss : 0.4442930829109003\n",
            "step 7000 | loss : 0.4439346561426563\n",
            "step 8000 | loss : 0.4428536748983897\n",
            "step 9000 | loss : 0.4419870347144703\n",
            "step 10000 | loss : 0.44147448554746804\n",
            "idx: 10/50 cost: 0.441 train_acc: 0.880 test_acc: 0.891\n",
            "step 1000 | loss : 0.43775725323706866\n",
            "step 2000 | loss : 0.4338094954974949\n",
            "step 3000 | loss : 0.43249483583122494\n",
            "step 4000 | loss : 0.4320240291170776\n",
            "step 5000 | loss : 0.4317274376541376\n",
            "step 6000 | loss : 0.4308828613509734\n",
            "step 7000 | loss : 0.43039578251008476\n",
            "step 8000 | loss : 0.4296027605654672\n",
            "step 9000 | loss : 0.42897069299966095\n",
            "step 10000 | loss : 0.4282185042493045\n",
            "idx: 11/50 cost: 0.428 train_acc: 0.900 test_acc: 0.893\n",
            "step 1000 | loss : 0.42051986176520584\n",
            "step 2000 | loss : 0.4214262487068772\n",
            "step 3000 | loss : 0.42073823396861554\n",
            "step 4000 | loss : 0.42021763376519083\n",
            "step 5000 | loss : 0.4196700578995049\n",
            "step 6000 | loss : 0.4185014027177046\n",
            "step 7000 | loss : 0.4183136618557785\n",
            "step 8000 | loss : 0.4178969183289446\n",
            "step 9000 | loss : 0.41688717578475676\n",
            "step 10000 | loss : 0.41656625107489526\n",
            "idx: 12/50 cost: 0.417 train_acc: 0.880 test_acc: 0.895\n",
            "step 1000 | loss : 0.4097711720392108\n",
            "step 2000 | loss : 0.4092127592414618\n",
            "step 3000 | loss : 0.40954936906695366\n",
            "step 4000 | loss : 0.409130998250097\n",
            "step 5000 | loss : 0.4084672288224101\n",
            "step 6000 | loss : 0.40788953048735854\n",
            "step 7000 | loss : 0.40727971796797857\n",
            "step 8000 | loss : 0.40715869862399995\n",
            "step 9000 | loss : 0.4064671204711\n",
            "step 10000 | loss : 0.4060907725550234\n",
            "idx: 13/50 cost: 0.406 train_acc: 0.900 test_acc: 0.896\n",
            "step 1000 | loss : 0.40068917349725963\n",
            "step 2000 | loss : 0.4018725885823369\n",
            "step 3000 | loss : 0.39999042397240797\n",
            "step 4000 | loss : 0.39919430543668566\n",
            "step 5000 | loss : 0.3989156314581633\n",
            "step 6000 | loss : 0.3986120049941043\n",
            "step 7000 | loss : 0.3981295711653573\n",
            "step 8000 | loss : 0.3976693005468696\n",
            "step 9000 | loss : 0.39732932639949853\n",
            "step 10000 | loss : 0.396876506537199\n",
            "idx: 14/50 cost: 0.397 train_acc: 0.920 test_acc: 0.897\n",
            "step 1000 | loss : 0.3887576600536704\n",
            "step 2000 | loss : 0.3911210468709469\n",
            "step 3000 | loss : 0.39126397780577343\n",
            "step 4000 | loss : 0.3903789794519544\n",
            "step 5000 | loss : 0.3904833868086338\n",
            "step 6000 | loss : 0.38997119033460814\n",
            "step 7000 | loss : 0.3897114927438753\n",
            "step 8000 | loss : 0.3892570769442245\n",
            "step 9000 | loss : 0.38908126179294455\n",
            "step 10000 | loss : 0.388643462651968\n",
            "idx: 15/50 cost: 0.389 train_acc: 0.960 test_acc: 0.898\n",
            "step 1000 | loss : 0.3818938762694597\n",
            "step 2000 | loss : 0.3823917015679181\n",
            "step 3000 | loss : 0.3828768671527505\n",
            "step 4000 | loss : 0.382589225159958\n",
            "step 5000 | loss : 0.3828016700685024\n",
            "step 6000 | loss : 0.38193634926651915\n",
            "step 7000 | loss : 0.3815819906730737\n",
            "step 8000 | loss : 0.38130515964794903\n",
            "step 9000 | loss : 0.381100648291409\n",
            "step 10000 | loss : 0.3807691391758621\n",
            "idx: 16/50 cost: 0.381 train_acc: 0.840 test_acc: 0.899\n",
            "step 1000 | loss : 0.3764666754975915\n",
            "step 2000 | loss : 0.37638226624950766\n",
            "step 3000 | loss : 0.37632127016286054\n",
            "step 4000 | loss : 0.3760060764066875\n",
            "step 5000 | loss : 0.3757674788162112\n",
            "step 6000 | loss : 0.3751339324216048\n",
            "step 7000 | loss : 0.37508611450344326\n",
            "step 8000 | loss : 0.37466772746667265\n",
            "step 9000 | loss : 0.3744428675994277\n",
            "step 10000 | loss : 0.37412824528887867\n",
            "idx: 17/50 cost: 0.374 train_acc: 0.930 test_acc: 0.899\n",
            "step 1000 | loss : 0.3705400832146406\n",
            "step 2000 | loss : 0.36859710310027005\n",
            "step 3000 | loss : 0.3696449519445499\n",
            "step 4000 | loss : 0.36941475144959984\n",
            "step 5000 | loss : 0.3689721175014973\n",
            "step 6000 | loss : 0.36900405037651457\n",
            "step 7000 | loss : 0.3689132322402937\n",
            "step 8000 | loss : 0.36833626896888016\n",
            "step 9000 | loss : 0.3680518608532018\n",
            "step 10000 | loss : 0.3675711080253124\n",
            "idx: 18/50 cost: 0.368 train_acc: 0.910 test_acc: 0.900\n",
            "step 1000 | loss : 0.36587802340090275\n",
            "step 2000 | loss : 0.3646734991427511\n",
            "step 3000 | loss : 0.36477947753667833\n",
            "step 4000 | loss : 0.3643225234095007\n",
            "step 5000 | loss : 0.3639408324196935\n",
            "step 6000 | loss : 0.36345848479370274\n",
            "step 7000 | loss : 0.3631244398930243\n",
            "step 8000 | loss : 0.3626301740221679\n",
            "step 9000 | loss : 0.3626148610520694\n",
            "step 10000 | loss : 0.3623707523792982\n",
            "idx: 19/50 cost: 0.362 train_acc: 0.960 test_acc: 0.902\n",
            "step 1000 | loss : 0.3598725467547774\n",
            "step 2000 | loss : 0.35927287066727875\n",
            "step 3000 | loss : 0.3581819522182147\n",
            "step 4000 | loss : 0.3583359132818878\n",
            "step 5000 | loss : 0.35793565591573717\n",
            "step 6000 | loss : 0.3578634319293002\n",
            "step 7000 | loss : 0.35735833562271935\n",
            "step 8000 | loss : 0.357382035471499\n",
            "step 9000 | loss : 0.35689346506777736\n",
            "step 10000 | loss : 0.3569241407547146\n",
            "idx: 20/50 cost: 0.357 train_acc: 0.890 test_acc: 0.902\n",
            "step 1000 | loss : 0.35270596426725387\n",
            "step 2000 | loss : 0.3530393608156592\n",
            "step 3000 | loss : 0.3520140422744056\n",
            "step 4000 | loss : 0.3527703173542395\n",
            "step 5000 | loss : 0.35270081954970955\n",
            "step 6000 | loss : 0.3524011545771112\n",
            "step 7000 | loss : 0.35218110850932344\n",
            "step 8000 | loss : 0.35216095272591336\n",
            "step 9000 | loss : 0.35188159905663796\n",
            "step 10000 | loss : 0.35153681586422025\n",
            "idx: 21/50 cost: 0.352 train_acc: 0.950 test_acc: 0.904\n",
            "step 1000 | loss : 0.3497702515795827\n",
            "step 2000 | loss : 0.3499619223009795\n",
            "step 3000 | loss : 0.3499362528510392\n",
            "step 4000 | loss : 0.3490195023575798\n",
            "step 5000 | loss : 0.34868567262217404\n",
            "step 6000 | loss : 0.3486055622392644\n",
            "step 7000 | loss : 0.347889432585133\n",
            "step 8000 | loss : 0.34771973090199754\n",
            "step 9000 | loss : 0.34776495991854206\n",
            "step 10000 | loss : 0.34749532729275523\n",
            "idx: 22/50 cost: 0.347 train_acc: 0.900 test_acc: 0.905\n",
            "step 1000 | loss : 0.344227181635797\n",
            "step 2000 | loss : 0.34493163448572156\n",
            "step 3000 | loss : 0.3444093921855092\n",
            "step 4000 | loss : 0.34415908629633485\n",
            "step 5000 | loss : 0.34460522133111954\n",
            "step 6000 | loss : 0.34330808885519704\n",
            "step 7000 | loss : 0.3434954973480531\n",
            "step 8000 | loss : 0.34336804632749407\n",
            "step 9000 | loss : 0.34322252907107276\n",
            "step 10000 | loss : 0.3428088225916028\n",
            "idx: 23/50 cost: 0.343 train_acc: 0.980 test_acc: 0.905\n",
            "step 1000 | loss : 0.34191448605805635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwY7_LSB1sFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## GPU 사용\n",
        "import os\n",
        "os.envirom['CUDA_VISIBLE_DEVICEES']= \"2\" \n",
        "os.envirom['CUDA_VISIBLE_DEVICEES']= \"1, 2\" ## 여러개 사용, 소스를 직접 짜야 한다.\n",
        "\n",
        "os.envirom['CUDA_VISIBLE_DEVICEES']= \"-1\" ## CPU 사용 \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}