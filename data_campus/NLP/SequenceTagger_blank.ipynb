{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SequenceTagger_blank.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/getChan/data_campus/blob/master/NLP/SequenceTagger_blank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixmf_AX6AeR5",
        "colab_type": "text"
      },
      "source": [
        "## 1. CoNLL-2003 Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NZW2Ayi8qMT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "outputId": "7867e33b-595c-4ff1-888f-90e08c13b887"
      },
      "source": [
        "!wget -O CoNLL-2003.zip https://www.dropbox.com/s/hfr0r95e9ggjozm/CoNLL-2003.zip?dl=0\n",
        "!mkdir CoNLL-2003\n",
        "!unzip CoNLL-2003.zip -d CoNLL-2003\n",
        "!rm CoNLL-2003.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-31 05:32:45--  https://www.dropbox.com/s/hfr0r95e9ggjozm/CoNLL-2003.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.1, 2620:100:6031:1::a27d:5101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/hfr0r95e9ggjozm/CoNLL-2003.zip [following]\n",
            "--2019-07-31 05:32:47--  https://www.dropbox.com/s/raw/hfr0r95e9ggjozm/CoNLL-2003.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc2f734d0979e524656dbefa5356.dl.dropboxusercontent.com/cd/0/inline/AltLpXRVKKytvdmPhr-afmNMf1WXcwANlzu2RdBokn0HgAOViasvMFIz7MaU1CNHQ4DssJld3Uz3Csw6jMvSMveFbnjDfDccLzCZaxe6FEbN1g/file# [following]\n",
            "--2019-07-31 05:32:47--  https://uc2f734d0979e524656dbefa5356.dl.dropboxusercontent.com/cd/0/inline/AltLpXRVKKytvdmPhr-afmNMf1WXcwANlzu2RdBokn0HgAOViasvMFIz7MaU1CNHQ4DssJld3Uz3Csw6jMvSMveFbnjDfDccLzCZaxe6FEbN1g/file\n",
            "Resolving uc2f734d0979e524656dbefa5356.dl.dropboxusercontent.com (uc2f734d0979e524656dbefa5356.dl.dropboxusercontent.com)... 162.125.81.6, 2620:100:601b:6::a27d:806\n",
            "Connecting to uc2f734d0979e524656dbefa5356.dl.dropboxusercontent.com (uc2f734d0979e524656dbefa5356.dl.dropboxusercontent.com)|162.125.81.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/Alusc7FqRJ_ao9s1R1Z4R42QZpmArGkyr6OmNGHygnlZx_E2GWITq5DFXdPBukSQNzko3woHSOhVYThi_QF3g96BJAVC4hzjMY6aGA8qlydXYOqtgNW8b-v4YSFK9JZQQvaDuN6F6Xx6WwO88QI4T4SHedljcXPbfYQY84Y7QdD7HGqFMXlBw8tZ0HYncWZGK0Rm5NfWRiHiwuDsZtRlHO0R6TmpUcU6HExTIv66kLJDwSJbyj-bBj23qshYBr9C2FYa4z-rsiSAvKUbqfZu-GSdGJTVP9bw31AB3pQfvnqh8_gGqDj65mTONFrZ2jhBYQOHI4Ac2Lg6QRmBoYklhwzW/file [following]\n",
            "--2019-07-31 05:32:49--  https://uc2f734d0979e524656dbefa5356.dl.dropboxusercontent.com/cd/0/inline2/Alusc7FqRJ_ao9s1R1Z4R42QZpmArGkyr6OmNGHygnlZx_E2GWITq5DFXdPBukSQNzko3woHSOhVYThi_QF3g96BJAVC4hzjMY6aGA8qlydXYOqtgNW8b-v4YSFK9JZQQvaDuN6F6Xx6WwO88QI4T4SHedljcXPbfYQY84Y7QdD7HGqFMXlBw8tZ0HYncWZGK0Rm5NfWRiHiwuDsZtRlHO0R6TmpUcU6HExTIv66kLJDwSJbyj-bBj23qshYBr9C2FYa4z-rsiSAvKUbqfZu-GSdGJTVP9bw31AB3pQfvnqh8_gGqDj65mTONFrZ2jhBYQOHI4Ac2Lg6QRmBoYklhwzW/file\n",
            "Reusing existing connection to uc2f734d0979e524656dbefa5356.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 691339 (675K) [application/zip]\n",
            "Saving to: ‘CoNLL-2003.zip’\n",
            "\n",
            "CoNLL-2003.zip      100%[===================>] 675.14K   557KB/s    in 1.2s    \n",
            "\n",
            "2019-07-31 05:32:51 (557 KB/s) - ‘CoNLL-2003.zip’ saved [691339/691339]\n",
            "\n",
            "Archive:  CoNLL-2003.zip\n",
            "  inflating: CoNLL-2003/label.vocab  \n",
            "  inflating: CoNLL-2003/sample.inputs  \n",
            "  inflating: CoNLL-2003/sample.labels  \n",
            "  inflating: CoNLL-2003/sample.vocab  \n",
            "  inflating: CoNLL-2003/test.inputs  \n",
            "  inflating: CoNLL-2003/test.labels  \n",
            "  inflating: CoNLL-2003/train.inputs  \n",
            "  inflating: CoNLL-2003/train.labels  \n",
            "  inflating: CoNLL-2003/train.vocab  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_91llDkIA95Z",
        "colab_type": "text"
      },
      "source": [
        "## 2. Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XstaB1Wa6Obr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "1112b144-42e0-41a3-d9cf-d4993dcdf8a7"
      },
      "source": [
        "import json\n",
        "import collections\n",
        "from datetime import datetime\n",
        "import os\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import tqdm\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_4JKAp8BKVA",
        "colab_type": "text"
      },
      "source": [
        "## 3. Logger Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKMt1HGt_Tel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_logger(path:str):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    logger = logging.getLogger()\n",
        "    logger.handlers = []\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    debug_fh = logging.FileHandler(os.path.join(path, \"debug.log\"))\n",
        "    debug_fh.setLevel(logging.DEBUG)\n",
        "\n",
        "    info_fh = logging.FileHandler(os.path.join(path, \"info.log\"))\n",
        "    info_fh.setLevel(logging.INFO)\n",
        "\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logging.INFO)\n",
        "\n",
        "    info_formatter = logging.Formatter('%(asctime)s | %(levelname)-8s | %(message)s')\n",
        "    debug_formatter = logging.Formatter('%(asctime)s | %(levelname)-8s | %(message)s | %(lineno)d:%(funcName)s')\n",
        "\n",
        "    ch.setFormatter(info_formatter)\n",
        "    info_fh.setFormatter(info_formatter)\n",
        "    debug_fh.setFormatter(debug_formatter)\n",
        "\n",
        "    logger.addHandler(ch)\n",
        "    logger.addHandler(debug_fh)\n",
        "    logger.addHandler(info_fh)\n",
        "\n",
        "    return logger\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZedCFnABXYA",
        "colab_type": "text"
      },
      "source": [
        "## 4. Hyperparameters Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX0vqhpz-N_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparams_dict = {\n",
        "  \"root_dir\": \"out_dirs/KoreaUniv_Data/TEST/\",\n",
        "  \"vocab_size\": 10000,\n",
        "  \"num_epochs\": 10,\n",
        "  \"batch_size\": 16,\n",
        "  \"embedding_dim\": 100,\n",
        "  \"rnn_hidden_dim\": 128,\n",
        "  \"rnn_depth\": 3,\n",
        "  \"dropout_keep_prob\": 1.0\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNQw791j-lDB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "84b05dc5-7a7a-49fd-90de-8b764f68ccf1"
      },
      "source": [
        "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "root_dir = os.path.join(hparams_dict[\"root_dir\"], \"%s/\" % timestamp)\n",
        "logger = init_logger(root_dir)\n",
        "logger.info(\"Hyper-parameters: %s\" %str(hparams_dict))\n",
        "hparams_dict[\"root_dir\"] = root_dir\n",
        "hparams = collections.namedtuple(\"HParams\", sorted(hparams_dict.keys()))(**hparams_dict)\n",
        "\n",
        "data_dir = \"./CoNLL-2003\"\n",
        "dropout_keep_prob_ph = tf.placeholder(tf.float32, shape=[], name=\"dropout_keep_prob\")\n",
        "logger = logging.getLogger(__name__)\n",
        "iterator_initializers = []"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-07-31 05:37:08,286 | INFO     | Hyper-parameters: {'root_dir': 'out_dirs/KoreaUniv_Data/TEST/', 'vocab_size': 10000, 'num_epochs': 10, 'batch_size': 16, 'embedding_dim': 100, 'rnn_hidden_dim': 128, 'rnn_depth': 3, 'dropout_keep_prob': 1.0}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Pw09XzBl40",
        "colab_type": "text"
      },
      "source": [
        "## 5. Make Vocab Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7151biR-lcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_vocab_table():\n",
        "    \"\"\"\n",
        "    [A]\n",
        "    Vocabulary(단어집) 파일을 로드합니다.\n",
        "    단어 -> id, id -> 단어 변환 테이블을 생성합니다.\n",
        "\n",
        "    \"\"\"    \n",
        "    with open(os.path.join(data_dir, \"train.vocab\"), \"r\") as _f_handle:\n",
        "        vocab = [l.strip() for l in list(_f_handle) if len(l.strip()) > 0]\n",
        "    if len(vocab) > hparams.vocab_size:\n",
        "        vocab = vocab[:hparams.vocab_size]\n",
        "\n",
        "    id2word = vocab\n",
        "    word2id = {}\n",
        "    for i, word in enumerate(vocab):\n",
        "        word2id[word] = i\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    [B]\n",
        "    Label(태그 모음) 파일을 로드합니다.\n",
        "    태그 -> id, id -> 태그 변환 테이블을 생성합니다.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    with open(os.path.join(data_dir, \"label.vocab\"), \"r\") as _f_handle:\n",
        "      labels = [l.strip() for l in list(_f_handle) if len(l.strip()) > 0]\n",
        "      labels.insert(0, \"PAD\")\n",
        "      id2label = labels\n",
        "      label2id = {}\n",
        "      for i, label in enumerate(labels):\n",
        "          label2id[label] = i\n",
        "\n",
        "    return (id2word, word2id), (id2label, label2id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "regd4vxPBnNA",
        "colab_type": "text"
      },
      "source": [
        "## 6. Build Graph (Sequence Tagger Model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyr0hDR8_p9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_graph(inputs:tf.Tensor, lengths:tf.Tensor, id2word, id2label):\n",
        "      print(\"Building graph for model: sequence tagger\")\n",
        "\n",
        "      \"\"\"\n",
        "      [C]\n",
        "      단어 임베딩 행렬을 생성합니다.\n",
        "      단어 id를 단어 임베딩 텐서로 변환합니다.\n",
        "      \"\"\"\n",
        " \n",
        "      # Number of possible output categories.\n",
        "      output_dim = len(id2label)\n",
        "      vocab_size = len(id2word) + 1\n",
        "      embeddings = tf.get_variable(\n",
        "          \"embeddings\",\n",
        "          shape=[vocab_size, hparams.embedding_dim],\n",
        "          initializer=tf.initializers.variance_scaling(\n",
        "              scale=1.0, mode=\"fan_out\", distribution=\"uniform\")\n",
        "      )\n",
        "      embedded = tf.nn.embedding_lookup(embeddings, inputs)\n",
        "      # shape = [batch_size, sequence_length: batch마다 이것보다 작은 문장들은 padding, embed_dim:100] : inputs\n",
        "      layer_out = embedded\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      [D]\n",
        "      단어 임베딩을 RNN의 입력으로 사용하기 전,\n",
        "      차원 수를 맞춰주고 성능을 향상시키기 위해\n",
        "      projection layer를 생성하여 텐서를 통과시킵니다.\n",
        "      \"\"\"\n",
        "\n",
        "      # batch, seqence_length, embedding_dim -> batch, sequence_length, rnn_hidden_dim\n",
        "      layer_out = tf.layers.dense(\n",
        "          inputs=layer_out,\n",
        "          units=hparams.rnn_hidden_dim,\n",
        "          activation=tf.nn.relu,\n",
        "          kernel_initializer=tf.initializers.variance_scaling(\n",
        "              scale=1.0, mode=\"fan_avg\", distribution=\"normal\"),\n",
        "          name=\"input_projection\"\n",
        "      )\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      [E]\n",
        "      양방향 RNN을 생성하고, 여기에 텐서를 통과시킵니다.\n",
        "      이렇게 하여, 단어간 의존 관계가 반영된 단어 자질 텐서를 얻습니다.\n",
        "      \"\"\"\n",
        "      with tf.variable_scope(\"bi-RNN\"):\n",
        "          # Build RNN layers\n",
        "          rnn_cell_forward = tf.contrib.rnn.LSTMCell(hparams.rnn_hidden_dim)\n",
        "          rnn_cell_backward = tf.contrib.rnn.LSTMCell(hparams.rnn_hidden_dim)\n",
        "\n",
        "          # Apply dropout to RNN\n",
        "          if hparams.dropout_keep_prob < 1.0:\n",
        "              rnn_cell_forward = tf.contrib.rnn.DropoutWrapper(rnn_cell_forward, output_keep_prob=dropout_keep_prob_ph)\n",
        "              rnn_cell_backward = tf.contrib.rnn.DropoutWrapper(rnn_cell_backward, output_keep_prob=dropout_keep_prob_ph)\n",
        "\n",
        "          # Stack multiple layers of RNN\n",
        "          # rnn_cell_forward = tf.contrib.rnn.MultiRNNCell([rnn_cell_forward] * hparams.rnn_depth)\n",
        "          # rnn_cell_backward = tf.contrib.rnn.MultiRNNCell([rnn_cell_backward] * hparams.rnn_depth)\n",
        "\n",
        "          (output_forward, output_backward), _ = tf.nn.bidirectional_dynamic_rnn(\n",
        "              rnn_cell_forward, rnn_cell_backward,\n",
        "              inputs=layer_out,\n",
        "              sequence_length=lengths, # embedding lookup에서 다 padding 해서 length 같지 않나?\n",
        "              dtype=tf.float32\n",
        "          )\n",
        "          \n",
        "          \n",
        "          hiddens = tf.concat([output_forward, output_backward], axis=-1)\n",
        "          # shape = [batch_size, max_sequence_length, rnn_hidden_dim*2] # backward vector concat하므로  *2 된 vector\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      [F]\n",
        "      마스킹을 적용하여 문장 길이를 통일하기 위해 적용했던 padding을 제거합니다.\n",
        "      \"\"\"\n",
        "      \n",
        "      # Donald Trump is the president of the United States\n",
        "      # Barack Obama was the president of the United States <PAD> <PAD>....\n",
        "      # I live in paris <PAD> <PAD> ,.....\n",
        "      \n",
        "      \n",
        "      # [3, 10, 100](embedding) -> [3,10,128] (input_projection) -> [3,10,128](rnn_forward) , [3,10,128](rnn_forward) ->  [3,10,256] \n",
        "      # -> [3,10,10]\n",
        "            \n",
        "      # [10, 6, 5] sequence_mask 통과시키면\n",
        "      # -> [[TRUE] * 10,\n",
        "      #     [TRUE] * 6 + [FALSE] * 4...] 이렇게 변한다.\n",
        "      \n",
        "      \n",
        "      mask = tf.sequence_mask(lengths)\n",
        "      \n",
        "      ## boolean mask : TRUE에 해당되는 것만 가져와라\n",
        "      bi_lstm_out = tf.reshape(tf.boolean_mask(hiddens, mask), [-1, hparams.rnn_hidden_dim * 2])\n",
        "      layer_out = bi_lstm_out  # shape=[sum of seq length, 2*LSTM hidden layer size]\n",
        "      # [21,256] W: [256,10], b: [10] ->[21,10] 왜 21? 패딩을 없어주면 10, 5, 6 나오기 때문임.\n",
        "\n",
        "      \"\"\"\n",
        "      [G]\n",
        "      단어 자질 텐서를 바탕으로 단어의 태그를 예측합니다.\n",
        "      이를 위해 fully-connected(dense) layer를 생성하고 텐서를 통과시킵니다.\n",
        "      \"\"\"\n",
        "\n",
        "      with tf.variable_scope(\"read-out\"):\n",
        "        prev_layer_size = layer_out.get_shape().as_list()[1]\n",
        "        weight = tf.get_variable(\"weight\", shape=[prev_layer_size, output_dim],\n",
        "                                 initializer=tf.initializers.variance_scaling(\n",
        "                                     scale=2.0, mode=\"fan_in\", distribution=\"normal\"\n",
        "                                 ))\n",
        "        bias = tf.get_variable(\"bias\", shape=[output_dim],\n",
        "                               initializer=tf.initializers.zeros())\n",
        "        predictions = tf.add(tf.matmul(layer_out, weight), bias, name='predictions')\n",
        "\n",
        "\n",
        "      return predictions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSmF2AfNBnsp",
        "colab_type": "text"
      },
      "source": [
        "## 7. Load Data (tf.data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5TRwTMs-ljq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(id2word, word2id, id2label, label2id):\n",
        "      \"\"\"\n",
        "      [L]\n",
        "      단어->id 및 태그->id 변환 테이블을 텐서 그래프에 추가합니다.\n",
        "      \"\"\"\n",
        "      word2id = tf.contrib.lookup.index_table_from_tensor(\n",
        "        mapping=tf.constant(id2word),\n",
        "        num_oov_buckets=1,\n",
        "        name=\"word2id\"\n",
        "      )\n",
        "\n",
        "      label2id = tf.contrib.lookup.index_table_from_tensor(\n",
        "        mapping=tf.constant(id2label),\n",
        "        default_value=label2id[\"O\"],\n",
        "        name=\"label2id\"\n",
        "      )\n",
        "\n",
        "      \"\"\"\n",
        "      [M]\n",
        "      입력 데이터 파일을 읽어들여 이를 단어 id로 변환하는 텐서 그래프를 생성합니다.\n",
        "      \"\"\"\n",
        "      input_dataset = tf.data.TextLineDataset(os.path.join(data_dir, \"train.inputs\"))\n",
        "      batched_input_dataset = input_dataset.batch(hparams.batch_size)\n",
        "      input_iterator = batched_input_dataset.make_initializable_iterator()\n",
        "      batch_input = input_iterator.get_next()\n",
        "      batch_input.set_shape([hparams.batch_size])\n",
        "      words = tf.string_split(batch_input, \" \")\n",
        "      word_ids = word2id.lookup(words)\n",
        "      dense_word_ids = tf.sparse_tensor_to_dense(word_ids)\n",
        "      # shape = [batch_size, time]\n",
        "\n",
        "\n",
        "      line_number = word_ids.indices[:, 0]\n",
        "      line_position = word_ids.indices[:, 1]\n",
        "      lengths = tf.segment_max(data=line_position,\n",
        "                               segment_ids=line_number) + 1\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      [N]\n",
        "      태그 데이터 파일을 읽어들여 이를 태그 id로 변환하는 텐서 그래프를 생성합니다.\n",
        "      \"\"\"\n",
        "\n",
        "      label_dataset = tf.data.TextLineDataset(os.path.join(data_dir, \"train.labels\"))\n",
        "      batched_label_dataset = label_dataset.batch(hparams.batch_size)\n",
        "      label_iterator = batched_label_dataset.make_initializable_iterator()\n",
        "      batch_label_str = label_iterator.get_next()\n",
        "      batch_label = tf.string_split(batch_label_str, \" \")\n",
        "      label_ids = label2id.lookup(batch_label)\n",
        "      dense_label_ids = tf.sparse_tensor_to_dense(label_ids)\n",
        "      # shape = [batch_size, time]\n",
        "\n",
        "      mask = tf.sequence_mask(lengths)\n",
        "      dense_label_ids = tf.boolean_mask(dense_label_ids, mask)\n",
        "\n",
        "      iterator_initializers.append(input_iterator.initializer)\n",
        "      iterator_initializers.append(label_iterator.initializer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      return dense_word_ids, dense_label_ids, lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XTEfi0RBoJT",
        "colab_type": "text"
      },
      "source": [
        "## 8. Train Model (session call)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjm5_0Bj-lp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def train_model():\n",
        "        sess = tf.Session()\n",
        "        with sess.as_default():\n",
        "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "            # vocab 만드는 부분\n",
        "            (id2word, word2id), (id2label, label2id) = make_vocab_table()\n",
        "            \n",
        "            # Data loading\n",
        "            inputs, labels, lengths = load_data(id2word, word2id, id2label, label2id)\n",
        "\n",
        "            # 실제 딥러닝 모델 구현\n",
        "            with tf.variable_scope(\"build_graph\", reuse=False):\n",
        "                logits = build_graph(inputs, lengths, id2word, id2label)\n",
        "\n",
        "            \"\"\"\n",
        "            [O]\n",
        "            모델을 훈련시키기 위해 필요한 오퍼레이션들을 텐서 그래프에 추가합니다.\n",
        "            여기에는 loss, train, accuracy 계산 등이 포함됩니다.\n",
        "            \"\"\"\n",
        "            \n",
        "            loss_op = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels,\n",
        "                                                                     name=\"cross_entropy\")\n",
        "            loss_op = tf.reduce_mean(loss_op, name='cross_entropy_mean')\n",
        "            train_op = tf.train.AdamOptimizer().minimize(loss_op, global_step=global_step)\n",
        "\n",
        "            eval = tf.nn.in_top_k(logits, labels, 1)\n",
        "            correct_count = tf.reduce_sum(tf.cast(eval, tf.int32))\n",
        "            accuracy = tf.divide(correct_count, tf.shape(labels)[0])\n",
        "\n",
        "\n",
        "            # Initialize iterators, tables, and variables.\n",
        "            local_iterator_initializers = tf.group(*iterator_initializers)\n",
        "            tf.tables_initializer().run()\n",
        "            tf.global_variables_initializer().run()\n",
        "\n",
        "            saver = tf.train.Saver()\n",
        "\n",
        "            for epochs_completed in range(hparams.num_epochs):\n",
        "                local_iterator_initializers.run()\n",
        "                accuracy_mean, loss_mean, idx_cnt = 0, 0, 0\n",
        "                while True:\n",
        "                    \"\"\"\n",
        "                    [P]\n",
        "                    그래프에 데이터를 입력하여 필요한 계산들을 수행하고,\n",
        "                    Loss에 따라 gradient를 계산하여 파라미터들을 업데이트합니다.\n",
        "                    이러한 과정을 training step이라고 합니다.\n",
        "                    \"\"\"\n",
        "                    try:\n",
        "                      accuracy_val, label_ids_val, loss_val, global_step_val, _ = sess.run(\n",
        "                          [accuracy, labels, loss_op, global_step, train_op],\n",
        "                          feed_dict={dropout_keep_prob_ph: hparams.dropout_keep_prob}\n",
        "                      )\n",
        "                      accuracy_mean += accuracy_val\n",
        "                      loss_mean += loss_val\n",
        "                      idx_cnt += 1\n",
        "                      if global_step_val % 50 == 0:\n",
        "                          accuracy_mean /= idx_cnt\n",
        "                          loss_mean /= idx_cnt\n",
        "                          logger.info(\"[Step %d] loss: %.4f, accuracy: %.2f%%\" % (global_step_val, loss_mean, accuracy_mean * 100))\n",
        "                          accuracy_mean, loss_mean,idx_cnt = 0, 0, 0\n",
        "                    except tf.errors.OutOfRangeError:\n",
        "                      # End of epoch.\n",
        "                      break\n",
        "\n",
        "                \"\"\"\n",
        "                [Q]\n",
        "                전체 학습 데이터에 대하여 1회 학습을 완료하였습니다.\n",
        "                이를 1 epoch라고 합니다.\n",
        "                딥러닝 모델의 학습은 일반적으로 수십~수백 epoch 동안 진행됩니다.\n",
        "                \n",
        "                \"\"\"\n",
        "                logger.info(\"End of epoch %d.\" % (epochs_completed+1))\n",
        "                save_path = saver.save(sess, \"saves/model.ckpt\", global_step=global_step_val)\n",
        "                logger.info(\"Model saved at: %s\" % save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x9dVWZEBpRv",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ofY5we0ADFW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a71c6b8-193f-4a9f-ecd9-1b1d2ecb6c32"
      },
      "source": [
        "# Train the vanilla Bi-directional LSTM model\n",
        "train_model()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-07-31 06:10:25,238 | WARNING  | \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "2019-07-31 06:10:25,469 | WARNING  | From <ipython-input-15-6b9277005a04>:24: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "2019-07-31 06:10:25,490 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/lookup_ops.py:978: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "2019-07-31 06:10:25,566 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`normal` is a deprecated alias for `truncated_normal`\n",
            "2019-07-31 06:10:25,567 | WARNING  | From <ipython-input-14-c485357300ab>:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Building graph for model: sequence tagger\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-07-31 06:10:25,955 | WARNING  | From <ipython-input-14-c485357300ab>:48: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "2019-07-31 06:10:25,957 | WARNING  | From <ipython-input-14-c485357300ab>:64: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "2019-07-31 06:10:25,962 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "2019-07-31 06:10:26,066 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "2019-07-31 06:10:26,085 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "2019-07-31 06:10:33,762 | INFO     | [Step 50] loss: 1.2781, accuracy: 73.64%\n",
            "2019-07-31 06:10:39,001 | INFO     | [Step 100] loss: 0.7891, accuracy: 82.36%\n",
            "2019-07-31 06:10:43,761 | INFO     | [Step 150] loss: 0.7566, accuracy: 80.67%\n",
            "2019-07-31 06:10:48,516 | INFO     | [Step 200] loss: 0.7332, accuracy: 78.76%\n",
            "2019-07-31 06:10:52,988 | INFO     | [Step 250] loss: 0.6459, accuracy: 79.70%\n",
            "2019-07-31 06:10:57,342 | INFO     | [Step 300] loss: 0.5438, accuracy: 84.68%\n",
            "2019-07-31 06:10:59,710 | INFO     | [Step 350] loss: 0.6227, accuracy: 80.89%\n",
            "2019-07-31 06:11:03,536 | INFO     | [Step 400] loss: 0.4566, accuracy: 84.93%\n",
            "2019-07-31 06:11:07,583 | INFO     | [Step 450] loss: 0.4166, accuracy: 86.18%\n",
            "2019-07-31 06:11:12,189 | INFO     | [Step 500] loss: 0.4061, accuracy: 85.66%\n",
            "2019-07-31 06:11:16,995 | INFO     | [Step 550] loss: 0.3555, accuracy: 89.80%\n",
            "2019-07-31 06:11:21,387 | INFO     | [Step 600] loss: 0.3808, accuracy: 86.43%\n",
            "2019-07-31 06:11:26,660 | INFO     | [Step 650] loss: 0.3833, accuracy: 88.18%\n",
            "2019-07-31 06:11:30,909 | INFO     | [Step 700] loss: 0.3114, accuracy: 89.72%\n",
            "2019-07-31 06:11:35,904 | INFO     | [Step 750] loss: 0.3851, accuracy: 87.45%\n",
            "2019-07-31 06:11:41,449 | INFO     | [Step 800] loss: 0.3158, accuracy: 90.02%\n",
            "2019-07-31 06:11:46,062 | INFO     | [Step 850] loss: 0.3158, accuracy: 89.62%\n",
            "2019-07-31 06:11:51,989 | INFO     | [Step 900] loss: 0.2531, accuracy: 92.08%\n",
            "2019-07-31 06:11:55,884 | INFO     | End of epoch 1.\n",
            "2019-07-31 06:11:56,078 | INFO     | Model saved at: saves/model.ckpt-937\n",
            "2019-07-31 06:11:57,526 | INFO     | [Step 950] loss: 0.3069, accuracy: 90.97%\n",
            "2019-07-31 06:12:01,323 | INFO     | [Step 1000] loss: 0.2868, accuracy: 91.75%\n",
            "2019-07-31 06:12:06,716 | INFO     | [Step 1050] loss: 0.2416, accuracy: 92.75%\n",
            "2019-07-31 06:12:10,865 | INFO     | [Step 1100] loss: 0.1703, accuracy: 95.63%\n",
            "2019-07-31 06:12:16,371 | INFO     | [Step 1150] loss: 0.2194, accuracy: 92.90%\n",
            "2019-07-31 06:12:19,924 | INFO     | [Step 1200] loss: 0.2224, accuracy: 93.46%\n",
            "2019-07-31 06:12:24,233 | INFO     | [Step 1250] loss: 0.1926, accuracy: 93.68%\n",
            "2019-07-31 06:12:26,908 | INFO     | [Step 1300] loss: 0.1387, accuracy: 96.53%\n",
            "2019-07-31 06:12:31,520 | INFO     | [Step 1350] loss: 0.1416, accuracy: 96.00%\n",
            "2019-07-31 06:12:35,133 | INFO     | [Step 1400] loss: 0.1518, accuracy: 95.10%\n",
            "2019-07-31 06:12:40,164 | INFO     | [Step 1450] loss: 0.1556, accuracy: 94.82%\n",
            "2019-07-31 06:12:44,601 | INFO     | [Step 1500] loss: 0.1796, accuracy: 94.60%\n",
            "2019-07-31 06:12:49,629 | INFO     | [Step 1550] loss: 0.1548, accuracy: 94.95%\n",
            "2019-07-31 06:12:54,966 | INFO     | [Step 1600] loss: 0.2110, accuracy: 93.29%\n",
            "2019-07-31 06:12:58,885 | INFO     | [Step 1650] loss: 0.1262, accuracy: 96.51%\n",
            "2019-07-31 06:13:04,451 | INFO     | [Step 1700] loss: 0.1768, accuracy: 94.79%\n",
            "2019-07-31 06:13:10,242 | INFO     | [Step 1750] loss: 0.1650, accuracy: 94.97%\n",
            "2019-07-31 06:13:14,868 | INFO     | [Step 1800] loss: 0.0987, accuracy: 97.17%\n",
            "2019-07-31 06:13:20,905 | INFO     | [Step 1850] loss: 0.1156, accuracy: 96.57%\n",
            "2019-07-31 06:13:23,119 | INFO     | End of epoch 2.\n",
            "2019-07-31 06:13:23,268 | INFO     | Model saved at: saves/model.ckpt-1874\n",
            "2019-07-31 06:13:26,004 | INFO     | [Step 1900] loss: 0.1327, accuracy: 96.39%\n",
            "2019-07-31 06:13:30,011 | INFO     | [Step 1950] loss: 0.1117, accuracy: 96.84%\n",
            "2019-07-31 06:13:35,806 | INFO     | [Step 2000] loss: 0.1165, accuracy: 96.62%\n",
            "2019-07-31 06:13:39,687 | INFO     | [Step 2050] loss: 0.0581, accuracy: 98.36%\n",
            "2019-07-31 06:13:45,524 | INFO     | [Step 2100] loss: 0.1106, accuracy: 96.68%\n",
            "2019-07-31 06:13:48,440 | INFO     | [Step 2150] loss: 0.0793, accuracy: 98.13%\n",
            "2019-07-31 06:13:52,623 | INFO     | [Step 2200] loss: 0.0847, accuracy: 97.38%\n",
            "2019-07-31 06:13:55,687 | INFO     | [Step 2250] loss: 0.0622, accuracy: 98.14%\n",
            "2019-07-31 06:14:00,527 | INFO     | [Step 2300] loss: 0.0815, accuracy: 97.74%\n",
            "2019-07-31 06:14:04,139 | INFO     | [Step 2350] loss: 0.0767, accuracy: 97.90%\n",
            "2019-07-31 06:14:09,413 | INFO     | [Step 2400] loss: 0.1191, accuracy: 96.72%\n",
            "2019-07-31 06:14:13,747 | INFO     | [Step 2450] loss: 0.0710, accuracy: 97.92%\n",
            "2019-07-31 06:14:19,183 | INFO     | [Step 2500] loss: 0.1336, accuracy: 95.70%\n",
            "2019-07-31 06:14:23,964 | INFO     | [Step 2550] loss: 0.1063, accuracy: 96.49%\n",
            "2019-07-31 06:14:28,245 | INFO     | [Step 2600] loss: 0.0763, accuracy: 97.84%\n",
            "2019-07-31 06:14:34,216 | INFO     | [Step 2650] loss: 0.1179, accuracy: 96.70%\n",
            "2019-07-31 06:14:39,533 | INFO     | [Step 2700] loss: 0.1189, accuracy: 96.28%\n",
            "2019-07-31 06:14:44,932 | INFO     | [Step 2750] loss: 0.0579, accuracy: 98.32%\n",
            "2019-07-31 06:14:50,641 | INFO     | [Step 2800] loss: 0.0903, accuracy: 97.43%\n",
            "2019-07-31 06:14:51,636 | INFO     | End of epoch 3.\n",
            "2019-07-31 06:14:51,785 | INFO     | Model saved at: saves/model.ckpt-2811\n",
            "2019-07-31 06:14:55,516 | INFO     | [Step 2850] loss: 0.0669, accuracy: 97.99%\n",
            "2019-07-31 06:15:00,155 | INFO     | [Step 2900] loss: 0.0644, accuracy: 98.29%\n",
            "2019-07-31 06:15:05,391 | INFO     | [Step 2950] loss: 0.0782, accuracy: 97.78%\n",
            "2019-07-31 06:15:09,644 | INFO     | [Step 3000] loss: 0.0559, accuracy: 98.21%\n",
            "2019-07-31 06:15:14,766 | INFO     | [Step 3050] loss: 0.0469, accuracy: 98.63%\n",
            "2019-07-31 06:15:18,452 | INFO     | [Step 3100] loss: 0.0520, accuracy: 98.63%\n",
            "2019-07-31 06:15:21,959 | INFO     | [Step 3150] loss: 0.0440, accuracy: 98.59%\n",
            "2019-07-31 06:15:25,213 | INFO     | [Step 3200] loss: 0.0357, accuracy: 99.05%\n",
            "2019-07-31 06:15:30,017 | INFO     | [Step 3250] loss: 0.0622, accuracy: 98.30%\n",
            "2019-07-31 06:15:34,169 | INFO     | [Step 3300] loss: 0.0481, accuracy: 98.63%\n",
            "2019-07-31 06:15:39,534 | INFO     | [Step 3350] loss: 0.0882, accuracy: 97.55%\n",
            "2019-07-31 06:15:43,608 | INFO     | [Step 3400] loss: 0.0663, accuracy: 97.95%\n",
            "2019-07-31 06:15:49,344 | INFO     | [Step 3450] loss: 0.0982, accuracy: 97.10%\n",
            "2019-07-31 06:15:54,219 | INFO     | [Step 3500] loss: 0.0639, accuracy: 98.02%\n",
            "2019-07-31 06:15:58,662 | INFO     | [Step 3550] loss: 0.0736, accuracy: 97.94%\n",
            "2019-07-31 06:16:04,359 | INFO     | [Step 3600] loss: 0.0875, accuracy: 97.51%\n",
            "2019-07-31 06:16:09,090 | INFO     | [Step 3650] loss: 0.0611, accuracy: 98.13%\n",
            "2019-07-31 06:16:14,952 | INFO     | [Step 3700] loss: 0.0659, accuracy: 98.08%\n",
            "2019-07-31 06:16:20,069 | INFO     | End of epoch 4.\n",
            "2019-07-31 06:16:20,211 | INFO     | Model saved at: saves/model.ckpt-3748\n",
            "2019-07-31 06:16:20,468 | INFO     | [Step 3750] loss: 0.0314, accuracy: 99.11%\n",
            "2019-07-31 06:16:24,646 | INFO     | [Step 3800] loss: 0.0460, accuracy: 98.71%\n",
            "2019-07-31 06:16:29,904 | INFO     | [Step 3850] loss: 0.0540, accuracy: 98.43%\n",
            "2019-07-31 06:16:34,723 | INFO     | [Step 3900] loss: 0.0469, accuracy: 98.86%\n",
            "2019-07-31 06:16:39,534 | INFO     | [Step 3950] loss: 0.0412, accuracy: 98.73%\n",
            "2019-07-31 06:16:43,851 | INFO     | [Step 4000] loss: 0.0311, accuracy: 99.14%\n",
            "2019-07-31 06:16:48,341 | INFO     | [Step 4050] loss: 0.0297, accuracy: 99.02%\n",
            "2019-07-31 06:16:50,776 | INFO     | [Step 4100] loss: 0.0234, accuracy: 99.33%\n",
            "2019-07-31 06:16:54,886 | INFO     | [Step 4150] loss: 0.0332, accuracy: 99.09%\n",
            "2019-07-31 06:16:59,113 | INFO     | [Step 4200] loss: 0.0398, accuracy: 98.89%\n",
            "2019-07-31 06:17:03,926 | INFO     | [Step 4250] loss: 0.0354, accuracy: 99.03%\n",
            "2019-07-31 06:17:08,746 | INFO     | [Step 4300] loss: 0.0594, accuracy: 98.38%\n",
            "2019-07-31 06:17:13,545 | INFO     | [Step 4350] loss: 0.0543, accuracy: 98.20%\n",
            "2019-07-31 06:17:19,050 | INFO     | [Step 4400] loss: 0.0854, accuracy: 97.40%\n",
            "2019-07-31 06:17:23,416 | INFO     | [Step 4450] loss: 0.0374, accuracy: 98.91%\n",
            "2019-07-31 06:17:28,515 | INFO     | [Step 4500] loss: 0.0585, accuracy: 98.37%\n",
            "2019-07-31 06:17:34,170 | INFO     | [Step 4550] loss: 0.0722, accuracy: 97.93%\n",
            "2019-07-31 06:17:38,715 | INFO     | [Step 4600] loss: 0.0462, accuracy: 98.58%\n",
            "2019-07-31 06:17:44,799 | INFO     | [Step 4650] loss: 0.0459, accuracy: 98.47%\n",
            "2019-07-31 06:17:48,419 | INFO     | End of epoch 5.\n",
            "2019-07-31 06:17:48,567 | INFO     | Model saved at: saves/model.ckpt-4685\n",
            "2019-07-31 06:17:50,251 | INFO     | [Step 4700] loss: 0.0403, accuracy: 98.80%\n",
            "2019-07-31 06:17:54,077 | INFO     | [Step 4750] loss: 0.0271, accuracy: 99.40%\n",
            "2019-07-31 06:17:59,961 | INFO     | [Step 4800] loss: 0.0396, accuracy: 98.91%\n",
            "2019-07-31 06:18:04,048 | INFO     | [Step 4850] loss: 0.0300, accuracy: 99.19%\n",
            "2019-07-31 06:18:09,807 | INFO     | [Step 4900] loss: 0.0358, accuracy: 99.01%\n",
            "2019-07-31 06:18:13,275 | INFO     | [Step 4950] loss: 0.0139, accuracy: 99.67%\n",
            "2019-07-31 06:18:17,730 | INFO     | [Step 5000] loss: 0.0246, accuracy: 99.28%\n",
            "2019-07-31 06:18:20,705 | INFO     | [Step 5050] loss: 0.0162, accuracy: 99.53%\n",
            "2019-07-31 06:18:25,516 | INFO     | [Step 5100] loss: 0.0312, accuracy: 99.04%\n",
            "2019-07-31 06:18:29,275 | INFO     | [Step 5150] loss: 0.0231, accuracy: 99.33%\n",
            "2019-07-31 06:18:34,200 | INFO     | [Step 5200] loss: 0.0276, accuracy: 99.24%\n",
            "2019-07-31 06:18:38,595 | INFO     | [Step 5250] loss: 0.0414, accuracy: 98.86%\n",
            "2019-07-31 06:18:43,783 | INFO     | [Step 5300] loss: 0.0443, accuracy: 98.26%\n",
            "2019-07-31 06:18:49,290 | INFO     | [Step 5350] loss: 0.0653, accuracy: 98.15%\n",
            "2019-07-31 06:18:53,489 | INFO     | [Step 5400] loss: 0.0334, accuracy: 99.07%\n",
            "2019-07-31 06:18:59,227 | INFO     | [Step 5450] loss: 0.0521, accuracy: 98.55%\n",
            "2019-07-31 06:19:04,815 | INFO     | [Step 5500] loss: 0.0533, accuracy: 98.38%\n",
            "2019-07-31 06:19:09,587 | INFO     | [Step 5550] loss: 0.0285, accuracy: 99.18%\n",
            "2019-07-31 06:19:15,877 | INFO     | [Step 5600] loss: 0.0364, accuracy: 98.90%\n",
            "2019-07-31 06:19:17,884 | INFO     | End of epoch 6.\n",
            "2019-07-31 06:19:17,915 | WARNING  | From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "2019-07-31 06:19:18,047 | INFO     | Model saved at: saves/model.ckpt-5622\n",
            "2019-07-31 06:19:21,055 | INFO     | [Step 5650] loss: 0.0276, accuracy: 99.15%\n",
            "2019-07-31 06:19:25,214 | INFO     | [Step 5700] loss: 0.0208, accuracy: 99.54%\n",
            "2019-07-31 06:19:30,948 | INFO     | [Step 5750] loss: 0.0347, accuracy: 98.97%\n",
            "2019-07-31 06:19:35,004 | INFO     | [Step 5800] loss: 0.0206, accuracy: 99.42%\n",
            "2019-07-31 06:19:40,982 | INFO     | [Step 5850] loss: 0.0215, accuracy: 99.38%\n",
            "2019-07-31 06:19:44,062 | INFO     | [Step 5900] loss: 0.0079, accuracy: 99.86%\n",
            "2019-07-31 06:19:48,427 | INFO     | [Step 5950] loss: 0.0187, accuracy: 99.39%\n",
            "2019-07-31 06:19:51,497 | INFO     | [Step 6000] loss: 0.0127, accuracy: 99.56%\n",
            "2019-07-31 06:19:56,472 | INFO     | [Step 6050] loss: 0.0241, accuracy: 99.29%\n",
            "2019-07-31 06:20:00,136 | INFO     | [Step 6100] loss: 0.0176, accuracy: 99.46%\n",
            "2019-07-31 06:20:05,443 | INFO     | [Step 6150] loss: 0.0333, accuracy: 99.18%\n",
            "2019-07-31 06:20:09,748 | INFO     | [Step 6200] loss: 0.0176, accuracy: 99.49%\n",
            "2019-07-31 06:20:15,271 | INFO     | [Step 6250] loss: 0.0498, accuracy: 98.38%\n",
            "2019-07-31 06:20:20,032 | INFO     | [Step 6300] loss: 0.0341, accuracy: 99.02%\n",
            "2019-07-31 06:20:24,452 | INFO     | [Step 6350] loss: 0.0284, accuracy: 99.22%\n",
            "2019-07-31 06:20:30,394 | INFO     | [Step 6400] loss: 0.0431, accuracy: 98.81%\n",
            "2019-07-31 06:20:35,601 | INFO     | [Step 6450] loss: 0.0419, accuracy: 98.78%\n",
            "2019-07-31 06:20:41,023 | INFO     | [Step 6500] loss: 0.0240, accuracy: 99.24%\n",
            "2019-07-31 06:20:46,721 | INFO     | [Step 6550] loss: 0.0313, accuracy: 99.06%\n",
            "2019-07-31 06:20:47,485 | INFO     | End of epoch 7.\n",
            "2019-07-31 06:20:47,638 | INFO     | Model saved at: saves/model.ckpt-6559\n",
            "2019-07-31 06:20:51,489 | INFO     | [Step 6600] loss: 0.0198, accuracy: 99.41%\n",
            "2019-07-31 06:20:56,296 | INFO     | [Step 6650] loss: 0.0162, accuracy: 99.48%\n",
            "2019-07-31 06:21:01,469 | INFO     | [Step 6700] loss: 0.0268, accuracy: 99.25%\n",
            "2019-07-31 06:21:06,041 | INFO     | [Step 6750] loss: 0.0161, accuracy: 99.50%\n",
            "2019-07-31 06:21:11,174 | INFO     | [Step 6800] loss: 0.0122, accuracy: 99.63%\n",
            "2019-07-31 06:21:15,013 | INFO     | [Step 6850] loss: 0.0102, accuracy: 99.69%\n",
            "2019-07-31 06:21:18,332 | INFO     | [Step 6900] loss: 0.0106, accuracy: 99.70%\n",
            "2019-07-31 06:21:21,612 | INFO     | [Step 6950] loss: 0.0090, accuracy: 99.77%\n",
            "2019-07-31 06:21:26,423 | INFO     | [Step 7000] loss: 0.0194, accuracy: 99.42%\n",
            "2019-07-31 06:21:30,689 | INFO     | [Step 7050] loss: 0.0153, accuracy: 99.54%\n",
            "2019-07-31 06:21:36,011 | INFO     | [Step 7100] loss: 0.0264, accuracy: 99.29%\n",
            "2019-07-31 06:21:40,072 | INFO     | [Step 7150] loss: 0.0202, accuracy: 99.45%\n",
            "2019-07-31 06:21:45,831 | INFO     | [Step 7200] loss: 0.0417, accuracy: 98.87%\n",
            "2019-07-31 06:21:50,691 | INFO     | [Step 7250] loss: 0.0205, accuracy: 99.42%\n",
            "2019-07-31 06:21:55,373 | INFO     | [Step 7300] loss: 0.0279, accuracy: 99.18%\n",
            "2019-07-31 06:22:01,229 | INFO     | [Step 7350] loss: 0.0353, accuracy: 99.00%\n",
            "2019-07-31 06:22:06,138 | INFO     | [Step 7400] loss: 0.0223, accuracy: 99.32%\n",
            "2019-07-31 06:22:12,057 | INFO     | [Step 7450] loss: 0.0249, accuracy: 99.27%\n",
            "2019-07-31 06:22:17,004 | INFO     | End of epoch 8.\n",
            "2019-07-31 06:22:17,159 | INFO     | Model saved at: saves/model.ckpt-7496\n",
            "2019-07-31 06:22:17,649 | INFO     | [Step 7500] loss: 0.0112, accuracy: 99.65%\n",
            "2019-07-31 06:22:21,642 | INFO     | [Step 7550] loss: 0.0129, accuracy: 99.65%\n",
            "2019-07-31 06:22:27,089 | INFO     | [Step 7600] loss: 0.0134, accuracy: 99.65%\n",
            "2019-07-31 06:22:31,889 | INFO     | [Step 7650] loss: 0.0170, accuracy: 99.51%\n",
            "2019-07-31 06:22:36,718 | INFO     | [Step 7700] loss: 0.0151, accuracy: 99.52%\n",
            "2019-07-31 06:22:40,972 | INFO     | [Step 7750] loss: 0.0094, accuracy: 99.73%\n",
            "2019-07-31 06:22:45,464 | INFO     | [Step 7800] loss: 0.0108, accuracy: 99.65%\n",
            "2019-07-31 06:22:47,901 | INFO     | [Step 7850] loss: 0.0079, accuracy: 99.79%\n",
            "2019-07-31 06:22:51,965 | INFO     | [Step 7900] loss: 0.0120, accuracy: 99.61%\n",
            "2019-07-31 06:22:56,100 | INFO     | [Step 7950] loss: 0.0136, accuracy: 99.49%\n",
            "2019-07-31 06:23:00,957 | INFO     | [Step 8000] loss: 0.0112, accuracy: 99.72%\n",
            "2019-07-31 06:23:05,549 | INFO     | [Step 8050] loss: 0.0219, accuracy: 99.33%\n",
            "2019-07-31 06:23:10,483 | INFO     | [Step 8100] loss: 0.0154, accuracy: 99.53%\n",
            "2019-07-31 06:23:15,944 | INFO     | [Step 8150] loss: 0.0400, accuracy: 98.73%\n",
            "2019-07-31 06:23:20,375 | INFO     | [Step 8200] loss: 0.0161, accuracy: 99.58%\n",
            "2019-07-31 06:23:25,707 | INFO     | [Step 8250] loss: 0.0214, accuracy: 99.24%\n",
            "2019-07-31 06:23:31,587 | INFO     | [Step 8300] loss: 0.0330, accuracy: 99.05%\n",
            "2019-07-31 06:23:36,492 | INFO     | [Step 8350] loss: 0.0191, accuracy: 99.41%\n",
            "2019-07-31 06:23:42,855 | INFO     | [Step 8400] loss: 0.0167, accuracy: 99.52%\n",
            "2019-07-31 06:23:46,325 | INFO     | End of epoch 9.\n",
            "2019-07-31 06:23:46,485 | INFO     | Model saved at: saves/model.ckpt-8433\n",
            "2019-07-31 06:23:48,454 | INFO     | [Step 8450] loss: 0.0116, accuracy: 99.59%\n",
            "2019-07-31 06:23:52,426 | INFO     | [Step 8500] loss: 0.0078, accuracy: 99.79%\n",
            "2019-07-31 06:23:58,359 | INFO     | [Step 8550] loss: 0.0131, accuracy: 99.64%\n",
            "2019-07-31 06:24:02,284 | INFO     | [Step 8600] loss: 0.0096, accuracy: 99.73%\n",
            "2019-07-31 06:24:08,180 | INFO     | [Step 8650] loss: 0.0167, accuracy: 99.55%\n",
            "2019-07-31 06:24:11,590 | INFO     | [Step 8700] loss: 0.0039, accuracy: 99.90%\n",
            "2019-07-31 06:24:16,105 | INFO     | [Step 8750] loss: 0.0090, accuracy: 99.73%\n",
            "2019-07-31 06:24:19,126 | INFO     | [Step 8800] loss: 0.0059, accuracy: 99.81%\n",
            "2019-07-31 06:24:24,030 | INFO     | [Step 8850] loss: 0.0101, accuracy: 99.72%\n",
            "2019-07-31 06:24:27,658 | INFO     | [Step 8900] loss: 0.0071, accuracy: 99.79%\n",
            "2019-07-31 06:24:32,889 | INFO     | [Step 8950] loss: 0.0107, accuracy: 99.72%\n",
            "2019-07-31 06:24:37,194 | INFO     | [Step 9000] loss: 0.0141, accuracy: 99.47%\n",
            "2019-07-31 06:24:42,607 | INFO     | [Step 9050] loss: 0.0146, accuracy: 99.59%\n",
            "2019-07-31 06:24:48,103 | INFO     | [Step 9100] loss: 0.0291, accuracy: 99.14%\n",
            "2019-07-31 06:24:52,524 | INFO     | [Step 9150] loss: 0.0173, accuracy: 99.47%\n",
            "2019-07-31 06:24:58,502 | INFO     | [Step 9200] loss: 0.0201, accuracy: 99.39%\n",
            "2019-07-31 06:25:04,252 | INFO     | [Step 9250] loss: 0.0259, accuracy: 99.21%\n",
            "2019-07-31 06:25:09,274 | INFO     | [Step 9300] loss: 0.0123, accuracy: 99.66%\n",
            "2019-07-31 06:25:15,493 | INFO     | [Step 9350] loss: 0.0153, accuracy: 99.57%\n",
            "2019-07-31 06:25:17,305 | INFO     | End of epoch 10.\n",
            "2019-07-31 06:25:17,460 | INFO     | Model saved at: saves/model.ckpt-9370\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz85PSZnNS2B",
        "colab_type": "text"
      },
      "source": [
        "## Model Load and Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWxSlqZd_iou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_predict(saved_file:str):\n",
        "    sentence = input(\"Enter a sentence: \")\n",
        "\n",
        "    \"\"\"\n",
        "    [H]\n",
        "    입력 문자열을 단어/문장부호 단위로 쪼개고, 이를 다시 단어 id로 변환합니다.\n",
        "    \"\"\"\n",
        "    sentence = word_tokenize(sentence)\n",
        "    word_ids = []\n",
        "    (id2word, word2id), (id2label, label2id) = make_vocab_table()\n",
        "\n",
        "    for word in sentence:\n",
        "        if word in word2id:\n",
        "            word_ids.append(word2id[word])\n",
        "        else:\n",
        "            word_ids.append(len(word2id))\n",
        "\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        \"\"\"\n",
        "        [I]\n",
        "        태깅을 수행하기 위해 텐서 그래프를 생성합니다.\n",
        "        \"\"\"\n",
        "        dense_word_ids = tf.constant(word_ids)\n",
        "        lengths = tf.constant(len(word_ids))\n",
        "        # Insert batch dimension.\n",
        "        dense_word_ids = tf.expand_dims(dense_word_ids, axis=0)\n",
        "        lengths = tf.expand_dims(lengths, axis=0)\n",
        "\n",
        "        with tf.variable_scope(\"build_graph\", reuse=tf.AUTO_REUSE):\n",
        "            logits = build_graph(dense_word_ids, lengths, id2word, id2label)\n",
        "        predictions = tf.argmax(logits, axis=1)\n",
        "\n",
        "        \"\"\"\n",
        "        [J]\n",
        "        저장된 모델을 로드하고, 데이터를 입력하여 태깅 결과를 얻습니다.\n",
        "        \"\"\"\n",
        "        saver = tf.train.Saver()\n",
        "        saver.restore(sess, saved_file)\n",
        "        pred_val = sess.run(\n",
        "            [predictions]\n",
        "        )[0]\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    [K]\n",
        "    태깅 결과를 출력합니다.\n",
        "    \"\"\"\n",
        "    pred_str = [id2label[i] for i in pred_val]\n",
        "    for word, tag in zip(sentence, pred_str):\n",
        "        print(\"%s[%s]\" %(word, tag), end=' ')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5_B0m2rG-Dq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "9a5a4921-0704-4f1f-a81d-7aad7f9a3a65"
      },
      "source": [
        "load_and_predict(\"/content/saves/model.ckpt-5622\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a sentence: where is my book?\n",
            "Building graph for model: sequence tagger\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-07-31 06:32:18,343 | INFO     | Restoring parameters from /content/saves/model.ckpt-5622\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "where[O] is[O] my[O] book[O] ?[O] "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpeLtVw6udSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}