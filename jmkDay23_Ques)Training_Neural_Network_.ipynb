{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jmkDay23_Ques)Training_Neural_Network_.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "FfWtUiir7XAK",
        "hxqP1tY965JF",
        "5YCjGFh77xeq",
        "xxghrniJ_JIm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minkyoJang/AI-BigDataStudy/blob/master/jmkDay23_Ques)Training_Neural_Network_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HjOvHCGI4FS",
        "colab_type": "text"
      },
      "source": [
        "# Training Neural Network 실습 \n",
        "### Copyright (C) 2018  Cheonbok Park <cb_park@korea.ac.kr>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oOxnJsphGb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn # 뉴럴네트워크는 이 nn 안에 있어요. 리니얼, rnn등 다양한거 안에 있어요. 이미 생성된거 사용하여 편리. \n",
        "import torch.nn.functional as F #tanh, relu, 일반적 파이토치 위한 뉴럴 function. 활성화 함수들이 여기에 있어요. \n",
        "import torchvision # 이미지 관련 처리, Pretrained Model 관련된 Package 입니다. \n",
        "import torchvision.datasets as vision_dsets #이미 주어진 데이터 셋 사용. 가지고 있는 일반적인 비전 데이터 셋을 넣어둔 것. \n",
        "import torchvision.transforms as T # 이미지 처리 (Vison) 관련된 transformation이 정의 되어 있습니다. 기본적 이미지는 RGB (0~256)인데 이건 값이 크니까 노말라이제이션해줘. 이거 해주는게 트랜스폼. \n",
        "import torch.optim as optim # pytorch 에서 정의한 수 많은 optimization function 들이 들어 있습니다. (옵티마이저. 아담!)\n",
        "from torch.utils import data # "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opjZGVu6mn_f",
        "colab_type": "text"
      },
      "source": [
        "# Process\n",
        "- 런타임 유형: 하드웨어 유형 gpu로 변경하여 빨리 처리하게 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlMBDh5JmlHv",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ABEFSDDZ8ON",
        "colab_type": "text"
      },
      "source": [
        "# MNIST Feed-forward Neural Network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qije6EXLt9sZ",
        "colab_type": "text"
      },
      "source": [
        "## Data Loader 불러오기\n",
        "- 0과 1로 노말레이제이션 하셨어요\n",
        "- 최대값1 최소0 민맥스하여 트랜스폼투 텐서\n",
        "- 셔플링 필수** \n",
        "    - 100개 중에 10개 뽑을때 shuffle하면 순서가 바뀜. \n",
        "    - 트레이닝할땐 다양하게 돌려서 object function나와야 해서 shuffe=True\n",
        "- 데이터 로더와 셋을 받습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo1YYiybty5e",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def MNIST_DATA(root='./',train =True,transforms=None ,download =True,batch_size = 32,num_worker = 1):\n",
        "\n",
        "\tprint (\"[+] Get the MNIST DATA\")\n",
        "\t\"\"\"\n",
        "  \ttorchvision.dataset 에는 우리가 많이 사용하는 데이터들을 쉽게 사용할 수 있도록 되어 있습니다. \n",
        "  \tMachine Learning 에서 Hello world 라고 불리는 Mnist 데이터를 사용해 보겠습니다. \n",
        "  \n",
        "  \n",
        "\t\"\"\"\n",
        "\tmnist_train = vision_dsets.MNIST(root = root,  #root 는 데이터의 저장 위치 입니다. \n",
        "\t\t\t\t\t\t\t\t\ttrain = True, #Train 은 이 데이터가 train 데이터인지 아닌지에 대한 정보입니다. \n",
        "\t\t\t\t\t\t\t\t\ttransform = T.ToTensor(), # 얻어낸 데이터를 pytorch가 계산 할 수 있는 Tensor 로 변환해 줍니다. \n",
        "\t\t\t\t\t\t\t\t\tdownload = True)  # 데이터를 다운로드 할지 여부를 물어봅니다. \n",
        "\tmnist_test = vision_dsets.MNIST(root = root,\n",
        "\t\t\t\t\t\t\t\t\ttrain = False,  # Test Data를 가져오기에 Train =False 를 줘야 합니다. \n",
        "\t\t\t\t\t\t\t\t\ttransform = T.ToTensor(),\n",
        "\t\t\t\t\t\t\t\t\tdownload = True)\n",
        "\t\"\"\"\n",
        "  \tData Loader 는 데이터와 batch size의 정보를 바탕으로 매 iteration 마다 주어진 데이터를 원하는 batch size 만큼 반환해주는 iterator입니다. \n",
        "  \t* Practical Guide : Batch size 는 어느정도가 좋나요? -- 클 수록 좋다는 소리가 있습니다. 하지만 gpu memeory 사이즈 한계에 의해 기본적으로 batch size 가 \n",
        "  \t커질 수록 학습에 사용되는 gpu memory 사이즈가 큽니다. (Activation map을 저장해야 하기 때문입니다.) 기본적으로 2의 배수로 저장하는 것이 좋습니다.(Bit size 관련) \n",
        "  \n",
        "\t\"\"\"\n",
        "\ttrainDataLoader = data.DataLoader(dataset = mnist_train,  # DataSet은 어떤 Data를 제공해 줄지에 대한 정보입니다. 여기서는 Training DATA를 제공합니다. \n",
        "\t\t\t\t\t\t\t\t\tbatch_size = batch_size, # batch size 정보를 꼭 줘야 합니다. 한 Batch 당 몇 개의 Data 를 제공할지에 대한 정보입니다. \n",
        "\t\t\t\t\t\t\t\t\tshuffle =True, # Training의 경우 Shuffling 을 해주는 것이 성능에 지대한 영향을 끼칩니다. 꼭 True 를 줘야 합니다. \n",
        "\t\t\t\t\t\t\t\t\tnum_workers = 1) # num worker의 경우 데이터를 로드하는데 worker를 얼마나 추가하겠는가에 대한 정보입니다. \n",
        "\n",
        "\ttestDataLoader = data.DataLoader(dataset = mnist_test, # Test Data Loader 이므로 Test Data를 인자로 전달해줍니다.\n",
        "\t\t\t\t\t\t\t\t\tbatch_size = batch_size, # 마찬가지로 Batch size 를 넣어줍니다. \n",
        "\t\t\t\t\t\t\t\t\tshuffle = False, # shuffling 이 굳이 필요하지 않으므로 false를 줍니다. \n",
        "\t\t\t\t\t\t\t\t\tnum_workers = 1) #\n",
        "\tprint (\"[+] Finished loading data & Preprocessing\")\n",
        "\treturn mnist_train,mnist_test,trainDataLoader,testDataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P2oJog_xnFJ",
        "colab_type": "code",
        "outputId": "3593e584-d42e-405b-faef-6a39ecf86839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "trainDset,testDset,trainDataLoader,testDataLoader= MNIST_DATA(batch_size = 32)  # Data Loader 를 불러 옵니다. "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[+] Get the MNIST DATA\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 8928217.07it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 142244.69it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2566399.06it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 53452.98it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "[+] Finished loading data & Preprocessing\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtWA2FTIxskN",
        "colab_type": "text"
      },
      "source": [
        "## Train Function \n",
        "- (N,RGB, width, weight)\n",
        "- (N, 1, 28,28)\n",
        "\n",
        "- 에폭\n",
        "    - 모든 데이터 돌때 하나의 에폭을 돌았다고 해요\n",
        "    - 미니배치로 업데이트 하는 것== 이터레이션\n",
        "    - 총 100개의 데이터 중에서 10번의 미니배치 사이즈를 하고 한번 에폭돌때는 몇 번의 이터레이션 돈거야? 10개\n",
        "- 뎁스. \n",
        "    - RGB는 3, 일차원 색상은 1\n",
        "    - cpu>gpu에 올려줘야해. 현재 우리 gpu로 작동하게 했으니\n",
        "        - 그래서 cuda로 올려줘\n",
        "- 옵티마이저: 그래디언트 값을 바탕으로 모델 웨이트 올려\n",
        "    - 모델의 파라미터 가지고 있어서 처음엔 zero_grad해야해\n",
        "        - 백프라그래하면 모델안에 저장이 되니까 축적되어 메모리 커짐.\n",
        "        - 버퍼 플러쉬\n",
        "- 뉴럴에 인풋넣고 아웃풋 내는데 이 아웃풋이 프레딕트\n",
        "- 이 아웃푹과 정답label 비교\n",
        "- 크리터리언으로 로스 비교. \n",
        "    - 크로스 엔트로피 로스를 사용할거에요. 클래시피케이션이니까. \n",
        "- 백워드 연산 실행\n",
        "    - 로스를 바탕으로 백프로시작을 해. \n",
        "    - 순차적으로 각 파라미터에 그라디언트 구할 수 있음. \n",
        "- 옵티마이저.스텝\n",
        "    - 가각 그래디언트에 대한 웨이트 업데이트하는 과정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA8glX3ox0Hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_network(net,optimizer,trainloader):\n",
        "  for epoch in range(4):  # loop over the dataset multiple times\n",
        "\n",
        "      running_loss = 0.0 # running loss를 저장하기 위한 변수입니다. \n",
        "      for i, data in enumerate(trainloader, 0): # 한 Epoch 만큼 돕니다. 매 iteration 마다 정해진 Batch size 만큼 데이터를 뱉습니다. \n",
        "          # get the inputs\n",
        "          inputs, labels = data # DataLoader iterator의 반환 값은 input_data 와 labels의 튜플 형식입니다. \n",
        "          inputs = inputs.cuda() # gpu에 데이터를 올립니다.\n",
        "          labels = labels.cuda()\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()    #  현재 기존의 backprop을 계산하기 위해서 저장했던 activation buffer 를 비웁니다. Q) 이걸 안 한다면?\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = net(inputs) # input 을 넣은 위 network 로 부터 output 을 얻어냅니다. \n",
        "          loss = criterion(outputs, labels) # loss fucntion에 주어진 target과 output 의 score를 계산하여 반환합니다. \n",
        "          loss.backward() # * Scalar Loss value를 Backward() 해주게 되면 주어진 loss값을 바탕으로 backpropagation이 진행됩니다. \n",
        "          optimizer.step() # 계산된 Backprop 을 바탕으로 optimizer가 gradient descenting 을 수행합니다. \n",
        "\n",
        "          # print statistics\n",
        "          running_loss += loss.item()\n",
        "          if i % 500 == 499:    # print every 2000 mini-batches\n",
        "              print('[%d, %5d] loss: %.3f' %\n",
        "                    (epoch + 1, i + 1, running_loss / 500))\n",
        "              running_loss = 0.0\n",
        "\n",
        "  print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGMkGKQiy8_r",
        "colab_type": "text"
      },
      "source": [
        "## Test Function\n",
        "- 주의) model.eval\n",
        "    - 실수하지 말아야 할 것! model.eval\n",
        "        - 트레이닝 플레그를 false로 해야해요\n",
        "        - 트레이닝 중인지 아니면 이벨인지에 대한 플레그를 갖고 있음. \n",
        "        - 모델(점)이발로 돌아오면 우리가 사용중인게 배치노말라이제이션 및 배치라고 하자. \n",
        "            - 트레이닝과 테스트일때 저 둘은 다르죠\n",
        "            - 플래그를 넣어줘야해요. 그래서 이발은 테스트 모델일때 드랍 아웃 꺼주고 했던거죠***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0Xn-H2zbc22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model,test_loader):\n",
        "  model.eval() # Eval Mode 왜 해야 할까요?  --> nn.Dropout BatchNorm 등의 Regularization 들이 test 모드로 들어가게 되기 때문입니다. \n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  for data, target in test_loader:\n",
        "    data, target = data.cuda(), target.cuda()  # 기존의 train function의 data 처리부분과 같습니다. \n",
        "    output = model(data) \n",
        "    pred = output.max(1, keepdim=True)[1] # get the index of the max \n",
        "    correct += pred.eq(target.view_as(pred)).sum().item() # 정답 데이터의 갯수를 반환합니다. \n",
        "\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('\\nTest set:  Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "      correct, len(test_loader.dataset),\n",
        "      100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdM-bsXD152f",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network  + Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmv3W0Ln29M1",
        "colab_type": "text"
      },
      "source": [
        "## 간단한 Neural Network 를 만들어 봅시다. (1)\n",
        "특징 : 2개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Sigmoid \n",
        "\n",
        "Layer 2 - input: 30 output:10\n",
        "          - 모델의 클래스갯수가 0에서 9니까 총 10개 그래서 10개 되야함\n",
        "\n",
        "Cross Entropy Loss  + SGD optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTJOsz9U2FnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__() # nn.Module 생성자 호출 Q) 왜 필요할까요?\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,30) # 리니어에서 인풋넣고, 아웃풋나오게 (인풋, 아웃풋)\n",
        "        self.fc1 = nn.Linear(30, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # view는 reshape 함수와 동일. 리니어를 하려면 하나의 디맨젼으로 쭉 펴줘야 하니까. -1은 자동으로 숫자 맞춰주라는 의미.\n",
        "        # reshape=28*28> 30으로 리니어를 하고, 넌리니얼해줘야해(시그모이드)f.시그모이드로 액티베이션하여 시그모이드 거치게. \n",
        "        x = x.view(-1,28*28) # x.view함수는 주어진 인자의 크기로 해당 데이터의 크기를 반환합니다. 즉, (Batch_size,28,28) --> (Batch_size,28*28)로 변환합니다.\n",
        "        x = F.sigmoid(self.fc0(x)) # 28*28 -> 30 -> Activation function 을 수행합니다. 즉, 레잉어 1번에 넣어서 아웃풋 30개 나오게 해 \n",
        "        x = self.fc1(x)  # 30 -> 10 으로 10개의 Class에 대한 logit 값을 호출합니다. \n",
        "        return x\n",
        "            # 로짓: 하나의 고양이(-2.7),개(2)등을 classification하게 된다면 softmax해줘야하죠. 그럼 고양이 0.01 개 0.99로 바꾸기 직전이 소프트 맥스."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvk4TTL83pcc",
        "colab_type": "text"
      },
      "source": [
        "#### Optimizer \n",
        "Optimizer 의 경우 기본적으로 torch.optim 안에 존재합니다. 다양한 optimziers 가 정의되어 있습니다. \n",
        "\n",
        "기본적으로 다음과 같은 구성을 따릅니다. optim.{Optimzier 이름}({Network Parameters},lr ={learning rate })"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd5tmyMnXcs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. 포인터로 가리켜서 그 그레디언트 기반으로 업데이트하려고 파라미터 줌. \n",
        "criterion = nn.CrossEntropyLoss() #소프트맥스한걸 아래에 주면 안됩니다.** 로짓을 넘겨야 해요******@@\n",
        "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다.  Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. 이 안에 소프트 맥스가 있음. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zALyUje-aehn",
        "colab_type": "code",
        "outputId": "f0931fa8-c880-4cfd-98f0-e29bc432000d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader) # 4 Epoch 정도 학습을 진행해봅니다. "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 2.309\n",
            "[1,  1000] loss: 2.292\n",
            "[1,  1500] loss: 2.276\n",
            "[2,   500] loss: 2.252\n",
            "[2,  1000] loss: 2.240\n",
            "[2,  1500] loss: 2.226\n",
            "[3,   500] loss: 2.204\n",
            "[3,  1000] loss: 2.190\n",
            "[3,  1500] loss: 2.177\n",
            "[4,   500] loss: 2.149\n",
            "[4,  1000] loss: 2.131\n",
            "[4,  1500] loss: 2.115\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfQrMIX2a3nn",
        "colab_type": "code",
        "outputId": "d365bdeb-a887-43d0-d458-3ba76a14bf41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "test(mnist_net,testDataLoader) # Test 정확도를 출력해 봅니다. "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 5958/10000 (60%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUK2GbHZ4x-K",
        "colab_type": "text"
      },
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (2)\n",
        "특징 : 2개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - tanh \n",
        "\n",
        "Layer 2 - input: 30 output:10\n",
        "\n",
        "Cross Entropy Loss  + SGD optimizer \n",
        "\n",
        "- INIT은 레고 만들고\n",
        "_ FORWARD로 만든다 생각**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZUmc76ZdOap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__() # nn.Module 생성자 호출 Q) 왜 필요할까요?\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,30)\n",
        "        self.fc1 = nn.Linear(30, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1,28*28) # x.view함수는 주어진 인자의 크기로 해당 데이터의 크기를 반환합니다. 즉, (Batch_size,28,28) --> (Batch_size,28*28)로 변환합니다.\n",
        "        x = F.tanh(self.fc0(x)) # 28*28 -> 30 -> Activation function 을 수행합니다.\n",
        "        x = self.fc1(x)  # 30 -> 10 으로 10개의 Class에 대한 logit 값을 호출합니다. \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBXketXnhA_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxYvKdt6tAn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qb3e4wqphDmZ",
        "colab_type": "code",
        "outputId": "0d450ba2-f302-4a25-d23d-032d16c1d21e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 2.222\n",
            "[1,  1000] loss: 2.067\n",
            "[1,  1500] loss: 1.924\n",
            "[2,   500] loss: 1.674\n",
            "[2,  1000] loss: 1.552\n",
            "[2,  1500] loss: 1.437\n",
            "[3,   500] loss: 1.256\n",
            "[3,  1000] loss: 1.176\n",
            "[3,  1500] loss: 1.107\n",
            "[4,   500] loss: 0.993\n",
            "[4,  1000] loss: 0.941\n",
            "[4,  1500] loss: 0.907\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ-WCpKrhEzR",
        "colab_type": "code",
        "outputId": "de4d32d3-f0b5-4235-b1e5-d7c228b13dc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 8327/10000 (83%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV_aGry05TIV",
        "colab_type": "text"
      },
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (3)\n",
        "특징 : 2개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Relu\n",
        "\n",
        "Layer 2 - input: 30 output:10\n",
        "\n",
        "Cross Entropy Loss  + SGD optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvAhsyP-hbwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,30)\n",
        "        self.fc1 = nn.Linear(30, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = x.view(-1,28) #reshape\n",
        "        x = F.relu(self.fc0(x))\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZVpqqCbhiNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKGyPdSIhmqx",
        "colab_type": "code",
        "outputId": "c5b44398-63e1-48da-8c14-6cae40a04aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7453c9d1376b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainDataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-ba8e3681a903>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(net, optimizer, trainloader)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# input 을 넣은 위 network 로 부터 output 을 얻어냅니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# loss fucntion에 주어진 target과 output 의 score를 계산하여 반환합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# * Scalar Loss value를 Backward() 해주게 되면 주어진 loss값을 바탕으로 backpropagation이 진행됩니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-78297be28d9f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# If the size is a square you can only specify a single number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#reshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1407\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [896 x 28], m2: [784 x 30] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:268"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8unN7zOBtmHj",
        "colab_type": "text"
      },
      "source": [
        "# sigmoid\n",
        "- 지그재그 (0~1)\n",
        "# tanh\n",
        "- -1~1\n",
        "\n",
        "#relu \n",
        "- 0또는 1이라 빠르다. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_tEaqWlhnUQ",
        "colab_type": "code",
        "outputId": "7905d63d-f7dd-459d-dcb7-a159f6967057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 8473/10000 (85%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfWtUiir7XAK",
        "colab_type": "text"
      },
      "source": [
        "### Q) 성능차이가 존재하나요? 존재한다면 무슨 이유일까요?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQALVsvI5ful",
        "colab_type": "text"
      },
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (4) \n",
        "특징 : 3개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 40 + Activation Fucntion - sigmoid \n",
        "\n",
        "Layer 2 - input: 40 output: 30\n",
        "\n",
        "Layer 3 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + SGD optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLEQ6RZXjOCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        self.fc0 = nn.Linear(28*28,40) # Layer 1\n",
        "        self.fc1 = nn.Linear(40, 30) # Layer 2\n",
        "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.sigmoid(self.fc0(x))# 시그모이드 레이어 3개 쌓았더니(아깐2개) 11프로 나왔네. \n",
        "        x = F.sigmoid(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MM_WIY6jVG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kWON9jtjXAy",
        "colab_type": "code",
        "outputId": "719f84d8-9f67-45d0-b946-19b7e0c02091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader) #"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 2.317\n",
            "[1,  1000] loss: 2.308\n",
            "[1,  1500] loss: 2.302\n",
            "[2,   500] loss: 2.299\n",
            "[2,  1000] loss: 2.300\n",
            "[2,  1500] loss: 2.299\n",
            "[3,   500] loss: 2.299\n",
            "[3,  1000] loss: 2.298\n",
            "[3,  1500] loss: 2.297\n",
            "[4,   500] loss: 2.297\n",
            "[4,  1000] loss: 2.297\n",
            "[4,  1500] loss: 2.297\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaCuXaHWjXzH",
        "colab_type": "code",
        "outputId": "a91a46c4-726f-4658-e900-abaad4b4ed21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 1135/10000 (11%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxqP1tY965JF",
        "colab_type": "text"
      },
      "source": [
        "### Q) 학습이 잘 되나요???? 안 된다면 왜 안될까요?\n",
        "- 시그모이드 레이어 3개 쌓았더니(아깐2개) 11프로 나왔네. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZIkQe9f6tdy",
        "colab_type": "text"
      },
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (5) \n",
        "특징 : 3개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 40 + Activation Fucntion - Relu \n",
        "\n",
        "Layer 2 - input: 40 output: 30\n",
        "\n",
        "Layer 3 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + SGD optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uHrf8bRjZFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,40) #Layer 1 \n",
        "        self.fc1 = nn.Linear(40, 30) # Layer 2\n",
        "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.fc0(x)) # Layer 1\n",
        "        x = F.relu(self.fc1(x)) # Layer 2\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEtXES71j9s6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.SGD(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxHiSRSskasK",
        "colab_type": "code",
        "outputId": "2046b646-9f2f-49c6-c4db-3d34dee759f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 2.305\n",
            "[1,  1000] loss: 2.295\n",
            "[1,  1500] loss: 2.284\n",
            "[2,   500] loss: 2.258\n",
            "[2,  1000] loss: 2.240\n",
            "[2,  1500] loss: 2.211\n",
            "[3,   500] loss: 2.146\n",
            "[3,  1000] loss: 2.096\n",
            "[3,  1500] loss: 2.040\n",
            "[4,   500] loss: 1.918\n",
            "[4,  1000] loss: 1.813\n",
            "[4,  1500] loss: 1.713\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0woJ7sbzkbqj",
        "colab_type": "code",
        "outputId": "dd83d666-f2b4-4241-8665-9913a8ba8438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 6494/10000 (65%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YCjGFh77xeq",
        "colab_type": "text"
      },
      "source": [
        "#### (4)와 차이가 존재하나요? 그렇다면 왜 그럴까요? (3) 이랑은 비교해보면 어떻나요? \n",
        "- 현재 렐루로 하니까 아까보다 loss 느리긴 하는데 많이 빠르진 않아요.\n",
        "- 오티마이저보다 좋은거있쬬 디폴트로 생각하라고. \n",
        "    - 아담. 아담엔 모멘텀+아레베스플라빗(캐쉬)있다 생각하면ㄷ ㅙ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezH4C21Y8JF4",
        "colab_type": "text"
      },
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (6) \n",
        "특징 : 3개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 40 + Activation Fucntion - Relu \n",
        "\n",
        "Layer 2 - input: 40 output: 30\n",
        "\n",
        "Layer 3 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + **Adam** optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-oDSJBL8Wk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,40) #Layer 1 \n",
        "        self.fc1 = nn.Linear(40, 30) # Layer 2\n",
        "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.fc0(x)) # Layer 1\n",
        "        x = F.relu(self.fc1(x)) # Layer 2\n",
        "        x = self.fc2(x) # Layer 3 \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJEAkxqX8YWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVGBPs3W8dRd",
        "colab_type": "code",
        "outputId": "bc9e35c9-1758-4666-efca-66b641a87d5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.684\n",
            "[1,  1000] loss: 0.299\n",
            "[1,  1500] loss: 0.260\n",
            "[2,   500] loss: 0.199\n",
            "[2,  1000] loss: 0.188\n",
            "[2,  1500] loss: 0.170\n",
            "[3,   500] loss: 0.137\n",
            "[3,  1000] loss: 0.140\n",
            "[3,  1500] loss: 0.140\n",
            "[4,   500] loss: 0.111\n",
            "[4,  1000] loss: 0.108\n",
            "[4,  1500] loss: 0.111\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKcDCwPJ8dqP",
        "colab_type": "code",
        "outputId": "527d7b6e-e833-4bce-8b64-22e64db4cc44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9661/10000 (97%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7_NK6ue8t6z",
        "colab_type": "text"
      },
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (7) Layer 를 줄여볼까요? \n",
        "특징 : 2개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Relu \n",
        "\n",
        "Layer 2 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + **Adam** optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjnnxkiG8slU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,30) #Layer 1 \n",
        "        self.fc1 =  nn.Linear(30, 10) # Layer 2\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.fc0(x)) # Layer 1\n",
        "        x = self.fc1(x) # Layer 2\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQea1DZS8s2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wPfgH1S8tFP",
        "colab_type": "code",
        "outputId": "5111fc95-dd8f-4aa9-8c50-c18f2df1af8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.685\n",
            "[1,  1000] loss: 0.324\n",
            "[1,  1500] loss: 0.284\n",
            "[2,   500] loss: 0.240\n",
            "[2,  1000] loss: 0.224\n",
            "[2,  1500] loss: 0.222\n",
            "[3,   500] loss: 0.188\n",
            "[3,  1000] loss: 0.191\n",
            "[3,  1500] loss: 0.177\n",
            "[4,   500] loss: 0.156\n",
            "[4,  1000] loss: 0.163\n",
            "[4,  1500] loss: 0.148\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8vW8d-18-Ix",
        "colab_type": "code",
        "outputId": "0e02877c-f0be-4389-e006-c75f852ecc60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9521/10000 (95%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKbhGSix9UQM",
        "colab_type": "text"
      },
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (7) Batch Norm 을 줘 볼까요?***********\n",
        "- 2디와 1디 있어요.\n",
        "- 평균을 빼고 배리언스로 나눠서 정규화 시키고 스케일링과 시프팅 있어요\n",
        "- 배치넘은 인풋 그대로 알려주고 하면ㄷ ㅙ\n",
        "\n",
        "특징 : 2개의 Layer를 가지는 Neural Network \n",
        "<구성>  \n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Relu  + Batch Norm\n",
        "\n",
        "Layer 2 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + **Adam** optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpakqWPv9cs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,30) #Layer 1\n",
        "        self.bn0 = nn.BatchNorm1d(30) # BatchNorm . 리니어 거치고 배ㅣ 넣을거야\n",
        "        self.fc1 =  nn.Linear(30, 10) # Layer 2\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.bn0(self.fc0(x))) # Layer 1.액티베이션 이전에만 배치넘 넣는건 아니야. 상황보고 넣으면 됩니다. \n",
        "        x = self.fc1(x) # Layer 2\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suyt84BXHzE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHANHwQT9c6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el8EvYky9dgz",
        "colab_type": "code",
        "outputId": "3f5c66a8-c9bc-483a-894e-079b16b9d355",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.893\n",
            "[1,  1000] loss: 0.370\n",
            "[1,  1500] loss: 0.301\n",
            "[2,   500] loss: 0.238\n",
            "[2,  1000] loss: 0.231\n",
            "[2,  1500] loss: 0.209\n",
            "[3,   500] loss: 0.183\n",
            "[3,  1000] loss: 0.176\n",
            "[3,  1500] loss: 0.176\n",
            "[4,   500] loss: 0.154\n",
            "[4,  1000] loss: 0.151\n",
            "[4,  1500] loss: 0.159\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIUIeYYe9rid",
        "colab_type": "code",
        "outputId": "629d61b8-c218-483b-d078-ef1e3d304eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9601/10000 (96%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBTuBB9_93mq",
        "colab_type": "text"
      },
      "source": [
        "### 간단한 Neural Network 를 만들어 봅시다. (8) 더 깊은 레이어에 Batch Norm 을 줘 볼까요?\n",
        "- 리니어로 98~99프로 되면 거의 되고\n",
        "- 컨벌류션 잘 주면 99프로 찍어요\n",
        "- 엠니스트는 파이썬 계의 헬로우 월드. 잘 학습 되고요. \n",
        "\n",
        "특징 : 2개의 Layer를 가지는 Neural Network\n",
        "\n",
        "<구성>  \n",
        "\n",
        "Layer 1 - input:28*28 , output : 40 + Activation Fucntion - Relu + BatchNorm\n",
        "\n",
        "Layer 2 - input: 40 output: 30 + Activation Fucntion - Relu  + BatchNorm\n",
        "\n",
        "Layer 3 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss  + **Adam** optimizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgGaaFW8-Ny7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc0 = nn.Linear(28*28,40) #Layer 1 \n",
        "        self.bn0 = nn.BatchNorm1d(40) #BatchNorm1 \n",
        "        self.fc1 = nn.Linear(40, 30) # Layer 2\n",
        "        self.bn1 = nn.BatchNorm1d(30) #BatchNorm1 \n",
        "        self.fc2 = nn.Linear(30, 10) # Layer 3\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.bn0(self.fc0(x))) # Layer 1\n",
        "        x = F.relu(self.bn1(self.fc1(x))) # Layer 2\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClLHQwZE-OCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxHuM-IF-nz2",
        "colab_type": "code",
        "outputId": "f9ceb39f-c4a1-4cc0-e1d8-4630df8319bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.818\n",
            "[1,  1000] loss: 0.277\n",
            "[1,  1500] loss: 0.222\n",
            "[2,   500] loss: 0.160\n",
            "[2,  1000] loss: 0.159\n",
            "[2,  1500] loss: 0.151\n",
            "[3,   500] loss: 0.117\n",
            "[3,  1000] loss: 0.127\n",
            "[3,  1500] loss: 0.126\n",
            "[4,   500] loss: 0.110\n",
            "[4,  1000] loss: 0.116\n",
            "[4,  1500] loss: 0.107\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK1kspbG-qB9",
        "colab_type": "code",
        "outputId": "f3ca70b4-dc8f-4b28-aac6-a45c36ee1fce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9707/10000 (97%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxghrniJ_JIm",
        "colab_type": "text"
      },
      "source": [
        "#### Batch Normalization 을 적용한 (7)과 (6)을 비교해보고 (8) 과 (5)를 비교해보면 어떻나요? 학습이 어떻게 달라졌을까요? \n",
        "\n",
        "## 왜 레이어 깊어질수록 성능이 좋아지나\n",
        "- 좀더 컴플렉스한거 마들어서\n",
        "\n",
        "## 배치넘하면 성능이 좋아지는 이유?\n",
        "- 학습이 안정화되어 동 이터레이션 대비 성능 향상되는 것\n",
        "\n",
        "#### 배치넘은 트레이닝을 안정화시켜서 빠른 성능 도달하게 하고, 깊은 레이어일수록 학습을 더 깊게 시킬 수 있다. 따라서 깊은 레이어일수록 배치넘으로 안정화 시킬 수 있을 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGwSFl9v_c4F",
        "colab_type": "text"
      },
      "source": [
        "### Let's Do it - 성능을 한번 끝까지 높여볼까요~? 마음대로 한번 최고 성능을 찍어봅시다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjM6p0gvkccx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        \n",
        "        self.fc0 = nn.Linear(28*28,16*5*5)\n",
        "        self.bn0 = nn.BatchNorm1d(??)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 84)\n",
        "        self.bn1 = nn.BatchNorm1d(??)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = x.view(-1,28*28)\n",
        "        x = F.relu(self.bn0(self.fc0(x)))\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGsCsD6blqBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wPzE5N0lq-Y",
        "colab_type": "code",
        "outputId": "98837cb0-c649-4632-aa48-60cb54de0e18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.469\n",
            "[1,  1000] loss: 0.182\n",
            "[1,  1500] loss: 0.149\n",
            "[2,   500] loss: 0.105\n",
            "[2,  1000] loss: 0.100\n",
            "[2,  1500] loss: 0.096\n",
            "[3,   500] loss: 0.062\n",
            "[3,  1000] loss: 0.070\n",
            "[3,  1500] loss: 0.071\n",
            "[4,   500] loss: 0.049\n",
            "[4,  1000] loss: 0.056\n",
            "[4,  1500] loss: 0.056\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhuKrwYElsAj",
        "colab_type": "code",
        "outputId": "7f4b393e-c283-47f5-9f07-a07a5fc7cdea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9791/10000 (98%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxip6TQc_txE",
        "colab_type": "text"
      },
      "source": [
        "## Practical Guide Pytorch nn.Sequential "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDexfQHd_72G",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "x = F.relu(self.bn0(self.fc0(x)))\n",
        "x = F.relu(self.bn1(self.fc1(x)))\n",
        "```\n",
        "너무 복잡하지 않나요?  그냥 x = self.fc(x) 쉽게 해버리면 안 될까요?\n",
        "- 큰 레이어 하나마다 블록을 만들어두고, 리니어 군집을 만들고.\n",
        "- 레이어는 시퀀스하게 가기에 어떠한 리스트에 순서를 정합니다. \n",
        "\n",
        "Solution : nn.Sequential + 자매품 nn.ModuList\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59l4rpz5ls39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        \n",
        "        layer_list = [] # 이 리스트에 모든 Layer 를 순차적으로 append 해보겠습니다. \n",
        "        layer_list.append(nn.Linear(28*28,40)) #Layer 1 \n",
        "        layer_list.append(nn.ReLU())# activation function\n",
        "        layer_list.append(nn.BatchNorm1d(40))#BatchNorm1 \n",
        "        layer_list.append(nn.Linear(40, 30)) # Layer 2\n",
        "        layer_list.append(nn.ReLU())# activation function\n",
        "        layer_list.append(nn.BatchNorm1d(30)) #BatchNorm1 \n",
        "        layer_list.append(nn.Linear(30, 10)) # Layer 3\n",
        "        self.net  = nn.Sequential(*layer_list) # nn.Sequential 에 layer list를 넘겨 줍니다.시퀀스 패키지에 전달하면 이 리스트 순서대로 구성된 뉴럴 네트워크  레이어를 만들 수 있어요\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,28*28) #리쉐입한거 넣으면 아웃풋이 나와요 \n",
        "        x = self.net(x) # 넣은 순서대로 적용이 됩니다. \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRRIS8y1mOOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF5Tr8dXBMmg",
        "colab_type": "code",
        "outputId": "b8ccce20-a568-42c5-fc5c-6be65293126d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.705\n",
            "[1,  1000] loss: 0.275\n",
            "[1,  1500] loss: 0.217\n",
            "[2,   500] loss: 0.164\n",
            "[2,  1000] loss: 0.157\n",
            "[2,  1500] loss: 0.167\n",
            "[3,   500] loss: 0.131\n",
            "[3,  1000] loss: 0.144\n",
            "[3,  1500] loss: 0.137\n",
            "[4,   500] loss: 0.113\n",
            "[4,  1000] loss: 0.120\n",
            "[4,  1500] loss: 0.120\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7o8XOAlBSFB",
        "colab_type": "code",
        "outputId": "a14b930f-f04c-4928-e9de-1a424f7759e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9680/10000 (97%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysv7uX-1Dh0b",
        "colab_type": "text"
      },
      "source": [
        "#### 연습해 봅시다 ! \n",
        "\n",
        "특징 : 2개의 Layer를 가지는 Neural Network <구성>\n",
        "\n",
        "Layer 1 - input:28*28 , output : 30 + Activation Fucntion - Relu + Batch Norm\n",
        "\n",
        "Layer 2 - input: 30 output : 10\n",
        "\n",
        "Cross Entropy Loss + Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-esUcJ8VDu_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNIST_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNIST_Net, self).__init__()\n",
        "        \n",
        "        layer_list = [] # 이 리스트에 모든 Layer 를 순차적으로 append 해보겠습니다. \n",
        "        layer_list.append(nn.??(??,??)) #Layer 1 \n",
        "        layer_list.append(nn.??())# ReLU activation function\n",
        "        layer_list.append(nn.??(??)) #BatchNorm1 \n",
        "        layer_list.append(nn.??(??, ??)) # Layer 2\n",
        "        self.net  = nn.Sequential(*layer_list) # nn.Sequential 에 layer list를 넘겨 줍니다.\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = x.view(-1,??)\n",
        "        x = self.net(x) # 넣은 순서대로 적용이 됩니다. \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaktO6PvD0M4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_net = MNIST_Net().cuda() # 생성한 뉴럴넷 Instance를 생성하고 빠른 학습을 위해 cuda 에 올립니다. \n",
        "criterion = nn.CrossEntropyLoss() # Loss Function을 정의 합니다. 여기서는 cross entrophy loss 를 사용합니다. \n",
        "optimizer = optim.Adam(mnist_net.parameters(), lr=0.001) # optimizer는 이와 같이 training 할 Parameter와 learning rate를 인자로 줍니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9She_4cMD1ES",
        "colab_type": "code",
        "outputId": "0146b8cf-77b0-491f-c5f1-68b11a03f6d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "train_network(mnist_net,optimizer,trainDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   500] loss: 0.760\n",
            "[1,  1000] loss: 0.383\n",
            "[1,  1500] loss: 0.352\n",
            "[2,   500] loss: 0.330\n",
            "[2,  1000] loss: 0.329\n",
            "[2,  1500] loss: 0.324\n",
            "[3,   500] loss: 0.305\n",
            "[3,  1000] loss: 0.308\n",
            "[3,  1500] loss: 0.323\n",
            "[4,   500] loss: 0.301\n",
            "[4,  1000] loss: 0.302\n",
            "[4,  1500] loss: 0.313\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kseseeW4D1zc",
        "colab_type": "code",
        "outputId": "ccccb43b-17f0-4da2-aac3-e463a65901b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "test(mnist_net,testDataLoader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set:  Accuracy: 9204/10000 (92%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}