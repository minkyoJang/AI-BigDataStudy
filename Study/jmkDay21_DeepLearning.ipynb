{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NEURAL Networks]\n",
    "- 주재걸 교수님 과정 박사님_강경필님\n",
    "    - 데이터마이닝+추천시스템 공부하심"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Deep Learning\n",
    "- AI는 1950년대 부터 개발.\n",
    "    - 엄청 커다란 분야였으나, 컴퓨팅 시스템이 따라주지 않아 거의 흥미 위주의 것들만 있었음(체스 등)\n",
    "- 머신러닝은 1980년부터.\n",
    "    - 퍼스널 컴퓨터 보급\n",
    "    - SVM, RandomForest등이 개발되어 성능이 올라가기 시작함.\n",
    "- 딥러닝은 2010년 부터\n",
    "    - 진입장벽이 낮아짐. 파이썬 및 파이토치만 하면 어느정도 구현 할 수 있었음\n",
    "    - 연구적인 측면에서는 수학적인게 필요한데, 적용자체로는 코드 10 몇줄이면 될 정도임.\n",
    "    - 성능은 굉장히 좋아져서 실현이 안될만한 것들이 잘 되기 시작함\n",
    "        - 컴퓨팅 파워, 데이터\n",
    "        \n",
    "#### 기계학습 거장\n",
    "- Geoffrey Hinton, Yann LeCun, Andrew Ng, Yoshua Bengio\n",
    "    - 앤드류 응:LDA모델 개발, 강화학습에서 유명한 논문\n",
    "\n",
    "#### 추천 강의        \n",
    "- 코세라에ㅐ서강의 중 Machine Learning (Hinton, Ng)\n",
    "- CS231(박사님도 거의 3번은 보셨대)\n",
    "\n",
    "#### 딥러닝 분야\n",
    "- 감정 및 사람 인식\n",
    "- CCTV \n",
    "- Object Detection  \n",
    "- Image Captioning\n",
    "    - 이 사진이 어떤 것인지 문장을 생성해주는분야\n",
    "    - https://github.com/yunjey/show-attend-and-tell\n",
    "        - 사진 하나에 대해 한 문장/여러 문장\n",
    "- Image Generation/Translation\n",
    "    - 허위의 사람을 실제 사람처럼 생성 가능\n",
    "    - 인풋 그림에 대해 모네/반고흐/세잔 풍으로 그림 변경하는 task 진행중\n",
    "        - 네이버와 협업) 네이버 웹툰을 보면 작가가 흑백으로 스케치하고, 일일이 색칠하는데 이게 시간이 오래 걸림. 그래서 화가에 맞는 색칠을 지원해주기 위해 이 분야 연구 중\n",
    "\n",
    "- DeepFakes(DeepLearning+Fake)\n",
    "    - 동영상에 사운드까지 입혀서 가짜 뉴스를 생성\n",
    "        - 삼성 연구\n",
    "            - 사진 한 장만 주면 그걸 동영상으로 만들어 주는 기술. 현재는 3D 모델이 필요했으나 그럴 필요X\n",
    "- Sentiment Analysis\n",
    "    - 감정 분석\n",
    "        - 배달의 민족 리뷰가 얼만큼 긍정적인지 부정적인지\n",
    "        - 간단하지만 활용도가 높은 테스크\n",
    "            - 어느 부분은 긍정이고 부정인지 팔별도 가능함\n",
    "                - 예) 인테리어는 좋은데 '맛'은 별로더라. 이 맛에 대해 얼마나 긍부정적인지 키워드로 가능\n",
    "\n",
    "- QA\n",
    "    - 챗봇 기능. \n",
    "        - SK, NAVER등이 이미 도입\n",
    "\n",
    "\n",
    "- Medical Analysis\n",
    "    - 병원에서도 도입. \n",
    "    - 미국) 3-4년 전에 왓슨 적용, 국내) 백병원에서 왓슨 AI를 도입\n",
    "    \n",
    "\n",
    "- 추천 시스템\n",
    "    - 1프로 올려도 매출이 몇 백억 될 정도\n",
    "    \n",
    "- Game AI\n",
    "    - 로봇이 어떻게 움직일지\n",
    "    - 군대에서 인원을 어떻게 분배할 것인지. 전략을 어떻게 짤 것인지\n",
    "\n",
    "#### OpenAI\n",
    "- 유명한 사이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Perceptron\n",
    "- 인간의 뉴런을 카피했고, 인간의 뉴론에 대한 기본 단위가 perceptron\n",
    "- 뉴런: 신경세포가 굉장히 많이 있고 그것 하나가 주변의 신경세포와 엄청 연결이 되어 있음. \n",
    "\n",
    "\n",
    "### single Layer Perceptron\n",
    "- AND Gate 방식\n",
    "    - 하나의 펄셉트론으로 구현이 가능\n",
    "    - Linear Regression 1차 회귀 모양.y=-x+1.3 이런 식의 그래프 그려져 이거 바깥은 y가 1, 안쪽은 0\n",
    "- OR Gate 방식\n",
    "    - AND에서 가중치가 0.8이었다면, OR로는 0.3으로 변경하여 연산하면 됨.\n",
    "    - 그래서 연산 값들이 0보다 크거나 같으면 y값 1, 그렇지 않으면 0이 되게 함 \n",
    "    - .y=-x+0.8 의 그래프를 그리고 이 안에 있으면 y값 0, 이 그래프 바깥은 1로 결과 나오게 함\n",
    "    \n",
    "    \n",
    "### 이때 (0,1), (1,0), (0,0) (1,1)에 점이 찍혀있다면 위의 방식처럼 단순 선으로 표현이 불가\n",
    "- Linearly ... Problem\n",
    "==> 2개의 퍼셉트론으로 하자고 했어요\n",
    "    - y=-x+0.3, y=-x+1.3 이런식으로 2개 생성해서.\n",
    "    \n",
    "#### XOR\n",
    "- 리니어 세퍼-- 프로블럼을 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.NEURAL NETWORK\n",
    "- 핵 안에서는 '셀바디' 계산을 하고, 액티베이션 펑션을 적용하여 아웃풋으로 적용\n",
    "\n",
    "#### 구글 플레이그라운드 툴\n",
    "- https://playground.tensorflow.org\n",
    "- 재생 버튼: 학습 시작\n",
    "\n",
    "- 세타(1)\n",
    "- 세타(2)\n",
    "- Ai(j) : i번째에 있는 j번째 엑티베이션 펑션\n",
    "    - X0는 보통1\n",
    "    - 나중에 미분(그래디언트) 통해서 학습을 하게 됨 \n",
    "\n",
    "### ■ FORWARD PROPAGATION\n",
    "#### 시그모이드 함수\n",
    "    - X>0일떄 y=1, x<0일때 y=0, x=일때 0<y<1이면 미분이 불가능\n",
    "    - 그래서 이걸 스무스하게 만들어서 미분 가능하게 만든 것이 sigmoid\n",
    "        - x가 무한해지면 1에 수렴하게 하여 총 0~1사이에서 놀게 만들어요\n",
    "        - g(x)= 1/1+e^(-x)\n",
    "        - 무조건 0~1로 값을 보내기에 확률과 유사함. 범위로 매핑이 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Example (엠니스트)\n",
    "- 0~9까지의 숫자를 사람들이 필기체로 작성하고, 이걸 땁니다.\n",
    "- 각각 사진에 대해 레이블링 되어있음. \n",
    "    - 이 사진은 0이라는 class\n",
    "    - 이미지를 픽셀단위 매트릭스로 표현하여 까만건 1, 하얀건 0으로 표현\n",
    "        \n",
    "- 딥러닝 모델에 숫자 매트릭스를 우리는 넣게 됩니다.\n",
    "        - concatenate(28*28을 쭉 펼쳤으니까 거거가 784이고, 각각 바이어스가 1임. 그래서 디맨젼 1추가해주며, 총 784 벡터에 맨 위에 1이라는 값을 더 추가한 셈. 그럼 이 매트릭스는 히든 레이어가 15개있다 치면 785곱하기 15매트릭스가 되게 됨. 왜냐하면 1짜리 바이어스가 있게 되니까 일곱하기 더블유 제로. )\n",
    " - 가중치 매트릭스와 처음에 있던 인풋 벡터를 다프로덕트(내접)을 하게 되는데 이게 mp.dot()임. \n",
    "     - 처음과 인풋 프로덕트를 내접하고 그 결과를 z2에 넣음\n",
    "     - 이 z2를 시그모이드 거치게 하여 a2에 넣음\n",
    "     - 이 a2가 히든 레이어에서 나온 갓ㅄ이 되고 이것을 다시 인풋으로 받아서 두번째 레이어에서도 1을 추ㅏ한[[1]] 벡터를 닷프로덕트하고 마찬가지로 시그모이드 해서 a3에 집어 넣어요. \n",
    "\n",
    "- concat: concatenation의 줄임말로 pandas의 concat을 이용하여 데이터 프레임 결합\n",
    "\n",
    "#### (Prediction-Target)^2\n",
    "- (=로스함수_ 얼마나 못 맞췄는지)\n",
    "- (= 평가 함수)\n",
    "- (= 인스퀘어 함수)\n",
    "- 예측값을 알기 위해서 서로 빼주고 제곱도 해줌\n",
    "    - 차이가 얼마나 나는지 절대적인 값을 알고 싶어. 그래서 제곱을 해준다고 하심\n",
    "    \n",
    "\n",
    "- 크로스 엔트로피 함수.\n",
    "    - 확률이 얼마나 실제 정답에 대해서는 확률이 높은지.\n",
    "    - 정답이 아닌 것에 대해서는 확률이 낮도록 학습하는 LOSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "- mean squared \n",
    "\n",
    "### Classification\n",
    "- cross entropy error 사용\n",
    "\n",
    "\n",
    "##### SOFT MAX(cross Entropy class)\n",
    "- 소프트 맥스로 모든 확률의 값이 1이되도록 함\n",
    "    - z1=0.8, z2=-0.3이라고 할때(a1=0.9, a2=0.1)\n",
    "    - 1번 액티베이션은 e^0.8/(e^0.8+e^-0.3), 2번 액티베이션은  e^-0.3/(e^0.8+e^-0.3)이 되게함\n",
    "        - 그러면 합이 1이 되네\n",
    "        - 지수함수로 그릴 경우 실제 0.8과 -0.3의 차이보다 더 차이가 크게 보이고, 그래디언트 구할때 학습이 안정적으로 나오기도함.\n",
    " \n",
    " \n",
    " #### 따라서 리그래션할때는 액티베이션 거치치 않거나 시그모이드를 하며, 클래시피케이션할때는 거의 소프트 멕스 사용★"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 소프트 맥스와 시그모이드 차이☆\n",
    "- 잘 모르겠어서 구글링함\n",
    "\n",
    "- sigmoid : binary-classification에서 사용\n",
    "- softmax : multi-classification에서 사용\n",
    "    \n",
    "        Softmax Function\tSigmoid Function\n",
    "    1\tlogistic regression 에서 multi-classification 문제에서 사용\t\n",
    "    <>logistic regression 에서 binary-classification 문제 에서 사용\n",
    "    \n",
    "     2\t확률의 총 합 = 1\n",
    "     <>확률의 총 합은 1이 아님\n",
    "     \n",
    "     3\t출력층에서 사용됨(확률 표현)\n",
    "     <>Activation 함수로 사용될 수 있음(실제 사용 하지 않음)\n",
    "     \n",
    "     4\t큰 출력 값은 그 class에 해당할 가능성이 높다는것을 뜻하며 실제 확률을 나타냄\t\n",
    "     <>큰 출력 값은 그 class에 해당할 가능성이 높지만 실제 확률 값을 나타내는 것은 아님"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 잘 되었나 판단척도까지 지금 했어요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BackPropagation\n",
    "- 학습 잘못 시켰으니 다시 해봐라\n",
    "\n",
    "## Gradient Descent\n",
    "- 눈을 가린 사람이 산 위에 있다고 칠때, 어떻게 하면 빨리 산을 내려갈 수 있을까요\n",
    "- 가장 빨리 내려가려면 발을 디디면서 가장 낮은 방향으로 내려가겠죠. 그게 그래디언트 디센트\n",
    "- 그때의 순간마다 경사가 낮은 방향으로 내려간다★\n",
    "    - Y=X^2-4X의 최소값을 구하려고 한다면\n",
    "        - 일단 그래프를 그려봅시다.\n",
    "        - 컴퓨터가 최소 값을 구하는 방법.★\n",
    "            - 1. 아무 곳이든 점을 찍습니다. (5,5)를 찍었다고 칩시다.\n",
    "                - 그럼 이 곳에서 경사를 구해요(미적) 그리고 그 경사값만큼 살짝 아래로 내려오고 더 내려와.\n",
    "            - 2. 어느 순간 다시 올라가게 되면 그게 올라가는 순간이 보통 미분값이0이라 여길 최소라 구해\n",
    "       - 이를 통해 가중치(세타)를 업데이트 하게 됩니다. \n",
    "    - 미분 값을 구하기 힘든 경우?\n",
    "        - 예) e^x+ cosX+e^(-3x)\n",
    "        - 이런 경우 그래디언트 디센트를 이용해서 값을 구해요.\n",
    "- 이 방식을 이용하여 백프로게이션이라고 하는 것을 구할 수 있어요\n",
    "    - 세타제로 및 세타원을 업데이트 해나가요\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 편미분\n",
    "- f(x,y) =xy+2x\n",
    "    - 이 식을 x 및 y에 대해서 미분을 할 수 있음\n",
    "    - 에프를 엑스에 대해서 미분한건 y를 상수 취급하면 되어 y+2\n",
    "    - 에프를 와이에 대해서 미분하면 x를 상수 취급하면 되어 x\n",
    "    \n",
    "### 체인룰\n",
    "- cs231을 보시고 이해하시길 추천드립니다.★.★.★.★.★\n",
    "- Backpropagation using Chain Rule\n",
    "- f(x)=x^2\n",
    "    - f(3x)=(3x)^2, g(x)=3x일때, f(3x)= f(g(x))\n",
    "    - f(g(x))를 3으로 미분하면 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Network\n",
    "- cs231 강의가 한시간 반 정도 되는데 보시면 이해 될거에요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Framework\n",
    "- TensorFlow\n",
    "    - 구글 만들었음\n",
    "    - 문법적으로 배울게 많음\n",
    "- Keras\n",
    "    - 구글 개발자가 나왔음\n",
    "    - 파이토치보다 더 쉬워서 초반에는 추천드림\n",
    "    - 케라스가 텐서플로우에 합병되면서, 텐서플로의 기능을 케라스도 같이 사용할 수 있게 되었음. \n",
    "- Pytorch\n",
    "    - 페이스북이 만들었음. 가독성이 좋고, 파이썬 문법에 가까움(따라서 디버깅도 좋음)\n",
    "- mxnet\n",
    "    - 아마존이 만든 네트워크로 많이 사용되지는 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Community\n",
    "- 개발하다 모르는 거 생기면 여기서 질문해도 좋음\n",
    "    - TensorFlow Korea\n",
    "    - PyTorch KR\n",
    "    - AI Korea\n",
    "    \n",
    "- 시각화\n",
    "    - http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch=========================================\n",
    "### 딥러닝 라이브러리 (Python)\n",
    "- pytorch.org\n",
    "\n",
    "#### 장점\n",
    "  -   GPU 연산을 통한 빠른 학습\n",
    "  -   코드가 간결하여 가독성이 좋고 디버깅이 쉬움\n",
    "  -   Dynamic Network 구조이기 때문에 모델의 변경이 자유롭고 중간 결과를 확인하기 쉬움\n",
    "      - 컴파일 안 하고도 중간 결과값을 볼 수 있어 모델 변경도 편하고 다이나믹 필요한 곳에 바로 구현 가능\n",
    "  -   파이썬, C++ 등의 언어와 연동이 편함\n",
    "  -   Documentation이 잘 정리되어 있고,활발한 커뮤니티 교류 (Forum, PyTorch KR 등)\n",
    "  \n",
    "#### 단점\n",
    "- 한국은 텐서플로 코리아가 주도적임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor\n",
    "\n",
    "- 기본적으로 Numpy Array와 비슷함\n",
    "- • Gradient 정보를 저장할 수 있음 (requires_grad=True인 경우)\n",
    "- • torch.FloatTensor\n",
    "- torch.LongTensor\n",
    "- torch.from_numpy\n",
    "-  view, flatten, squeeze, unsqueeze, transpose 등의 함수 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOOGLE_COLAP====여기서부턴 코랩에서 =====\n",
    "- colab.research.google.com/\n",
    "- [ runtime>change gpu runtime(python3, hardwardaccelator(gpu/tpu))]\n",
    "- [tool> dark> 행 표시 클릭> 띄어쓰기 4칸]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설치\n",
    "- !pip3 install torch torchvision\n",
    "- import torch\n",
    "- torch.cuda.is_available() #cpu 잘 돌아가는지보는코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이토치 다큐멘테이션\n",
    "https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu\n",
    "- x=<1, y=0\n",
    "- x>1, y=x\n",
    "- x=0일때 미분값 0으로 고정시킴\n",
    "- 레루가 학습이 굉장히 빠르고 정확하게 하는 액티베이션 펑션이라 많은 경우 이거 사용\n",
    "\n",
    "#### 시그모이드 함수같은 경우는 완만한 cosx그림 같다면 (0<y<1)\n",
    "#### tahh 함수는 (-1<y<1)\n",
    "\n",
    "감정분석\n",
    "- 긍부정에서 tanh등 사용\n",
    "#### 소프트 맥스\n",
    "- 분모는 전체, 분자는 각각이고 각각의 합이 1이되게합니다.\n",
    "- dim. 디맨젼 어떻게 할것인지 워닝 떠요. (F.sigmoid(linear(x)))한다면.\n",
    "    - torch.sigmoid(linear(x))하면 워닝 안뜨니 우리 이렇게 쓰도록 해요\n",
    "    \n",
    "#### ADAM- 90프로가 사용함. (option)그리고 여기서 옵션 쓰곤함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.cuda\n",
    "- CUDA 및 GPU 관련 함수 제공\n",
    "• torch.cuda.is_available()\n",
    "- GPU가 사용 가능한 상태인지 체크\n",
    "• torch.cuda.device_count()\n",
    "- 사용가능한 GPU 개수 체크\n",
    "• torch.cuda.FloatTensor 등\n",
    "- GPU 메모리에 올라가 있는 Tensor 생성\n",
    "• torch.device(‘cuda’) or torch.device(‘cpu’)\n",
    "- 어떤 디바이스를 사용할지 선택\n",
    "• y.to(device): 해당 텐서를 해당 디바이스의 메모리로 복사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.utils.data\n",
    "- Dataset 관련 처리 모듈 제공\n",
    "- • torch.utils.data.DataSet\n",
    "갖고 있는 데이터셋을 다루기 편하게 관리해주는 인터페이스\n",
    "\n",
    "- • torch.utils.data.DataLoader\n",
    "해당 Dataset을 멀티프로세싱, batch processing, iterating 등\n",
    "다양한 기능을 추가 제공해주는 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn\n",
    "- • Neural network layer 관련 클레스들 제공\n",
    "• Containers\n",
    "nn.Module, nn.Sequential, nn.ModuleList 등\n",
    "• Linear: nn.Linear\n",
    "• CNN: nn.Conv2d, nn.MaxPool2d 등\n",
    "• RNN: nn.RNN, nn.GRU, nn.LSTM\n",
    "• Regularization/Normalization:\n",
    "nn.Dropout, nn.BatchNorm2d 등\n",
    "• 제공하는 기능들이 많으므로 꼭\n",
    "Documentation 살펴볼것!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn.functional\n",
    "- torch.nn과 유사하지만 클래스가 아닌 함수로 제공\n",
    "• 각종 Non-linear Activation function들 제공\n",
    "(relu, tanh, sigmoid, softmax 등)\n",
    "• Loss 함수 제공\n",
    "• 제공하는 기능들이 많으므로 꼭\n",
    "Documentation 살펴볼것!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최적화 알고리즘 클래스 제공\n",
    "• SGD, Adam, AdaGrad 등 다양한 optimizer\n",
    "    - 보통 아담 많이 사용함\n",
    "• 많은 경우 Adam이 좋은 성능을 보임\n",
    "• weight_decay는 l2 regularization 관련 Hyperparameter\n",
    "• Learning rate는 중요한 Hyperparameter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비전 관련 기능 제공-\n",
    "\n",
    "\n",
    "Vision 관련 기능들 제공\n",
    "• torchvision.dataset\n",
    "MNIST, CIFAR10 등의 데이터셋 제공\n",
    "• torchvision.transforms\n",
    "이미지 관련 전처리 기능 함수들 제공\n",
    "• torchvision.models\n",
    "VGGNet, ResNet, Inception 등 미리 학습된 모델 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습\n",
    "- !pip3 install torch torchvision\n",
    "- !ls #실제 배쉬에서 사용가능한거 나옴. 이게 매직코드. \n",
    ">> sample_data\n",
    "\n",
    "- import torch\n",
    "- torch.cuda.is_available() #cpu 잘 돌아가는지보는코드\n",
    ">> True\n",
    "- a=torch.FloatTensor([[2,3,4,5]]) #토치 생성(어레이와 같은 방식)\n",
    "- a\n",
    ">> tensor([[2., 3., 4., 5.]])\n",
    "\n",
    "# view는 넌파이의 reshape와 같음\n",
    "- import numpy as np\n",
    "- b=np.array([[3,4,5,6]]) #넌파이와 같이 비교할게요\n",
    "- b.shape\n",
    ">> (1,4)\n",
    "- b.reshape(2,2)\n",
    ">> array([[3, 4],\n",
    "       [5, 6]])\n",
    "\n",
    "- a.view(2,2)\n",
    ">> tensor([[2., 3.],\n",
    "        [4., 5.]])\n",
    "\n",
    "- a.view(2,-1).flatten() #플랫된 벡터로  펼쳐짐\n",
    ">> tensor([2., 3., 4., 5.])\n",
    "\n",
    "# 스퀴즈 언스퀴즈\n",
    "- a.unsqueeze(1).shape #언스퀴즈. shape은 1,4인데 현재 1,1,4로 뭔가 끼어들어갔음\n",
    ">> torch.Size([1, 1, 4])\n",
    "\n",
    "- a.shape\n",
    ">> torch.Size([1, 4])\n",
    "\n",
    "- a.unsqueeze(1).squeeze(0)\n",
    ">> tensor([[2., 3., 4., 5.]])\n",
    "\n",
    "- a.unsqueeze(1).squeeze(0).shape #이렇게 다시 없앨 수 있음\n",
    ">> torch.Size([1, 4])\n",
    "\n",
    "#size통해 첫번째 디멘션의 크기가 몇인지 알 수 있음\n",
    "- a.size(0)\n",
    "\n",
    "# torch.cuda _중요한 메소드 잇어서 볼게요\n",
    "\n",
    "#cpu 사용 여부 체크\n",
    "print(torch.cuda.is_available())\n",
    "# 만일 사용이 가능하다면, 이 컴퓨터의 지피유 몇개가 가능한지\n",
    "print(torch.cuda.device_count())\n",
    ">>True\n",
    ">>1\n",
    "\n",
    "# torch.cuda 플롯 팬서\n",
    "# 지피유에 올라간 텐서생성\n",
    "##################실습################3\n",
    "- device=torch.device('cuda')\n",
    "- a=a.to(device)\n",
    "- a\n",
    ">> tensor([[2., 3., 4., 5.]], device='cuda:0')\n",
    "\n",
    "\n",
    "# !nvidia-smi실제 지피유 얼마나 사용하는지 확인\n",
    "- a=torch.cuda.FloatTensor(1024,1024,256) #256메가바이트\n",
    "- !nvidia-smi  #실제 지피유 얼마나 사용하는지 확인\n",
    "\n",
    "# 리니어 구현\n",
    "- linear=torch.nn.Linear(4,1) #input:4, output:1\n",
    "- x=torch.FloatTensor([0,1,1,1])\n",
    "\n",
    "# 리이어에 함수 형식으로 넣으면, 값이 이렇게 나와요. (tensor([-0.3247], grad_fn=<AddBackward0>))\n",
    "# 리니어 자체가 함수라서 linear. (점) 클릭하면 사용할 수 있는 값들이 나옴\n",
    "- linear(x)\n",
    ">> tensor([-0.3247], grad_fn=<AddBackward0>)\n",
    "\n",
    "#torch.nn.functional. 시그모이드나 소프트맥스 등. 액티베이션으로 시그모이드 및 소프트맥스 쓸 수 있는데 그게 여기에 있어요. 함수가 워낙 많아서 docs 볼게요\n",
    "\n",
    "- import torch.nn.functional as F #귀찮으니까 그냥 F로 부르게해\n",
    "- F.sigmoid(linear(x))\n",
    ">> /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
    "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
    "tensor([0.4195], grad_fn=<SigmoidBackward>)\n",
    "\n",
    "- F.tanh(linear(x))\n",
    ">> /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
    "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
    "tensor([-0.3137], grad_fn=<TanhBackward>)\n",
    "\n",
    "- F.sigmoid(linear(x))\n",
    ">> /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
    "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
    "tensor([0.4195], grad_fn=<SigmoidBackward>)\n",
    "\n",
    "- torch.sigmoid(linear(x))\n",
    ">> tensor([0.4195], grad_fn=<SigmoidBackward>)\n",
    "\n",
    "# ADAM- 90프로가 사용함. (option)그리고 여기서 옵션 쓰곤함\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### MODEL은 클래스 형식으로 정의\n",
    "- n드. 클래스 정의 할 때 이 함수가 호출 되는 식으로 필수적으로 구현되어야 함. \n",
    "-  self.(점)으로 하고\n",
    "\n",
    "\n",
    "- relu라는 액티베이션을 통해 계산. \n",
    "- forward 함수도 필수적으로 구현이 되어야 하며 이건 내가 사용하는 학습되어야 할 데이터 대한 것\n",
    "- 즉 ** __init__ 및 forward는 모델에서 꼭 구현이 되어야 함\n",
    "\n",
    "\n",
    "#### Training Model\n",
    "- 인풋 디맨젼 8, 히든 디맨전 128 지정하여 모델 생성 (단 초반에는 리니어에 있는 가중치 랜덤 초기화)\n",
    "- model.parameters()\n",
    "- lr: 러닝 메이트 (10의 -3승을 넣게 되어요)\n",
    "- criterion: MSELose를여기에서 하고\n",
    "- model.train() : 학습모드라고 말해주고\n",
    "- epochs : \n",
    "\n",
    "- 포문: 여러번 학습시켜요(매 학습마다 데이터 순서는 랜덤하게 바꿔줘야해요 ** 딥러닝 성능이 좋다보니 저 순서를 고정해버리면 순서 자체를 외울 수 있음. 따라서 랜덤하게 바꾸며 학습하게 됨)\n",
    "\n",
    "- batch: 예,20개씩 한번에 넣어서 학습하는 식. 하나씩 넣게 되면 너무 속도가 느려서.\n",
    "    - 각각에 대해 평균을 내서 그거에 대한 프로포게이션\n",
    "    - gpu 메모리에 복사를 하게 되면 20장에 대해 모델에 넣게 되고, \n",
    "    \n",
    " - 로스 함수 계산; 20개, 타겟 20개에 대한 민ㅅ퀘어 계산을 하래. 그래서 백워드를 쭉 하면 레이어1,2,3별로 계싼이 되어요. 백워드하면 그ㅐ디언튿르이 각각 생김\n",
    " \n",
    " - 옵티마이저.스탭) ㄱㄱ각 레이어 들이 각각 그래디언트를 가지고 업데이트를 하게 됩니다. \n",
    " \n",
    " - 이 과정 반복하면 모델의 성능이 조아져요\n",
    " - 옵티마이점 제로 그래드는\n",
    "    - 백워드 한번하면 그래디언트 계싼되는데 또 백워드를 하면 그래디언트가 플러스가 됨. 이전 계산에 지금 계산 추가되어 누적되니, 이전 계싼은 날려버리곘다는 뜻.\n",
    "    \n",
    "    \n",
    "    \n",
    "#### Testing Model\n",
    "    - 우선 그래디언트 계산할 필요 없음\n",
    "        - torch.no_grad로 계산 안하겠다 하여 지피유 메모리 적게 쓰게 함\n",
    "        - model. eval() : 평가 모드로 하겠다. \n",
    "            - 나중에 드랍아웃 및 기타 랜더마이즈한 기법 적용시 학습에서 사용하다가 평가할때는 랜덤하게 사용하겠다 그렇게 고정하겠따는건데 일단 무시(''//'')\n",
    "         - 아까는 MSE에러였는데 이제는 loss . \n",
    "           - 즉 |0.8-1| 로 0.2만큼 차이 있는거 직관적으로 알 수 있게끔. \n",
    "           - 다 더하고 평균내서 토탈 로스로 출력한다고 보시면 됩니다.n.Module: 최상위 부모 클래스\n",
    "- __init__ : 스페셜 메소\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### LOAD and Save Mode\n",
    "- torch.save()\n",
    "    - 이 함수로 저장\n",
    "    - 모델 자체를 넣지 않고, 모델의 상태를 불러와서 저장하게 됨\n",
    "    - torh.save(the_model.state_dict(), PATH)\n",
    "        - 상태) 이런 모델의 상태값 저장을 하고 모델 자체를 저장해도 되나 권장하는 것은 상태저장\n",
    "        - 그래야 변형시 유연함\n",
    "- \n",
    "\n",
    "- the_model+THeModelClass(*args, **kwargs)\n",
    "- the_model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    - 파일에서 모델을 불러오는 경우.\n",
    "    - 클래스로 정의했기에 실제 객체로 만들어줘야해요. \n",
    "    - 만들지 않고 하면 모델이 없는 채에서 함수 사용하는 식이라 에러 생길 수 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xtrain, Xtest, Ytrain, Ytest 이해\n",
    "\n",
    "- 만일 키를 통해 몸무게를 추측한다고 합시다\n",
    "- 키에 대한 데이터 172, 180cm등은 Xtrain이고, 178일때의 결과를 얻고 싶은 Xtest라하면\n",
    "- 몸무게(결과 예측)에 대한 데이터는 62, 65kg이 ytain이고, 값이 나왔어야 하는 결과값 70은 ytest\n",
    "- 이때 ytest와 ytrain에 대한 차이가 loss\n",
    "- 에폭: 데이터 넣고 돌리는 한 세트\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 하는 이유\n",
    "- 안정적으로 만들기 위함\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
