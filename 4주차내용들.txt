1. 넌파이 사용하는 이유 2가지
- 속도가 빠르다
- 편하고 쉽다.(엘리먼트와이즈 방식. + - X 수학 그대로 쓸 수 있음) (scipy_과학분야에서 많이 쓰는 stats,signal등 미리 계산해둔거 쓰면됨. 굳이 구현 안하게 해둠)

2. 넌파이에서 속도가 빠른 이유
- C로 만들었다
- 벡터라이즈된 데이터 구조/알고리즘으로 만들어졌다. 
   - 벡터라이즈: 연산을 for 없이 한다.
- 호모지니어스
- 내부구조가 일련이다.(strides)(실제로는 1열인데, 사람에게는 n*n행으로 보이게해)

3. 어레이는 포문을 돌린다 안돌린다? 안돌린다 속도느려진다.

4. 루프 안 쓰는 3가지 //이제-제가-컴맵
- 이터레이터, 제네리이터
- 컴프리핸션
- 재귀함수
- 맵 필터 리듀스 하이 아더

5. 넌파이는 로우?하이 레벨?
- 로우 레벨

6. dir과 __dir의 차이?
- 정렬의 문제
- 대문자 A은 65, 소문자a는 87. 따라서 dir했을때 대문자부터 등장

7. 파이썬은 상수 있음?
- 명시적 상수 없고, 대문자로 이으면 관례처럼 상수처럼 씀

8. 데이터 생성 방식 3가지
- 인스턴스 방식) 순수한 객체라서
- 리터럴 방식) float=0.0, str=""이렇게 명시적으로
- 팩토리 메소드) 인스턴스 하지 않고 다른 함수 힘을 빌어서 인스턴스화
   - 예)호모지니어스한 어레이에서 a=np.array(['1',2,3]) 후 a치면 전부 타입이 문자열

9.아웃풋에서 뭐라고 나오면 넌파이임?
- array()

10. array는 어떤 타입?
- 호모지니어스함.

11. 어레이는 뮤터블 /이뮤터블?
- 뮤터블. 따라서 카피(스왈로/딥 카피) 알아야함
- 이는 튜플을 입력헀는데 리스트로 출력되는걸 통해 알 수 있음 !!!@@@@

12. 3차원 만드는 경우 공백은?
- 그 사이에 겹치는게 있나보다
- 

13. 어레이는 로우레벨? 하이레벨?
- 로우레벨. 얘는 C방식 포트란 방식, 리틀앤디언, 빅 앤디언 방식으로 지원
- < 는 리틀 앤디언, >는 빅앤디언

14. 타입 쳤을때 numpy, ndarray 나왔으면 만들 수 있나?
- 클래스. 클래스는 인스턴스 하면 된다.
>> n=np.ndarray(0)
>> n 치면 결과로 array([], dtype=float64)

15. 호모지니어스라면서 왜 dtype이 나올까?
- 팩토리 메소드와 연계됨

@@a=np.array(['1',2,3])(클래스처럼) 이거는 Type Error나는데 ,  a=np.array(['1',2,3]) 하면 아무 문제 없던건 왜?

16. 행 열 바꾸게 하려면?
> a.T

17. 엘리먼트 와이즈?
- 수학적 연산 편리하게 쓰게 하려고. (더하기, 뺴기 등)

18.차원을 아는 방법?
- ndim : 차원
- len : 차원
- shape :몇행 몇열. 연산할때 모양 맞추는게 중요해서 얘도 중요해
- flags: 씨인지 포트란인지
    - 씨는 옆으로, 포트란은 아래로
- size:원소의 갯수
- itemsize: 하나가 몇 비트인가
- dtype

19.파이썬 언더바★★★★★
- 1. 앞에 언더바 하나)관례상 프라이빗. 
    - 그러나 관례상이 아니었던 곳이 있었어 from * 쓰는 경우 임포트 된다/안된다?  안된다.
    - 앞에 한개 붙으면 '프라이빗'으로 쓰인다. from import * 하면 안됨
- 2. 앞에 두개 붙을때 __) OOP할때 dir 할때 안 나왔어. 
    - 맹글링.(dir했더니 안나옴)
- 3. 앞에 둘 뒤에 둘) 매직메소드/스페셜 메소드
- 4. _
    - 관례상 필요없는 변수 이름 일때
- 5. _
    - 언더바가 마지막을 가리킬떄
- 6. 
    - 언더바 한개 붙고 이름을 _(a)로 표현하는경우
    - 다국어 처리하는 경우
- 7. 
    - 이름 뒤에 언더바 붙은 경우
    - 파이썬 키워드하고 똑같을때 이름 충돌을 피하려고 사용

20. 액시스
- 행끼리 계싼? 0
- 열끼리 계싼? 1
- 꼼수로 shape 10*3인 행렬에 대해서 
    - 액시스0은? 0자리의 10을 가린다. 그럼 몇개? 3개. 
    - 액시스 1은? 1자리의 3을 가린다. 그럼 몇개? 10개. 


21. 어디부터 어디까지 range 유사ㅣ한거 있음
- a=np.arange(10)
22. reshape 행렬 모양 ㅂ녀경

22. b=np.ones((3,3)), c=np.full((3,2),6), d=np.ones_like(a)

23. like
- 내가 지정한 애랑 동일항 행/열로 만들어줘요
- 행렬 연산할때 크기가 중요해요. shape이 똑같아야해.
- 이거 사용하면 크기 맞춰서 뭘 해준대
- 대신 값은 1로 다 채워주고

24. 단위행렬
- eye, identity 
- 차이 : eye에 옵션이 많음. 

25. 라인별로 자르고 싶을때
- d=np.linspace(0,100)

26. 로그로 잘라
- e=np.logspace(1,100)

27. 쓰레기값으로 채워서 만드는거
- e=np.empty((4,2))

28. 뽑아내기
-1. 인덱싱/슬라이상 + 2.키
-2. ... 전부다 뽑아내기
-3. 뒤에 조건을 넣을 수 있어요. 
 a[(a>3) | (a<8)] #d이건 OR
 a[(a>3) & (a<8)] #이건 가능 and
>> 다만 괄호 안쓰면 에러

29. 마스킹 기법
- 트루만 뽑기** 할당하는 방식 아니고 저렇게 작성해야함
- a=np.arange(10)
- a[[True, False, True,True,True,True,True,True,True,True]] 
   - array([0, 2, 3, 4, 5, 6, 7, 8, 9])

30. 팬시 인덱싱
- 마스킹과 유사. 꺽쇄가 두개. 
- a[[1,2]] 1행,2행 뽑아옴

31.  추가 5가지 넘파이 지원하는 것
- 콤마 땡떙떙 조건 마스크 팬시 인덱싱

32.괄호 두개!
- 팬시 인덱싱) 행만 뽑아요
- a[[1,2,4],[2,3,4]]는 (1,2),(2,3),(4,4)에 있는거 뽑는거

33. 브로드 캐스팅
-간단한 크기는 넌파이가 알아서 맞춰줌
-3+np.arange(10)


34. 파이썬 넌파이 쉬운 이유
1. 인덱싱,슬라이싱
2. 브로드 캐스트
3. ufunc

35. ufunc?
- u: universial 보편적인
- 넌파이에서 원소 한개 짜리 스칼라에ㅐ서도 지원이 되고, 벡터에도 지원이 됨. 
   - 예) np.abs([-1,1,2]), np.ceil([2.9,1,2,3,0.3])

36. 넌파이 설명 보는법
- np.info(np.ceil)
-1)np.info()
   - /짝대기는 포지셔널 온리, *별표는 키워드. 키워드 이후부터는 키워드만 써야함
- 2)iinfo()
   - np.iinfo(3)
   - 인티저 타입에 대한 인포메이션 알려줌
   - 유니버셜은 이거로 봐야해. 설명에 안나와

37. 간단한 함수말고 좀 어려운 함수는 고급 함수는?
- 싸이 파이
- 팩토리얼도 가능
- from scipy.special import factorial
- factorial([3,4,5])

38. np.where(아주중요)
- b=np.arange(5,10) 
- b[np.where(b>5)] #얘는 값이 나오니까 6,7,8,9
   -array([6, 7, 8, 9])
-b
   -array([5, 6, 7, 8, 9])
- np.where(b>5)
   -(array([1, 2, 3, 4], dtype=int64),)
   -#np.where는 인덱스를 가져오니까 해당되는 인덱스는 1,2,3,4에 대한 거(인덱스0인건 5라 충족X)

39. np.max
- 가장 큰 값

40. 인덱스 중에서 가장 큰거/ 가장 작은거
-np.argmax(), np.argmin()

41. np.argmin과 b.argmin차이
- 앞에게 함수로, 뒤에게 메소드
- 이 차이는 SHIFT TAB보면 알 수 있어요

42. 떙떙떙
- 1.줄임표
- 2. 인덱스 슬라이싱에서 모두다 표현

43. 아주 길때 몇개 볼지 고르는거
- np.set_printoptions(threshold=np.non)
- np.nan 되게 골치아파
    - 우리 이거 float 에서 본적 있죠
    - float과 무한대 형태로 있는데 np.nan. 아까 파이는 같았으나 이건 또 달라요. 이런건 외울수 밖에 없어★

44. np.newaxis
-이런건 외워야한다
- np.newaxis==None //True
- np.newaxis is None //True

45. setprintoption
- 여러개중 몇개 볼건지

46.  파이썬에서 검생이 안되는데 넌파이때문에 만든 연산자. 
- @
- 이 연산자는 유일하게 넌파이만 사용 가능. 파이썬 3.6 이상


47. 다섯개 새로운거~
인덱싱 슬라이싱
콤마. /앞은 행 뒤는 열
떙떙떙
마스킹
조건

-----
1. 관련된 설명 보여주는거?
<까먹으면 안 되는 2가지>
- np.lookfor
   > np.lookfor('shape')
   > np.lookfor('create array')
   > np.lookfor('* array')
- np.info

2. array 몇 차인이 어떻게 아는가?
- []괄호의 갯수

3. 여러개를 하나로 줄이는것?
- 리듀스

4. 모양 바꾸려면 무엇을 이용하나?
- reshape ()
- b=a.reshape(3,-1) # -1이 9로 바뀜


5. 리쉐입은 뮤터플? 이뮤터플?
- 뮤터플

#예시
-  a=np.arange(27)
- a.shape
>> (27,)
- a.reshape(3,9)
#얘는 1차. 콤마는 왜 표현을 했을까? 튜플로 표현했기에 하나짜리는 저렇게 표현한거에요. 
- b=a.reshape(3.9).copy()
   >> 이렇게 써야함*****
6. 기본적으로 카피는 딥? 스왈로?
- 딥

7. reshape하면 원래 값도 바뀔수 있기에 어떻게 써야하나?
- 카피 써야한다
- b=a.reshape(3.9).copy()

8. Reshape, Resize 차이?
- Resize는 뮤터블 자기자신 변경
- Reshape는 재할당이며, 실수 막기 위해 copy같이

9. Resize 크기 맞춘다?
- 안 맞춰도 실행되긴한다.
- 부족하면? 0으로 패딩하고
- 넘치면? 잘라서 나온다. 
a=np.arange(27)
- a.resize((3,5))

10. Resize 네거티브 허용?
- 허용 안한다. 

b=np.arange(5)
b
   >>array([0, 1, 2, 3, 4])

c=np.arange(5,10)
c
   >>array([5, 6, 7, 8, 9])

- np.stack((b,c))
array([[0, 1, 2, 3, 4],
       [5, 6, 7, 8, 9]])

- np.stack((b,c),1)
array([[0, 5],
       [1, 6],
       [2, 7],
       [3, 8],
       [4, 9]])

-np.hstack((b,c))
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

- np.vstack((b,c))
array([[0, 1, 2, 3, 4],
       [5, 6, 7, 8, 9]])


11. Reshape 네거티브 허용?
- 허용
- b=a.reshape(-1,-1)
- b=a.reshape(3,-100)

12. split?
- 기준에 의해 쪼기는거

13. split 결과값 어떤 형태로 보내나요?
- 리스트

14. split 3종류?
- split, vsplit, hsplit

15. 파이썬 접근 방식 2개?
- 펑션 방식/ 메소드 방식

16. 차원 변화 (1>2)
- reshape
- a=np.arange(6)
-a
   >> array([0, 1, 2, 3, 4, 5])
- a.reshape(6,1)
array([[0],
       [1],
       [2],
       [3],
       [4],
       [5]])

이런거 말고, 양 옆에 [] 더 추가하는 식으로 하고 싶으면?





16. np.split(a,(1,2)) 어떻게 잘린거?
- 세번 잘림. 0~1까지, 1~2까지, 2~3까지

17. split 튜플 형태로 자를수 있나?
- 응

18. np.split(a,(2,2))?
-0~2사이까지, 
[array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],
        [ 9, 10, 11, 12, 13, 14, 15, 16, 17]]),
 array([], shape=(0, 9), dtype=int32),
 array([[18, 19, 20, 21, 22, 23, 24, 25, 26]])]

19. 오잉 np.split(a,(5,5),axis=1) 
[array([[ 0,  1,  2,  3,  4],
        [ 9, 10, 11, 12, 13],
        [18, 19, 20, 21, 22]]),
 array([], shape=(3, 0), dtype=int32),
 array([[ 5,  6,  7,  8],
        [14, 15, 16, 17],
        [23, 24, 25, 26]])]


20. split 액시스 0이?1이? 
- 브이스플릿, 액시스1한게 브이스플릿

21. stack 종류?
- hstack, vstack
-np.hstack((b,c))
- np.stack((b,c),1)


22. 언더바 삼총사(
- np.r_[b,c] 
- np.c_[b,c] 
- np.s_[b,c] //슬라이스

23. 위의 세개는 뭐의 단축?-
- stack 단축표현들

24. 컬럼 붙일때 사용?
- np.column_stack([b,c])
array([[0, 5],
       [1, 6],
       [2, 7],
       [3, 8],
       [4, 9]])

-------------
a=np.arange(6)
--------------
@@@@@
newaxis
a[:, np.newaxis]

a[:,:, np.newaxis]

25. shape을 매트릭스의 모양을 자유자재로 바꾸는데 쓰는거
1)- []
a[:, np.newaxis]
a[:,:, np.newaxis]

-----------예시--------------
-a=np.arange(6)
-a
>>array([0, 1, 2, 3, 4, 5])
-a[:,np.newaxis]
>>array([[0],
       [1],
       [2],
       [3],
       [4],
       [5]])

이거 어려워서 쓰는게 expand_dims()
np.expand_dims(a,1)
array([[0],
       [1],
       [2],
       [3],
       [4],
       [5]])
----------------------------------------
2)- expand_dims
- np.expand_dims(a,1)
-array([[0],
       [1],
       [2],
       [3],
       [4],
       [5]])

26. expand_dims(,)는 차원을 몇 단계 더 올릴 수있나?
- 1개

27.  1차원<> 3차원 가능하나? 
- 응 이럴떄 사용하는게
- swapaxes((,)) & moveaxies((,))
array([[[ 0,  1,  2],
        [ 3,  4,  5],
        [ 6,  7,  8]],

       [[ 9, 10, 11],
        [12, 13, 14],
        [15, 16, 17]],

       [[18, 19, 20],
        [21, 22, 23],
        [24, 25, 26]]])


a=np.arange(27).reshape(3,3,3)


28. 축끼리 값 변경?
- swapaxes((,))
- np.swapaxes(a,0,2)
array([[[ 0,  9, 18],
        [ 3, 12, 21],
        [ 6, 15, 24]],

       [[ 1, 10, 19],
        [ 4, 13, 22],
        [ 7, 16, 25]],

       [[ 2, 11, 20],
        [ 5, 14, 23],
        [ 8, 17, 26]]])


29. 축 이동?
- moveaxies((,))
- np.moveaxis(a,0,2)
array([[[ 0,  9, 18],
        [ 1, 10, 19],
        [ 2, 11, 20]],

       [[ 3, 12, 21],
        [ 4, 13, 22],
        [ 5, 14, 23]],

       [[ 6, 15, 24],
        [ 7, 16, 25],
        [ 8, 17, 26]]])

30. 스트럭쳐드 어레이?

31. 스트럭쳐드 어레이랑 유사한거?
- 네임드  튜플
from collections import namedtuple
namedtuple
Address=namedtuple('Address',('name','age','weight'))
al=Address('Rex', 27,81.0)

32. 파이썬 클래스 정의?
- 정의 안 해도 된다
- 메타클래스 쓰면 클래스 정의 안해도 클래스 처럼 쓸 수 있어
- 네임드 튜플도 유사해.
- 튜플에 있는 인덱스하고 카운트까지 지원해
- 따라서 얘는 튜플이라서 어떻게 쓸 수 있따?
    - al[0]
    - al[1] 
    - 즉, 이름 및 인덱스로도 사용할 수 있음


33. 인덱스로 접근 가능함?
- 응 al[0]
'Rex'

34. 키로 접근 가능함?
- 아니 보통 인덱스로 접근하면, '키'로는 접근이 안돼. 생성은 되긴함. al['name']
타입에러

35.  네임드 튜플에서 안 된거?
- 키로 네임 접근하니까 안되었음. 

36. 넌파이는?
- 그러나 넌파이 스트럭쳐드는 키로도 접근이 가능함
    - 이 방식은 편히 '딕셔너리'방식이라고 할게요. 그럼 어떤 데이터만 가져오냐면 
        - 즉, 행으로도, 열로도 가능해요

37. - 인덱스 치면 ? 
- 로우 가져온다. 튜플/딕셔너리 방식으로하면 컬럼 데이터를 가져온다

38. 스트럭쳐드는 어디에ㅓㅅ 돌아?
- 메모리

39. 엑셀은 어디에서 돌아?
- 디스크, 가상 메모리. 좀 느려

40. 설명
#### 데이터 프레임은 판다스의 기본적인 자료형태
- 판다스도 넌파이 기반
- tpe(tips) 쳐봐
- 위의 결과는 스트럭쳐드 어레이 사용한거야
    - 세로 뽑을떄 부슨 방식 으로 뽑아? 
        - 아까 키로 뽑았잖아

#### 어트리뷰트는 아까 안됬죠 넌파이는
- 근데 판다스는 스트럭쳐 플러스 알파가 들어왔어
- 즉 점으로 접근 가능
- .어르티 뷰트 방식으로도 접근 가능.
- 네임드 튜플의 성질을 이용했는데 확장을 시킴. 

####  사이즈 왜 될까
- 얘는 넌파이 기반
- 넌파이에서 중요한 속성(shape, dtype)
    - shppe에서 두개 곱하면 사이즈 알 수 있땠됴. 
        - tips.size하면 사이즈 메소드를 부르는 거에요. 
        - 얘는 '넌파이' 기반이니까
        - 그럼 넌파이를 알아야 저게 왜 안 되는지 알 수 있죠
- 그럼 우린 무슨 방식으로 부르면 된다?
    - 키방식.
    - 점으로 불렀더니 '키'가 넌파이에 있는 가장 기본적인 메소드 이름고 ㅏ동일하기 때문에 문제가 생겼어. 원하는 컬럼값 안불러와
- 그럼 스트럭쳐드니까 키 방식으로 불러봅시다.
    - 문제 있어. 
    - 간혹 공백 있으면 안 불러 올 수 있어. 
    - 공백있으면 무슨 방식으로 부른다? 키 방식. 왜냐하면 스트럭쳐드 어레이니까.
- 그럼 가로 한 줄씩 뽑아내나?
    - 스트럭쳐드가 가능했잖아. 그래서 여기서도 가능해
    - 근데 같은 방식으로 뽑을때 얘도 문제가 있어. 
- 스트럭쳐드의 문제점
 
### 아까 숫자로 인덱스 시켰죠.
- 그러면 얘가 숫자인지 문자인지 모르겠죠. 넌파이에서는 name이 문자라서 얘로 뽑으면 세로로 했고, 인덱스로는 가로로 뽑앗는데
- 판다스에서는 이름도 숫자로 표현되어서 위와 같은 방식으로 ㄹ바로 쓸 수 없어
- 그래서 판다스가 어떤 고민을 했냐
    - 
### 아까 숫자로 인덱스 시켰죠.
- 그러면 얘가 숫자인지 문자인지 모르겠죠. 넌파이에서는 name이 문자라서 얘로 뽑으면 세로로 했고, 인덱스로는 가로로 뽑앗는데
- 판다스에서는 이름도 숫자로 표현되어서 위와 같은 방식으로 ㄹ바로 쓸 수 없어
- 그래서 판다스가 어떤 고민을 했냐
    - 뭘 하나 더 붙이고 tips.loc[] 저렇게 스트럭쳐드에서 행을 뽑아왔어요
  #### 이게 정형데이터
- 가장 밑에 숨어있는 개념이 스트럭쳐드 어레이!!

#### 나름교수님정리) 이름있고 이름 위에 데ㅐ이터의 타입이 있죠
- 키 방식으로 사용하면 열이 추출되고, 
- 인덱스 방식으로는 로우가 추출
- 그러면 인덱스는 슬라이싱 가능? 네
    - 그럼 부분적인 데이터만 가져올 수 있을거에요.
#### 넌파이의 '스트럭쳐드 어레이'



41. 판다스에서 맨위에 있는거?
    - 컬럼 및 헤더
42. 판다스에서 맨 왼쪽에 있는거?
    - 인덱스
43. 판다스에서 2차원 데이터 구조
    - 데이터 프레임

44. 데이터 뽑는방식2개
-1. 키로 뽑는방식
-2. 어트리뷰트로 뽑는방식
    - 넌파이는 없지만 판다스에는 있어

45. 
    - 어트리뷰트에 문제가 사실 있어. 저 size 같이 기본 메소드랑 동일 이름이거나 공백 있으면 사용하지 못했거든
    - 근데 왜 지원하게 되었나? 편해서
    - 키 방식으로 쓰면 4글자 써야하고 어트리뷰트는 점 하나만 쓰면 되니까........ 그래서 어트리뷰트 지원하는데 골치아픈 이슈 생긴대
    - 나중에 나올건데 



46. 판다스에서 팬시 인덱싱가능?
- 응 넌파이 기반이니까
-tips[['day','time']]

47.값을 1개 뽑을떄랑 2개이상 뽑을떄 왜 그림이 다르죠?
- 타입을 해봐
- type(tips.day)
>> pandas.core.series.Series

48 시리즈?
   -  왼쪽에 있는 숫자들
49. tips.tip.values 
    - 어찌아느냐? 점 벨류즈(r
array([ 1.01,  1.66,  3.5 ,  3.31,  3.61,  4.71,  2.  ,  3.12,  1.96,
        3.23,  1.71,  5.  ,  1.57,  3.  ,  3.02,  3.92,  1.67,  3.71,
        3.5 ,  3.35,  4.08,  2.75,  2.23,  7.58,  3.18,  2.34,  2.  ,
        2.  ,  4.3 ,  3.  ,  1.45,  2.5 ,  3.  ,  2.45,  3.27,  3.6 ,
        2.  ,  3.07,  2.31,  5.  , 

50. 브로드 캐스팅 되나?
- 응
- tips.tip+5

51. 리덕션 되나?
- 응. 여러개 한번에 표시하는 sum 되더라
-tips.tip.sum()
>>731.5799999999999

52. 값은 ?
- 넌파이 어레이 포멧

53.가로/세로~ '시리즈'
- 우리가 배운 어레이에서는 방향 없어. 시리즈는 가로든 세로든 중요하지 않아.
- 즉 방향이 없다는거야. 
    
 
54. 행가져오는 방식 2가지
- loc, iloc
>> tips.iloc[3]
>> tips.loc[3]

55. 쟤네 슬라이싱 가능?
- 가능

56. loc & iloc 차이
- tips.loc[3:6] 6이포함되고, tips.iloc[3:6]하니까 6이 포함 안돼
- loc는 이름 기반. 끝에있는거 포함
- iloc는 인덱스 기반. 끝에 있는거 포함 안함

57. , 조건, 마스킹, 팬시인덱싱, 컴마 다 가능하니까
- tips.loc[3:6,['day','smoker']]
- tips.loc[3,'day']

58. 행 가져올때 순서 바꿔도 되니?
- 응

59. #### 컴마 인덱싱, 컴마 슬라이싱 지원
- iloc는 파이썬과 동일
-tips.iloc[3:6:]

60. ㄸ애땡떙 지원?
- 이건 안해 iloc.

61 열 뽑아오는 방식 3개 기본//어키팬인
- 어트리뷰트 방식: 편하게 쓰려고 지원하나 사이즈/공백 있을떄 안 되고 있어요
- 키 방식: 가장 기본적이야. 넌파이 스트럭쳐 그대로 가져오니
- 팬시 인덱싱
    - 특징: 데이터 프레임으로 가져와@
    - tips[['tip']]

62. 열 뽑는 3개중 속도 빠른거? 
- 키 방식. 넌파이 그대로 가져오니까

63.하나만 가져오는 방식 3개
- tips.loc[3,'tip']
- iat으로 인덱스 방식
    - tips.iat[3,5]
- at으로 이름 방식
    -tips.at[3,'time']

64. 정리
- 특정 열만 뽑는거 3개 //어/키/팬인
- 행 뽑는거 2개 //loc iloc
- 특정 값 4개// loc, iloc, iat, at

65. 넌파이의 역할?
- 벡터 기반으로 데이터 처리하는 라이브러리

66. 판다스 역할
- 판다스는 넌파이 기반으로 eda (exp..)하고 데이터 전처리 하는 애

67. eda?
- Exploratory Data Analysis

68. eda 기본 역할?
- 미싱 데이터 찾는다. 
- 탐색적 데이터 분석
- 기술통계 편하게 하려고 하는 거
- 기초 분석 하는 도구
- 데이터 먼징 렁글링 멍징 하는 도구

69. 데이터 랭글링?
- 데이터를 불러오고 그걸 합치고 간단한 전처리하고 기초분석까지 하는 것
 - ETL과 유사해요. 

70. ETL과 랭글링 차이
- etl는 개발자, 랭글링은 분석가 차이

71. 버전관리

72. 워터마크
- %watermark -a'문근영' -d -p numpy,pandas
- -a: 저장
- -d: 날짜
- -p : 

73. 그래프
- %matplotlib inline
- pd.plotting.scatter_matrix(data)

74. missing data 처리방식
    - 1.삭제
    - 2.특별한 수로 채운다
    - 3.인터폴레이트

75. <1>datainfo에서 봐야하는거
- range index
- missing data
- object
- dtypes
- 메모리 사용률

76. 판다스에서 문자열?
- 문자열은 오브젝트로 표현돼


77.<2>data.describe()
- 컬럼에 대한 통계 보여줌

78. size와 count 차이?
- size: 미싱 데이터 포함
- count: 미싱 데이터 불 포함 갯수


79. data.describe()하면 카운트, 민, 표준분포, 등이 나와 이떄 중요한 2개념?
- 큰수의 법칙, 중심 극한 정리

80. 외도, 첨도?
- 외도는 얼마나 쳐우쳤는지, 첨도는 뾰족한지
- data.skew()
- data.kurt()
- data.kurtosis()

81. data.describe로 뭐 알 수 있어?
- 정규성과 박스플롯

82. 박스플롯 그리는법
- data.boxplot()
----점심-----
83. describe로 뭐한다?
- 데이터의 분포를 본다.
- data.describe(include='object')
- data.describe(include='all')
   -float64, number처럼 원하는것만 include해서 할 수도 있다.


84.info>describe 다음에 할 것?
- seaborn.
- import seaborn as sns
   1. s는 statistical 통계쪽에 대한 멘트임. 
   2. 이뻐
      - info>describe하면 숫자만 봐서 안 예뻐

85. 분석 순서
- info
- describe(boxplot그림)
- pairplot에서 그림, 

86. 카테고리?
- 판다스에서 지원하는 성별 같은 몇 없는 문자열/숫자열

87. describe에서 볼 수 있는것?
- 숫자 데이터

88. describe에서 더 보고 싶으면?
- include

89. 분석에서 필요한것?
-프로그래밍 하나, 통계 수학 하나, 도메인 지식

90. 어그리게이션 분석?
- 그룹으로 묶어서 날짜로 묶어서 최소 최대 보기 및 성별로 평균/합계 등 보기 할때 사용
- 예시) tips.groupby('sex').mean()


# 2개만 보고싶으면?
- 팬스 인덱싱
- tips.groupby(['sex','day'])['total_bill','tip'].mean()

# 어떤 상황에서 팁이 많은가 볼 수 있다
- tips.groupby(['sex','day','time'])['total_bill','tip'].mean()

91. 어그리게이션 어떤거 주의하고 어떤게 좋아?
- 개별적인거 확대할 수 있어 주의하고
- 경향을 보여줄 수 있어서 사용

92. 그래프 그리려면?
- .plot 하면 그래프 그리는거 나와요. 탭키 눌러봐요

93. 판다스 분석에서 할수 있는거 2가지
1. 기초통계분석을 위해 데이터 불러오기
2. 어그리게이션분석으로 그림 그리기

94. 공공데이터 불러올떄 옵션
- olds =pd.read_csv("C:\Python\download/ttt.csv",engine='python',encoding='cp949')
- olds

95. 연변
- 필요 없어
- 인덱스 쓰니까
    -1. 연번을 인덱스로 만드는 방법
    -2. 연변을 삭제하는 방법
- 인덱스: 맨 왼쪽에 있고 0부터 시작
    - index 컬럼이있는데 이 옵션을 줄게요
        - olds =pd.read_csv("C:\Python\download/ttt.csv",engine='python',encoding='cp949', index_col='연번')

96. 원본 저장
- olds_raw=old.copy()
---기본 인덱스 관련된 3가지----
1. 가져올때 정하는 방법
2. 가져와서 setindex
3. reset인덱스해서 기존의것 초기화시키기

------------------------------------
컬럼 지우는 방법
1. drop으로 컬럼 지움
- drop은 판다스에서 모든거 지울떄 사용
- olds.drop(columns='연변', inplace=True)
- olds.drop(label=0, inplace =True)
- old.drop(label=[2,3,4,])

97. 라벨?
- 가로든 세로든 지울 수 있게끔 할려고(axis=0, axis=1) 
- 가능한 중복된 의미가 없는게 좋은 데이터 소스니까요. 
- olds.drop(labels='소재지', axis=1) //소재지 지워

98. 이름 변경
- rename
- olds.rename({'소재지':'주소'})
- olds.rename({'소재지':'주소'}, inplace=True)
- olds.rename({'소재지':'주소'}, axis=1)
- tips.rename({'size':'size_'}, axis=1) //된다
- 어트리뷰트 방식으로는 안 됩니다.

99. 원본으로 돌아가는 방법@@


100. 해보고 잘 되면 그때 inplace=True옵션
- 재할당 하지 않고 반영

101. 파이썬을 날짜를 관리하는 패키지가 3개 있어
1. 타임 패키지
   - 속도 측정
2. 칼린더 패키지
   - 시간 날짜에 대한것 
3. 데이트 타임 패키지
   - 엄청난 단점 있엇 파이ㅏ썬 하는 사람은 잘 안쓰고 데이트 타임 유티이라는 써드 파티 많이 씀
   - import datetime
   - dir()
   
# dir했더니 거슬리는거 있음
- datetime: 시간 날짜 동시에
- date: 날짜만
- time: 시간만
- timedelta: 시각
- timezone: 표준시간대(미국등은 여러개니까)

102.dir했더니 아주 거슬리는 애가 있어
- 데이트 타임 패키지를 했는데 
- datetime: 시간 날짜 동시에
- date는 날짜만
- time은 시간만 관리
- timedelta: 시각
- timezone: 표준시간대(미국등은 여러개니까)

103. 관례상 시간 날짜 대해서 어찌 임포트해서 쓰니
- from datetime import datetime, timedelta, date, time 
- import datetime
- from datetime import datetime


104. datetime.strptime
- 문자열을 데이트 타입 객체로 관리할 수 있어요
-a=datetime.strptime('2019-07-23','%Y-%m-%d')

105. 파이썬 타입 변환 있다 없다? 
- 없다
- 그래서 인스턴스 하는 방식으로 사용합니다.

106. 넌파이는 타입 변환 있다 없다?
- 있다.
- 넌파이는 씨 기반이니까.

107. 공공 데이터 판다스에 불러 올떄 잘 안되면 해볼거
- import pandas as pd
-1. 엔진 2.인코딩 3. 잘된거 같으면 inplace True
- olds =pd.read_csv("C:\Python\download/ttt.csv",engine='python')
- olds =pd.read_csv("C:\Python\download/ttt.csv",engine='python',encoding='cp949', index_col='연번')

108. 인덱스 2개 이상 붙일 수 있는것-
- set_index, 멀티 인덱스
- 우리는 한개만 붙임
- olds.set_index('연번')

109. 판다스에서 지우는거3개
-drop
   -drop인덱스 하면 되겠죠? 근데 없어.
   - olds.drop(columns='연번', inplace=True)
- drop중에서 한번에 지울 수 있는 방법
   - olds.drop(label=[2,3,4,])
- reset_index
   - olds.reset_index(drop=True)
----------------
110. 넌파이에서 타입 변경?
- assype
- a.astype('float64')
- olds['기준일자'].astype('datetime64')

111. 타입 알고 싶을떄 디 타입 될까?
- 얘 넌파이 기반. 판다스는. 
- 넌파이 중요2개는 디타입이랑 쉐입
   - olds['기준일자'].dtype
- 쉐입 될까 안될까? 
   - olds['기준일자'].shape
- olds['기준일자'].astype('datetime64') ***


112소재지에서 '구'만 뽑아내고 싶어
- 너희 정규식 배웠음 ^-^
- 아까 오브젝트 타입이랬지 .str을 사용할 수 있음
- x=olds['소재지'].str

113. 밸류카운트
- 카테고리별로 갯수 세주는것
- tips.sex.value_counts() 
   >> 했더니 나녀 촏 앙려주네
Male      157
Female     87
Name: sex, dtype: int64

- loc는 이름으로
- tips.sex.value_counts().loc['Male']
   >> 157
- iloc는 숫자로 접근
- tips.sex.value_counts().iloc[0]
   >>157

114. 판다스에서 인덱스는?
- x축. 
- tips.sex.value_counts().plot.pie()
- 그림 그리고 싶으면 .plot
- 인덱스는 엑스축!!!점 플랏만 쓰면 돼

115. 문자열 추가기능?
- 점 str

116. 카테고리 추가기능?
- 점 cat
-t=tips.sex.cat

117. 카테고리에서 코드란?
- code: 문자열로 하면 비효율적이라서 숫자로 관리
- t.codes

118. codes를 통해 뭐가 편해져?
- 라벨링
-즉 여자0, 남자1로 관리

119. 유니크
- tips.sex.nunique()
- 카테고리 에서는 female, male 이외 쓰면 에러 발생
- 유니크하면 카테고리 이름 나오고, 
- 유니크 앞에 엔 유니크 하면 카테고리 갯수가 나와요


#인덱스슬라이싱
- loc iloc 열 다 했는데 
- 이제 복잡한거 해볼거에요.
- 기계학습해봤던 사람은 보스턴으로 리그레션했을거에요.


교수님----------
1. info
2. describe
3. head
4. tail
5. sample

#### 데이터 수집이 일정하게 되는지 여부를 체크하는거에요
- 최신 데이터하고 일년전 데이터가 다른 경우가 많아요
- 시간에 따라 축적된 양이 많을떄
--

120. 데이터 부분 보기.  데이터의 부분을 보는 명령어가 있어요
1. 앞에서 보기 
- mpg.head()
2. 뒤에서 보기
- mpg.tail()
3. 무작위로 보기
- mpg.sample()

121. 결측치 확인
- missingno
- !pip install missingno
- import missingno as mino
- dir() //처음 나왔으니 봐요

122. 그래프로 결측치 보기
- mino.matrix(mpg)
- 아까 mpg에서 미싱데이터 6개랬는데, 여기 선 6개가 horsepower있지? 그거야

123. 그래프
- mino.matrix(mpg)
- mino.dendrogram(mpg)
- mino.bar(mpg)
- mino.heatmap(mpg)

124. 히트맵
- 크면 잘 안나오는 에러 있음

125. 결측치 보기
- 그래프) 
- 식) ISNA
   - mpg.horsepower.isna()

126. 트루 펄스만 나오면 쓸 수 있느 기법?
- 마스킹
- mpg[mpg.horsepower.isna()]
   - 지금 넌값있는애만 찾을 수 있어요

127. 결측치 총값 구하기
- mpg.horsepower.isna().sum()

128. isna하고 isnull 같다?
- 네

129. 미싱데이터 처리하기 
- 1. 없애기
- 2. 가짜 데이터 넣어요. 

130. 미싱데이터 처리에서 없애는 방법
-  mpg.dropna()

131. dropna() 옵션
- mpg.dropna() 
#ShiftTab하면 보이겠지만 any가 디폴트라 그냥 하고, 나중에 반영을 위헤 inplace
- mpg.dropna(inplace=True)
- any: 하나라도
- all: 전부 다 이상하면 지워

132. dropna() 하고 반영하기 위해서?
- inplace

133. na 채우려고 하면?
- mpg.fillna(3).loc[32]

134. 콜드스타트?
- 가짜 데이터 만들기
- 데이터가 적어서 알고리즘 잘 적용 안되는 경우

135. 인터폴레이트
- 도메인 따라 다 달라. 다 하기에 시간 많이 걸려
- 싸이 파이에 많아
- from scipy import interpolate

# mpg.fillna(3).loc[32]
<- 이거 결측치 할떄 채워넣었음

136. 가로 세로 값 변경
- mpg.describe().T

137. 그래프 그리기
- mpg.describe().T.plot.bar()

138. 현재 컬럼들 봐볼까
-mpg.columns 
> Index(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',
       'acceleration', 'model_year', 'origin', 'name'],
      dtype='object')
- mpg.columns[:-2]
> Index(['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',
       'acceleration', 'model_year'],
      dtype='object')


139. 그래프에서 'weight' 하나 빼자. 무식하게 말구
- mpg[mpg.columns[1:4]]
- mpg[mpg.columns[:-2]]

140. reg로 특정 값(weight) 이외의 것만 나오게 하기
- mpg.filter()
- mpg.filter(regex="[^weight]")

141. 열 뽑는 방식 (최소)★★★★★
1. 딕셔너리로 
2. 어트리뷰트로 뽑기
3. 팬시 인덱싱 
-  컬럼즈 (팬시 인덱싱 응용)
4. 필터 이용해서 내가 원하는 컬럼만 뽑기
- 아까 레귤러
5. select_dtype
- 데이터 타입(플롯, 오브젝트) 따라 뽑기

142. 그래프 그릴때 인덱스?
- 엑스축
- mpg.filter(regex='[^weight]').describe().T
- mpg.filter(regex='[^weight]').describe().T.plot.bar()


143. 쌓인 바플랏 그리기
- mpg.filter(regex='[^weight]').describe().T.plot.bar(stacked=True)

144. 그래프 out에 이상한 문자 나오는거 지우기?
- 끝에 세미콜런

145. 페어플랏
- 색상이 있는 그래프
- import seaborn as sns
- sns.pairplot(mpg)

146. 페어플랏 색상
- classification 가능하면 색상 진한편
- 리그레션은 색상 잘 안 칠해짐 그래서 굳이 안 해도 돼

147. 공분산
- mpg.cov()

148. 상관계수
- 1이면 관련성 높고, -1일 수록 상관성 잘 안보여
- 피어슨, 스피어만, 캔달
- mpg.corr()

149. 상관계수 관계 잘 안보이면 쓰는거?
- heatmap
- sns.heatmap(mpg.corr(), annot=True)

150. 다중 공산성**********
- 피쳐 셀렉션에 써요

151. 박스플랏 쓰는이유?
- mpg.boxplot()
- 아웃라이어 확인하기 위해서. 
- 딥러닝 뉴럴네트워크에서 시작이 노말라이제이션인데, 이걸 캐치하기 좋아

152. Outlier **************
- 1만 시간의 법칙
- 공부하세요 ^-^;
- 평균에 벗어나는 ;;
- pyod 라이브러리 기똥차
- 예측 모델의 성늦 낮춰서 보통은 없애는데, 이게 키일 수도 있어

153.  아웃라이어 체크하는 박스 플랏 퍼센트는?
- 볼 수 있어요. 안 좋은 예시인데 보여만 줄게
- mpg.describe([.005,0.5,.75])

#여기까지는 모든 데이터 동일하고 이제는 다른 접근 방식

154. Hadley Wickham
- Rstudio 만든 사람
- 최근 논문: tidy data

155. 와이드포맷
- 엄청 자주 있는 실수
- pew=pd.read_csv('C:\Python/pew.txt', delimiter='\t')

#####3
롱폼
- 멜트로 바꾼다
   - pew.melt(['religion']) 
- 그룹바이 및 디스크라이브로 의미있게 볼 수 있다
   - pew.describe()
   - pew.groupby('variable').mean()

# 이름 바꾸고 싶을떄
- pew.melt(['religion'], var_name='문', value_name='근영')
=====================


156. na값 없애기
- weather=pd.read_csv('C:\Python\weather.txt', delimiter='\t')
- weather //여기에 na 많음

157. 여러개 중에서 몇 개 컬럼만 보고파
- weather.melt(weather.columns[:4])

158. na 없애기
- weather.melt(weather.columns[:4]).dropna()

159. 컬럼명 변경
- weather.melt(weather.columns[:4]).dropna()

160. 멜트보다 조금더 예쁘게 만들어주는 (롱폼으로)
- pd.wide_to_long

161. pd.wide_to_long
- pd.wide_to_long(weather, 'd', ['id', 'year','month','element'],'')
- 공통된거 그룹화하여 보기 좋게 함

162. 타이드 데이터 생성
- 멜트, 데이터 타입, 이름 변경해서 만드는 짓
- 빌보드, 웨더 등에서
- 와이드를 롱폼으로 바꾸는 등으로 해서 하는거 알죠?

---
1. Groupby를 진행할게요
- (sex/age)모으고, 그 다음에 집계 함수(count/sum 등)
- 다른 의미로 스플릿하고 집계함수 하는 식 
- 그룹바이가 하나씩 돌아가면서 그 데이터 하나씩 나눠서 시군구로 자르고, 강남구 만큼 카운트. 강동구 만큼 카운트. 그걸 어플라이.

2. 최종적으로 각각 합쳐서 한번서 보여주는걸 ?
-컴바인

3.  구룹바이 숨어있는 기법?
- sac 기법

4. 스플릿하고 카운트 하는 순간?
- 어플라이 지원돼.

5. 어플라이 지원되는 순간 컴바인 돼?
-아니. 
- 집계함수 하면 넌파이 특성상 리덕션이 되어서 어그리게이션(섬 카운트)에서, 굳이 컴바인 할 필요가 없어

6. Grouper

7. Counter
- 그룹바이 자주하니까 반복되는거 중에 몇 개 있는지 알려주는거
- from collections import Counter
- Counter('aaaaabbbeee')
>> Counter({'a': 5, 'b': 3, 'e': 2})

8. Counter랑 유사한데 이쁜거(카운터는 딕셔너리 )
- value_counts()
- data['업종대분류'].value_counts()
소매          34650
생활서비스       25164
학문/교육       19140
음식          19080
관광/여가/오락     9672
부동산          6522
숙박           5430
스포츠          4092
Name: 업종대분류, dtype: int64

9. most_common 무슨 형식?
- 딕셔너리 말고 리스트 형식

10. 밸류 카운트?
- 파이썬의 카운트 이용해서 만드는거
- 우리 이거 생각 안나면 그냥 카운트 생각해서 쓰면 돼

11. 소트 한 이후에 결과 안나온다
- 뮤터블이다
- x.most_common().sort() 

12. sort() 도움말 보면 얜 무슨 기법? *있네
- 키워드 기법
- 그리고 INPLACE도 써있네

13. 리스트 형태면 값으로도 보낼 수 있니?
- 응
- 소트에서 나옴. 갑으로도 나와
-s=x.most_common()
-s
[('소매', 34650),
 ('생활서비스', 25164),
 ('학문/교육', 19140),
 ('음식', 19080),
 ('관광/여가/오락', 9672),
 ('부동산', 6522),
 ('숙박', 5430),
 ('스포츠', 4092)]

- s.sort(key=lambda t:t[1])
-s
[('스포츠', 4092),
 ('숙박', 5430),
 ('부동산', 6522),
 ('관광/여가/오락', 9672),
 ('음식', 19080),
 ('학문/교육', 19140),
 ('생활서비스', 25164),
 ('소매', 34650)]

14. #### value_counts()
- 이뮤터블임. 시리즈 방식. 얘도 소트가 있음. 
- 노말라이즈) 전체에 비율이 나타남.
- 얘는 자유자재로 변경 못함. (파이썬은 가능함)

- 파이썬의 카운트 이용한 것. 따라서 일반적인 카운트는 자유자재로 변경 가능함

15. 밸류 카운트의  노말라이제이션?
- 비율로 나타낼 수있음
- ss=data['업종대분류']
- ss.value_counts(True).plot.pie()
- ss.value_counts(True,False).plot.bar()


16. 그래프 모양 바꾸기
- import matplotlib.pyplot as plt
- plt.style.available
- plt.style.use('ggplot')
- ss.value_counts(True,False).plot.bar()
   <- 아까랑 같은데 지지플랏 적용되서 배경 회색에 빨간 차트로 이쁘죠?
   <- 우리 디쓰리 있는데 가시화에서 배워서 예쁘게 하는거 배울거야. D3. js

17. DS.js
- Data Driven Documents
- 얘가 좀 빡센데 로우레벨까지 만들 수 있음
- 시각화 예쁘게 차트 그리는거 (8월 14일) 

18. 동시에 여러 카운트/평균/ 표준편차 하고 싶어 이때 사용하는거?
- agg 및 aggregate(두개 다 같은거야)
- 타이디 데이터면 그룹바이 가능
- t=data[(data['광역시도']=='서울특별시' )& (data['업종대분류']=='부동산')].groupby('시군구')
- import numpy as np
- t.agg(['count','sum'])
- t.agg(['count',np.std])

19. transform?
- 그룹바이했던걸 각각의 값으로 변경하는 것
- t.transform('count')
- t.transform('sum') //이건 에러

20. 알아야 할 것 
내부구조
sac 기법
집계함수
어그리게이트로 2개이상 
트렌스폼으로 각각의 값으로 변신가능

21. # 파이썬에서 오아(OR)가 나오면 
- 인 테크닉 사용
- t=data[(data['광역시도']=='서울특별시')|(data['광역시도']=='강원도')& (data['업종대분류']=='부동산')].groupby('시군구') 대신
- data['광역시도'].isin(['서울특별시','강원도'])



22. 여러개 조건있을떄 사용?
- 이즈 인
- data.where(data['광역시도'].isin(['서울특별시','강원도']))
인은 연산자고
이스인은 함수

23. 이즈는 무슨 함수?
- 프레디케이트 함수로 트루 펄스 반

24. 넌파이에도 ISIN 있나요?
- 네
- np.isin

25. 판다스 접근2방식
1. 넌파이
- 대부분 넌파이 그대로 가져옴
2. 분석 관점

26. 판다스 이즈
- 이즈
-이즈 앤에이(=이즈 널)
- 이즈 인


27. na 아닌거 알고 싶을때(isna 반대)
- data.notna()

28. 그룹바이 3총사
- 크로스테이블: 내가 원하는거 빨리 만들어줌
- pd.crosstab(data['광역시도'],data['년'])
- pd.crosstab(data['광역시도'],data['년'], data['수'], aggfunc='count')

29. SCI
- stack은 column을 index로 만드는거
- 언스택은 인덱스를 컬럼으로 만드는것
-data.groupby(['년','광역시도']).count().unstack()
- 이거 어렵죠? 그래서지원하는게 크로스 탭 
   - 그래프 그리기 좋아
   - 크로스탭 

30. 그룹바이 삼총사★★★★★
- 1.그룹바이
- 2.크로스탭
- 3. 피봇테이블

31. 피봇테이블 생성
- 기본이 평균값
data.pivot_table('수','년', aggfunc='count')

32. pivot과 pivot_table 동일?
- 다름

33. 피봇테이블 언제 쓰면 좋아?
- 부분합 할떄 사용ㅎ면 좋아(마진)

34. 그래프 예쁘게 그리기
-  ggplot은 그랜마 오브 그래픽으로 R 했죠 이쁘게?
    - 그림 그리는 것을 문법화하여 자동적으로 그림 그리게 한 것
- Vega
    - 문법만 맞추면 이쁘게 그려줌
    - 문법 모르면 도루묵.
    - Visualization Grammar
    - 그래프 문법 알면 간단하고 이쁘게 그릴 수 있어요
- pdvega
   - 문법 몰라도 괜찮아
   - !pip install pdvega
   - PNG로 익스포트 할 수 있음.

35. 추천 시스템 종류 2가지
- 협업 필터링(사람기반)
- 콘탠츠 기반 필터링(과거 데이터 기반)

36. Sernedipity
- 신선한 추천
- 나랑 유사한 사람이 나랑 같은거 다했는데, 하나만 달라. 그러면 그 안 한 한개를 추천해줄 수도 있지
- 물건 파는 사람들이 좋아하고, 이게 대부분의 쇼핑몰에서 지원하는 추천방식이야

37. 추천 시스템 문제점
1. 콜드 스타트 문제
2. 롱테일 문제
3. gray sheep/black sheep 문제

38. 콜드 스타트?
- 힘들고 추운 상태에서 시작
- 새로운 물건 대해 패턴 데이터가 없어. 그래서 성능 그닥

39. 롱테일
- 항상 사람들에 많이 팔린건 유사도 높아서 비대칭적 쏠림이 일반적.
- 이거 해결하려고 했는데 20프로의 또 20프로에 쏠리는 식

40. gray sheep/black sheep 
- 독특한 일반적이지 않는 경우
- 선호도가 이상한 사람. 왔다갔다 애매한 사람

41. 콘텐츠 기반 필터링
- 핵심) 내용에서 어떻게 유사도를 찾는가

42. 하이브리드 시스템
- 협업+ 콘텐츠 기반 

43. 매트릭스
- 가로:피쳐
- 세로:벡터

44. 사람끼리 유사도 찾는 방법
- 코사인 시뮬러리티 (코사인을 이용하여 유사도 처리)
   - 판례) 유사도 싹다 저렇게 해서 찾는거야.

45. 아이템 베이스 콜라보레이트
- 콘텐츠 속성 고려하지 않고
- 사람들이 과거에 어떤거 했는지를 보고 유사도를 측정
- 디비에 저장 하는등 하면서 캐시로 빠르게 사용 가능+ 선응도 좋았다. 

46. 컨텐츠 베이스 콜라보레이티브
- 메타데이터(콘텐츠 고유의 속성) 만으로 유사도 고려하여 테이블 생성

47. 콜라보레이티브 차이
1. 사람이 한 과거의 행동(아이템기반/유저기반)
2. 컨텐츠 베이스는 행동과 상관없이 콘탠츠고유의 속성만 가지고 유사도를 계산


48. 파일 불러오기
- item= pd.read_csv('C:/Python/ml-100k/ml-100k/u.item', sep='|', header=None, engine='python', encoding='latin1')
- 구분지을떄 sep 혹은 delimeter
- u.data할때 \로 구분되는 경우
   - data=pd.read_csv('C:/Python/ml-100k/ml-100k/u.data', engine='python', header=None, sep='\t')
   - sep='\t'되야한댔어

49. 불러온거 이름 변경
- info.rename({1:'table', 0:'number'}, axis=1)

50. # 우리는 추천 시스템 만들떄 처음에 할일?
- 테이블 만들기
- 테이블의 구성요소는 몇 개? 
    - 왼쪽에 유저, 오른쪽에는 영화 제목, 그리고 가운데 값에는 영화 평점이 들어가야해
    - 이런 정보 들어간 테이블이 data 테이블 
        - user, item, rating있네요. 
        - 이거 ㄹ이용하면 필요한 행렬 만들 수 있어
 
51. userid를 왼쪽에 넘기는건?
- 인덱스 

52. 분리된 테이블(데이터프레임끼리 ) 조인하는거 이너조인?
- merge

53. 컬럼끼리 합칠ㄸ
- 조인

54. pd.to_datetime(data.timestamp)?
- 시간 데이터 변경
- timestamp는 1970년1월1일부터 플러스 일

55. pd.to_datetime(data.timestamp, unit='s') 
- 판다스 나노 세컨드인데, 이거 세컨드로 바꿨음

56. 삼총사?~
피봇은 어그리게이션 펑션이 아니야. 값을 그대로 박아

57. 코릴레이션? 유사도 구하기
- 컬럼끼리 관계
- reco.T.corr()

58. @@@@@@@@@@@@
reco_corr.loc[42].sort_values(ascending=False)

59. 재활용 리코멘데잇연
- def recommendation_user(user_id, k):
    return reco_corr.loc[user_id].sort_values(ascending=False)[1:k]
- recommendation_user(1,5)

60. 내가 본 영화 어떻게 거를까(나 42번이라할떄)
-data[data.user_id==42].item_id

61. 916번과 41번의 본 영화 차이
- len(set(data[data.user_id==916].item_id)- set(data[data.user_id==42].item_id))

62. # duplicate
- 판다스는 중복 여부 알려주어 겹치는거 없애ㄴ는

63. concat
- s와 s1 테이블 붙이기
- pd.concat([s,s1])

64. 중복여부
- x=pd.concat([s,s1]).duplicated()

65. 중복 지우기
- x.drop_duplicates(keep=False)

# 인플레이스는 뮤터블 재할당하고 리턴해야해
- 뮤터블은 리턴이 넌

# 머지하려면 컬럼 이름이 같았어야해
# rename (컬럼즈하면 액시스 안 써도되고, 컬럼즈 안쓰면 액시스 이꼴 1 써야해)

66. 피클링
- 객체 자체 저장- 텍스트가 아닌 객체 자체를 저장한다. 
- 객체 자체 저장하는 방법?

67. Persist
- 지속 가능한. CS에서는 파일 저장 용어


69. Flat file
- 구조가 없음- txt나 csv처럼 보통 열리는 것처럼. 
- 사람이 볼 수 있는 형태로 만들어진 텍스트 파일

70. raw file
- 구조가 있음

71. 파일 구조
- 경로+이름+확장자
- 경로가 만일 없으면
    - 1.경로)현재 작업하고 있는 파일과 동일한 위치에 있다는 것 
        - 윈도우) 역슬러시,
        - 리눅스 유닉스) \
 #### 문제점) 파이썬은 어느 운영체제인지 상관없이 실행됨. 
     - 파이썬 3.4부터 testlib 지원하여 같은 형태 사용해도 다양한 운영체제에서 사용할 수 있게 만들었음
    - 2.이름)
    - 3.확장자)

72.운영체제마다 다른거 아는법?
    - import platform
    - 파이썬은 플랫폼 디팬던트 안하게 만드는 중 (자바처럼)
    - 약간 플랫폼마다 차이가 있긴해 특히 파일 경로 설정하는 경우에

73. 파이썬 open
- 내부적으로 io 객체라는게 있음
- 이 인풋 아웃풋 사이에 문을 연다는 개념.
    - 그래서 쓰거나 읽거나 하는것

74. 유닉스 리눅스 맥은 
- utf-8, 

#  utf-8은 에러날 수 있음
- 파이썬 자체는 일반적으로 utf-8 기본

75. 윈도우는 
- cp949 일 수 있음.(내가 그럼)

76. 한줄씩 불러들이기
- file.readline() 
- 이터레이터/제네리이터개념) 한줄씩 뽑아내요 메모리에서.
- 'asd ffdasf\n' (끝에 \n)

77. readline()
- 맨 끝에 에러 안내고 공백 보여줌

78. file.readlines() 
-리스트니까 덧붙일 수 있네
- 대신 얘는 리스트니까 next 안됩니다**
- 모드가 쓰기가 아니면 덧붙일 수 없어요
- 얘 next하니까 안돼 (아까 moon.txt는 dir해서 next 있길래 이터레이터로 next 했는데)

79. 파일 쓰기모드 주의?-
- close 안하면 작성한 내용이 파일에 반영 안되네요
- 그래서 우리 배운거? with

80. with 내부에 숨은ㅇ거?
- enter exit

###파이썬에는 사용이 끝난 메모리를 정리해주는 가비지 컬렉터가 있어서 파일을 닫지 않아도 가비지 컬렉터가 파일을 닫아줍니다. 하지만 프로그래머가 파일을 직접 닫아야 하는 이유는 다음과 같습니다.

- 너무 많은 파일을 열어 두면 그만큼 메모리 공간을 차지하므로 성능에 영향을 줄 수 있습니다.
- 파일을 닫지 않으면 데이터가 쓰기가 완료되지 않을 수도 있습니다. 운영체제는 파일을 처리할 때 성능을 위해서 데이터를 버퍼(임시 공간)에 저장한 뒤 파일에 씁니다. 때에 따라서는 파일이 닫히는 시점에 버퍼의 내용이 파일에 저장됩니다.
- 이론적으로 운영체제에서 열 수 있는 파일의 개수는 한계가 있습니다.
- 운영체제에 따라 파일을 열었을 때 파일을 잠금 상태로 처리하는 경우가 있습니다. 실질적으로 파일 처리가 끝났더라도 파일을 닫지 않으면 다른 프로그램에서 파일을 사용할 수 없는 상태가 됩니다.

보통은 파일을 닫지 않아도 큰 문제가 없습니다. 하지만 실무에서는 사소한 실수로도 큰 문제가 발생하는 경우가 있으므로 파일을 정확히 닫는 습관을 기르는 것이 좋습니다.


81. pickle
- 객체 의미 가지며 저장
- 파이썬 공식 라이브러리가 있음
- name은 문자열, scres를 딕셔너리로 저장해본다고 해보자
- 이걸 텍스트파일로 저장한다하면? 무조건 텍스트로 저장이 되긴해


82. 피클 확장자
- 안 중요

83. 피클에서 b는 어떻게 저장?
- 바이너리로. 우리 눈에 안보이게
- dump load 쓰네 제이슨처럼
- import pickle

with open('moon.pickle', 'wb') as file:    # 확장자 마음대로
    pickle.dump(name, file)
    pickle.dump(scores, file)#dump는 제이슨에서 저장할때

with open('moon.pickle', 'rb') as file:    #b는 바이너리라 우리 눈에 안보여
    name = pickle.load(file)
    scores = pickle.load(file)
    print(name)
    print(scores)


84. 피클은 어떤 순서로 불러오나?
- 순서별로 불러옵니다.
- 한개만 불러올 수 있는데 약간 불편하네요

86. 저장_피클
import pickle
 
name = 'Abder'
website = 'http://abder.io'
english_french = {'paper':'papier', 'pen':'stylo', 'car':'voiture'} # dictionary
tup = (31,'abder',4.0) # tuple
 
pickled_file = open('pickled_file.pickle', 'wb')
pickle.dump(name, pickled_file)# 저장시키는 방법이 있어 
pickle.dump(website, pickled_file)
pickle.dump(english_french, pickled_file)
pickle.dump(tup, pickled_file)

87. 피클러
- 클래스 방식으로 사용
- 인스턴스 하는것도 맞아요
- 순서에 조금더 유연할 수 있어요. 

from pickle import Pickler
 
name = 'Abder'
website = 'http://abder.io'
english_french = {'paper':'papier', 'pen':'stylo', 'car':'voiture'} # dictionary
tup = (31,'abder',4.0) # tuple
 
pickled_file = open('pickled_file.pickle', 'wb')
p = Pickler(pickled_file)
p.dump(name); p.dump(website); p.dump(english_french); p.dump(tup)

88. 언피클러
from pickle import Unpickler
 
pickled_file = open('pickled_file.pickle', 'rb')
u = Unpickler(pickled_file)
name = u.load(); website = u.load(); english_french = u.load(); tup = u.load()
 
print('Name: ')
print(name)
print('Website:')
print(website)
print('English to French:')
print(english_french)
print('Tuple data:')
print(tup)

# 각 커다란 프레임워크는 자기만의 피클링 방식 지원
#### 경로 안 적혀있으면 현재 작업중인 디렉토리에 저장
- 확장자에 뭐라고 적혀있어요? 
    - npy
- 확장자 안 써도 불러줌


### x=np.s하면 
- savetxt
- savez 등 나오는데
- savetxt는 아무의미없지만 savez는 집! 압축


#### 다른 쥬피터 파일 열고 하려고 하면
- x=np.load('npaaaa') 이거론 에러나
- x=np.load('npaaaa.npy')해야지. 확장자까지 써야하니까




--
1. 시간 날짜 변경할떄
   - pd.to_datetime

2. 국제 년/월/일 국제표준
- 시간 날짜 표현할떄 yyyy-mm-dd
- 이건 외부 프레임워크 잘 받아들임

3. #### 이 날짜 넣어둔 temp의 타입은 뭘까?
- 시리즈
239   2017-12-21
240   2017-12-22
241   2017-12-23
242   2017-12-24
243   2017-12-25
244   2017-12-26
245   2017-12-27
246   2017-12-28
247   2017-12-29
248   2017-12-30
249   2017-12-31

4. 안에 넣은게 데이트 타입이면 뭘 넣을 수 있냐면
    - resample

5. 리샘플?
        - 시간날짜 간격에 따라서 합칠 수 있음

# 리셈플 유무는 타입 속성 따라 달라
        - dust['날짜'].resample('D') 하면 타입 에러 나
            - 일단 현재는 속성이 시리즈임 type(temp) 보면 알 듯이 

6. dateutil
- 시간날짜 문자를 변환해줘(한글은 안됨)
- 자동으로 파싱해서 날짜로 만들어줘.
- from dateutil import parser
- date = parser.parse("4th of July, 2015")
>> datetime.datetime(2015, 7, 4, 0, 0)

7. strptime, strftime 차이
- strftime: 시간날짜 객체를 스트링으로 변경
   - a.strftime('%A')
      -'Monday'
- strptime: 스트링으로부터 시간날짜 객체를 반환
   - 즉 파서와 유사함

-from dateutil import parser
- date = parser.parse("4th of July, 2015")
- date
>> datetime.datetime(2015, 7, 4, 0, 0)

8. dateutil : 얘는 strptime보다 더 잘 돼 
   - 근데 년/월/일은 잘 안 돼
   - 국제 표준에서는 괜찮
9. to_datetime
- 문자열에서 시간 날짜로 변경
   -내부적으로 dateutil 사용

# 자동적으로 파싱 되는데 타입하면 이상해
- import pandas as pd
- date = pd.to_datetime("7 30 2018")
- date
> Timestamp('2018-07-30 00:00:00')

#자동적으로 파싱 되는데 타입하면 이상해
- type(date)
> pandas._libs.tslibs.timestamps.Timestamp
   >> 아까 타입해서는 시리즈였는데 지금은 타임스탬프가 나왔네

10. 타임스탬프?
- 파이썬의 데이트타임을 가지고 와서 상속했어 판다스
- 그래서 하나씩 조작을 할때 파이썬 문법을 사용해야함

11. 타입해서 언제 타임스탬프? 언제 시리즈?
# 한개 반환) 타임스탬프
# 여러개 반환) 시리즈

12. pd.to_datetime
- 데이터를 한개 넣었을떄 여러개넣었을때 결과물이 달라
-즉! 시리즈 넣으면 시리즈,
- 하나 넣으면 타임스탬프(= 파이썬 타임데이트와 같음),
- 한개도 시리즈도 아닌애 넣었더니 데이트타임 인덱스

- t= pd.to_datetime(['2014-07-04', '2014-08-04',
                          '2015-07-04', '2015-08-04'])
- type(t)
>> pandas.core.indexes.datetimes.DatetimeIndex

13. 그림그리려면 임포트 해야하는
- %matplotlib inline

14. date_range()
- 시간을 자동으로 만들어줌
- 옵션이 많음
- index= pd.date_range('1/1/2000', periods=9, freq='T')
- series= pd.Series(range(9), index=index)
- series
>>>
2000-01-01 00:00:00    0
2000-01-01 00:01:00    1
2000-01-01 00:02:00    2
2000-01-01 00:03:00    3
2000-01-01 00:04:00    4
2000-01-01 00:05:00    5
2000-01-01 00:06:00    6
2000-01-01 00:07:00    7
2000-01-01 00:08:00    8
Freq: T, dtype: int64



15. #### 시리지의 ~가 ~면 resample 사용가능 @@@@@@
- 리샘플 하려면 인덱스가 '데이트타임 인덱스' 여야 한다 혹은 뭐시기 뭐시기 있어

16. #### 리샘플?
- dust['2017-03']
-  리샘플 하려면 인덱스가 '데이트타임 인덱스' 여야 한다 혹은 뭐시기 뭐시기 있어
- 리샘플 뒤에 집계함수 있어야 함

17. ### 교수님 정리
- 판다스는 넌파이 가져왔는데
- 판다스는 데이트 타임에 관련해서는 파이썬에서 가지고 왔어요 
    - 그중  아까 넌파이 편한기능 가져와서 합쳐 쓸 수 있게했고
    - 그 이름도 넌파이에서 가지고 왔대요
- 넌파이는 국제 표준만 2017-01-02 이런식만 파싱 가능
- import numpy as np
- date=np.array('2015-07-04', dtype=np.datetime64)
- date
>> array('2015-07-04', dtype='datetime64[D]')

- date+np.arange(12)
>> array(['2015-07-04', '2015-07-05', '2015-07-06', '2015-07-07',
       '2015-07-08', '2015-07-09', '2015-07-10', '2015-07-11',
       '2015-07-12', '2015-07-13', '2015-07-14', '2015-07-15'],
      dtype='datetime64[D]')



t= pd.to_datetime(['2014-07-04', '2014-08-04',
                          '2015-07-04', '2015-08-04'])
type(t)

index = pd.DatetimeIndex(['2014-07-04', '2014-08-04',
                          '2015-07-04', '2015-08-04'])
data = pd.Series([0, 1, 2, 3], index=t)

data.loc['2014-07']

18. to_datetime
- 파이썬 기반
- 파이썬에서 쉽게 변해주는게 데이트 유틸리티
- 그걸 피디점 투데이트타임에서 구현했음
- 그래서 저 아래에 이상한 것들. 국제 표준 아닌 것들도 다 파싱할 수 있음 4th of July, 2015',
- 그래서 에러가 안생김(넌파이였으면 생겼을거야)

19. to_datetime 이름은 어디서 따옴?
-  넌파이따왔음(0000-00-00)
- DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-06', '2015-07-07',
               '2015-07-08'],
              dtype='datetime64[ns]', freq=None)

20. to_period

-  기간 / 시각
- 얘는 빈도가 적혀있음
- dates.to_period('H')
    - PeriodIndex(['2015-07-03 00:00', '2015-07-04 00:00', '2015-07-06 00:00',
             '2015-07-07 00:00', '2015-07-08 00:00'],
            dtype='period[H]', freq='H')
- 그런데 to~index는 빈도가 안 적혀있을거야 

21. Timedelta
- 파이썬은 한개만
- 판다스는 넌파이처럼 벡터라이즈 지원해서 동시에 여러개 계산 가능

22. Time delta_range
- 자동으로 구간을 만들어줌

23. Period_range
- 자동으로 구간 만들어줌

24. Propeht
- facebook
- 가장 유명한게 파이토치. 딥러닝(페이스북 공개)
- 프로핏이 현재 나온 시계열 알고리즘중에 좋고 사용이 편함
- R, python지원(더 협소하게는 판다스 지원)

25. fb prophet 뭐위해 사용?
- 예측분석 
- 보통 R 기반
- Propeht
    - 예측분석에서 유명한 사이킥과 완전 똑같이 사용될 수있음
    - 설치 잘 안돼 종속성 때문
- facebook 지원. 



26. 설치
- !pip install fbprophet

27. #### 설치문제
- 공식사이트에 보면 설치하는 방법이 적혀있음.
- 우리는 아나콘다 사용하니까 그 방식 이용해서 설치해볼게요
- 아나콘다프롬프트 관리자 권한으로 열고
    - conda install gcc
    - y
    - conda install -c conda-forge fbprophet.
        - 느리게 다운 받아지는듯하면 눌러서 ?! 진행되게
from fbprophet import Prophet
data=pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/master/examples/example_wp_log_peyton_manning.csv')
m=Prophet()

28. 최적화
- m.fit(data)
- vars(data)

29. fbplot
1. 임포트 -클래스 임포트
- from fbprophet import Prophet
- data=pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/master/examples/example_wp_log_peyton_manning.csv')
2. 인스턴스화
- m=Prophet() #인스턴스화 . 클래스이고 인스턴스화를 해서 사용행햐ㅐ
3. fit해서 최적화한다.
- m.fit(data)
- f=m.make_future_dataframe(periods=365)
- f

4. 프레딕트
- m.predict(f)
- 원하는거만 보려면 팬시인댁싱
- m.predict(f)[['ds','yhat','yhat_lower','yhat_upper']]
 혹은
- forecast=m.predict(f)
- forecast[['ds','yhat','yhat_lower','yhat_upper']]

5. 결과는 
- 플랏으로 보거나/플랏.component
- m.plot(forecast)
- m.plot_components(forecast)
--------------------
<fbprophet>
- import pandas as pd
- dust= pd.read_csv('C:/Python/dust/dust.csv',engine= 'python', encoding='cp949')
- dust
- dust.info()
- pd.to_datetime(dust['날짜'])
- dust['날짜'] = pd.to_datetime(dust['날짜'])
- dust #레인지 인덱스라 리샘플 안됨
- dust['날짜'].resample 
- type(temp)

## dateutil
- 시간날짜 문자를 변환해줘(한글은 안됨)
- 자동으로 파싱해서 날짜로 만들어줘.
- from dateutil import parser
- date = parser.parse("4th of July, 2015")
>> datetime.datetime(2015, 7, 4, 0, 0)

### trptime, strftime 차이
- strftime: 시간날짜 객체를 스트링으로 변경
   - a.strftime('%A')
      -'Monday'
- strptime: 스트링으로부터 시간날짜 객체를 반환
   - 즉 파서와 유사함

-from dateutil import parser
- date = parser.parse("4th of July, 2015")
- date
>> datetime.datetime(2015, 7, 4, 0, 0)

8. dateutil : 얘는 strptime보다 더 잘 돼 
   - 근데 년/월/일은 잘 안 돼
   - 국제 표준에서는 괜찮
9. to_datetime
- 문자열에서 시간 날짜로 변경
   -내부적으로 dateutil 사용

# 자동적으로 파싱 되는데 타입하면 이상해
- import pandas as pd
- date = pd.to_datetime("7 30 2018")
- date
> Timestamp('2018-07-30 00:00:00')

#자동적으로 파싱 되는데 타입하면 이상해
- type(date)
> pandas._libs.tslibs.timestamps.Timestamp
   >> 아까 타입해서는 시리즈였는데 지금은 타임스탬프가 나왔네

10. 타임스탬프?
- 파이썬의 데이트타임을 가지고 와서 상속했어 판다스
- 그래서 하나씩 조작을 할때 파이썬 문법을 사용해야함

11. 타입해서 언제 타임스탬프? 언제 시리즈?

##  pd.to_datetime
- 데이터를 한개 넣었을떄 여러개넣었을때 결과물이 달라
- 즉! 시리즈 넣으면 시리즈,하나 넣으면 타임스탬프(= 파이썬 타임데이트와 같음),한개도 시리즈도 아닌애 넣었더니 데이트타임 인덱스 ★

- t= pd.to_datetime(['2014-07-04', '2014-08-04',
                          '2015-07-04', '2015-08-04'])
- type(t)
>> pandas.core.indexes.datetimes.DatetimeIndex


### 한개 반환) 타임스탬프★
### 여러개 반환) 시리즈★
### 한개도 여러개도 아닌경우) 데이트 타임 인덱스★

# 그래프
- %matplotlib inline
- dust.plot.line()


# 리샘플
- dust['2017-03']
-  리샘플 하려면 인덱스가 '데이트타임 인덱스' 여야 한다 혹은 뭐시기 뭐시기 있어
- 리샘플 뒤에 집계함수 있어야 함

t= pd.to_datetime(['2014-07-04', '2014-08-04',
                          '2015-07-04', '2015-08-04'])

type(t)


index = pd.DatetimeIndex(['2014-07-04', '2014-08-04',
                          '2015-07-04', '2015-08-04'])

data = pd.Series([0, 1, 2, 3], index=t)
#### 셋인덱스 하고 나니까 되는거래
dust.setindex('날짜', inplace=True) #
dust['2017-03'] #뀨 안 되었었음

## 교수님 정리
- 판다스는 넌파이 가져왔는데
- 판다스는 데이트 타임에 관련해서는 파이썬에서 가지고 왔어요 
    - 그중  아까 넌파이 편한기능 가져와서 합쳐 쓸 수 있게했고
    - 그 이름도 넌파이에서 가지고 왔대요\
- 넌파이는 국제 표준만 2017-01-02 이런식만 파싱 가능


## fb prophet★★★★★★★★★★
# 시계열 분석한다 할때 이거로 돼.
- 최신
#### 예측 분석
- 보통 R 기반으로 사용함
- Propeht"
    - 예측분석에서 유명한 사이킥과 완전 똑같이 사용될 수있음
        - 설치 잘 안돼 종속성 때문
            - 아나콘다. 의존성 관리하기에 시간이 오래걸려요. pip는 의존성 관리 안해서 이거 까는 순간 다른게 안 될 수있어요.
- facebook 지원. 
- 가장 유명한게 파이토치. 딥러닝(페이스북 공개)
- 프로핏이 현재 나온 시계열 알고리즘중에 좋고 사용이 편함
- R, python지원(더 협소하게는 판다스 지원)

-------------
!pip install fbprophet

#### 설치문제
- 공식사이트에 보면 설치하는 방법이 적혀있음.
- 우리는 아나콘다 사용하니까 그 방식 이용해서 설치해볼게요
- 아나콘다프롬프트 관리자 권한으로 열고
    - conda install gcc
    - y
    - conda install -c conda-forge fbprophet.
        - 느리게 다운 받아지는듯하면 눌러서 ?! 진행되게

- from fbprophet import Prophet
- data=pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/master/examples/example_wp_log_peyton_manning.csv')
- m=Prophet() #인스턴스화 . 클래스이고 인스턴스화를 해서 사용행햐ㅐ
- m.fit(data)
- vars(data)

#데이터 프레임 생성
- f=m.make_future_dataframe(periods=365)
- f

### 팬시 인덱싱으로 필요한 값
- yhat: 예측값
    - 이거 회사임

- forecast=m.predict(f)
- forecast[['ds','yhat','yhat_lower','yhat_upper']]
- m.plot(forecast)
- m.plot_components(forecast)

###  fbprophet 5줄이면 끝나요
-1. 데이터임포트
-2. 
-3. 
-4. 
-5. 그래프 가시화
   - plot,plot_components
   >> m.plot(forecast)
   >>m.plot_components(forecast)



#### plotly
- 아주예쁘고 좋은데
- 얘는 비싸
    - from fbprophet.plot import plotly


--------------------
50.  위치기반
- 사용하는건 folium
- !pip install folium
- import folium
-
- m=folium.Map(location=[37.5665, 126.9780],zoom_start=15)

51. zoom_start?
- 지도상에서 어떻게 이느냐

52.  location은?
-  위도 경도

53. 폴리엄은 시스템용?
- 시스템 아니에요.
- 보고서 용이에요. 

54. 시스템적으로 하고 싶다면?
- 구글 에이피아이 가져와야해

55. ties?
 - 조각조각 내서 가져오는데, 이 지도의 모양을 보여주는옵션 

56. 마크?
- 지도상에 박을 수 있고
- add to로 지도상에 박을 수 있음(html코드)

57. 마커
- 누르면 탁 티어나오는 거
- 써클 마커하고 서클하고 차이 있어요. 

57. 주석 빨리 해제
- 컨트롤 슬러쉬

58. 위도 경도 (교수님 많이 사용하시는거)
- m5 = folium.Map( location=[37.5665, 126.9780], tiles='Stamen Toner')
- m5.add_child(folium.LatLngPopup())

59.  ClickForMarker
- 특정한 값 찍을떄 딱 팝업되게 코드 찍을 수 있어
- 10개 데이터 받아서 찍거나
- 이거로 10개의 점 찍어서 그 가운데 타게팅 간으

60. 단계 구분도
- 히트맵을 지도로 표현
   - 성북구/강남구 이렇게 영역별로 구분지어 하는거
- 판다스 연결해서 할 수 있어

61. 제이슨 로드하면?
- 딕셔너리 형식ㅇ로 바뀜

62. 서울 지도
- import json
- seoul_topo_json = open('asset/seoul_municipalities_topo_simple.json', encoding='utf-8')
- seoul_topo_json = json.load(seoul_topo_json)

63. 서울지도에서 예쁘게 뽑기
- from pprint import pprint
- pprint(seoul_topo_json)

64. 토포제이슨에서 강남구 정보 뽑기
- seoul_topo_json['objects']['seoul_municipalities_geo']['geometries'][2]
>>
{'arcs': [[6, -6, 7, 8, 9, 10]],
 'type': 'Polygon',
 'properties': {'code': '11230',
  'name': '강남구',
  'name_eng': 'Gangnam-gu',
  'base_year': '2013'}}

65. arcs?
- 강남구 영역을 벡터로 표현
- 위치 뽑아냄

66.지오제이슨에서 강남 뽑아보자
- seoul_geo_json = open('asset/seoul_municipalities_geo_simple.json',encoding='utf-8')
- seoul_geo_json = json.load(seoul_geo_json)
- seoul_geo_json
- seoul_geo_json['features'][2]
>>
{'type': 'Feature',
 'properties': {'code': '11230',
  'name': '강남구',
  'name_eng': 'Gangnam-gu',
  'base_year': '2013'},
 'geometry': {'type': 'Polygon',
  'coordinates': [[[127.05867359288398, 37.52629974922568],
    [127.0690698130372, 37.522279423505026],
    [127.06860425556381, 37.51812758676938],
    [127.06926628842805, 37.51717796437217],
    [127.0719146000724, 37.50224013587669],
...

67.  토포제이슨과 지오제이슨 있음. 
- 둘다 벡터라서 확대해도 안 깨짐

68. 지오제이슨
- 위도 경도를 표현
- 굉장히 많이 표현
- 우리는 이거 사용

69. 지오제이슨의 단점?
- 파일 크기가 토포제이슨 4배만큼 커
>>
-m9 = folium.Map( location=[37.5665, 126.9780], tiles='Mapbox Bright') 

#지오제이슨
-folium.GeoJson(seoul_geo_json,name='geojson').add_to(m9)

#토포제이슨
-folium.TopoJson(seoul_topo_json,"objects.seoul_municipalities_geo",name='topojson').add_to(m9)
-folium.LayerControl().add_to(m9) #레이어를 이용하여 add_to를 집어넣어요
-m9


70. 판다스 삼총사 많이 사용해보세요. 
- 어플라이 맵- 맵

71.맵
- 브로드캐스팅으로 하면 상세 조건대로 해주기 힘드니 사용

71.5 맵 어디서 사용?
- 시리즈 및 데이터 프레임


72. 맵 내부 3개
- 함수/딕셔너리/시리즈
    - 맵은 시리즈에서 하나씩 쓸 수 있어 포처럼 돌앋 ㅏ니면서

73. 판다스 무슨 툴?
- 랭그링
- 가져와서 프리프로세싱하고 간단하게 그래프 그리는 툴
- 분석하면 고급 분석 있어야 하는데 판다스에 없음
- 그냥 어그리게이션 뿐이얌

74. tips에서 성별 바꾸는 방식 2가지
- tips.sex.map({'Male':1, 'Female':2})
- tips.sex.map(lambda x: 1 if x=='Male' else 2)

75. 어플라이
- 어플라이에 잘 보니까 액시스 개념 있어요. ★***★
- 맵은 시리즈에서 사용(시리즈+데이터 프레임)
-  시리즈했을때 맵과 같고, 데이터 프레임에서는 조금더 복잡하게 사용가능
- tips.sex.apply(lambda x: 1 if x=='Male' else 2)
- tips[['total_bill','tip']].apply(lambda x:sum(x))
- tips.apply(lambda x: x['total_bill']+x['tip'], axis=1)

76. 포 안쓰는 기법 몇 가지?
- 1.이터레이터 제네레이터

- 2. 컴프리핸션 (포 쓰지만 포 안쓰는 거로 보기로 했어요)
- 3. 재귀
- 4. 맵 필터 리듀스 하이

77. 어플라이 맵
- 전체를 싹다 하나씩 적용


78. anaconda prompt로 설치하세요
- conda install geopandas

79. geopandas 사용하는 이유
- 폴리엄에서 못했던 것들 여기서 사용할 수있음
- 단, 예쁘진 않음


--
1. # 판다스에서 분석하게 바꾸기 (손에 익어야)
-from sklearn.datasets import load_iris
-import pandas as pd
-iris=pd.DataFrame(data.data, columns=data.feature_names)
-iris_target=pd.DataFrame(data.target, columns=['target'])
-iris=pd.concat([iris,iris_target],axis=1)
-iris


2. 트레이닝 데이터용 쪼개기
- from sklearn.model_selection import train_test_split
-iris.sample(frac=0.75) #75퍼센트 랜덤하게

3 샘플링방식. 
- 팝을 이용해요
- 팝은 리스트/셋/딕셔너리서 자신도 뮤터블이고 결과도 바꾸는애
- iris.pop(train) 

4. ### pop 단점?
-  중복에 약해
- 그래서 train_test_split을 교수님이 좋아하셔

5. train_Test_split단점이랄까?
- 근데 얘는 사용하는게 좀 귀찮아
    - 리턴은 4개 분리시켜서 해야하거든. 언패킹이 네개가 되야해서 다 외워야해
    - 근데 이건 무조건 기곅학습할떄 시작하는 행동이라 외우고 있어야해
        - 엑스 트레인 엑스 테스트,와이 트레인, 와이 테스트
                - test_size-0.33 33프로를 그렇게 쓰겠다
                - random_state=42) 지정하면 일정하게 짤려요
                    - 역시 파이썬에는 42가 많이 나오네요
6.X_train, X_test, y_train, y_test=train_test_split(iris.iloc[:,:-1], iris.iloc[:,-1]) #모든행엣어 모든 열까지 끝까지 하나ㅏ

7. #### 데이터 리키즈 문제
- 150개 뿐인데 테스트 스플릿 하면,데이터 리키즈 문제 생김
- 작은거에서 뽑았더니 트레이닝엔 없고 테스트에 있는 경우가 있을 수 있어

8. 로지스틱 리그레이션
- from sklearn.linear_model import LogisticRegression
- lr=LogisticRegression()
- lr.fit(X_train,y_train)
- lr.score(X_test, y_test) //정확도 체크

9. 검증 여러번 했는데 그 값이 들쭉날쭉하다면?
- 데이터를 새로 수집

10. 스코어
- 어느정도인지 검증
- np.mean(knn.predict(X_test)==y_test.values) 이렇게 하는것보다 쉽지?

11. y_test 하니까 뭐로나와?
- 어레이

12. 판다스의 값을 넌파이로 바꾸는거?
- 점 벨류즈

13. 로지스틱 리그레션 왜 쓴다?
- 선형과 비선형성 알고리즘을 비교했을떄 성능이 비슷하다면, 이건 ㅓㄴ형적으로 판별할 수 있는 확률이 많다
- 그때부터는 굳이 학습시간 많이 걸리는 비선형에 집중할 필요 없음ㄴ
-  선형으로 치우치는지 보고 싶어서 사용

14. 선형과 비선형 
- 둘이 성능 비슷하면? 선형
- 비선형이 훨씬 좋으면? 비선형

15. 시작을 어떻게 하느냐 따라 시간을 줄일 수 있는거야
- 재밌는건, 빅데이터 하는 사람들은 통계학자 빼고 선형 알고리즘 체크 안 해요
- 현실이 복잡할 수록 비선형으로 갑니다.
- 그래서 선형 알고리즘 하는건 설명가능한 여부떄문이지 대부분은 안 써요.
- CS는 결과/성능을 높이는거 좋아해ㅑ

16. 비선형알고리즘의 문제?
- 비선형은 데이터가 없으면 성능이 안 좋아해

#### 특히 그 비선형알고리즘 중에서 뉴럴네트워크는 데이터가 많을 수록 성능이 좋은알고리즘

17. 뉴럴네트워크
- 데이터가 많으면 많을수록 성능 높아짐
- 어떤애들은 많다고 좋은건 아닌데, 여기는 많으면 좋아요

18. 순서
- 1.로지스틱회귀모델) 선형/비선형 여부 알고 싶어서 사용. 선형인 경우 시간을 줄일 수 있어 
- 2.디시전트리_의사결정나무)
    - 우리가 사용하면 한심해
    - 하나의 룰로 분류할 수 있어요
        - 뭐가 뭐보다 클때, 작을떄. 이걸 '룰'로 바꿀 수 있다.
            - 즉, 시스템화 할 수 있어요. 데이터 마이닝 및 시스템 만드는 사람들은 정말 좋아해
    - if then만 하면 되서 쉽고, 성능이 막 나쁜 것도 아니야
    - 지니/엔트로피사용하는거 (데헷 자 복습)
    - 문제점) 오버피팅. 
    - 해결방안) 가지치기
    - ■ Cart (Classification and regression trees_레오레이만_엄청난 논문 냈어요)
        - 논문명) two cultures
        - 이전에 통계하는 사람은 파라미트릭 모델을 좋아했고(우린 몰라도 돼) 근데 실제 세상은 달라
        - 넌 파라미트릭이 많아요. 그래서 그 모델의 중요성을 설파한 논문

19.  트리모델로 만든거
        - 앙상블- 랜덤포레스트 : 이거 한때 왕자였음. 
            - 그러나 지금은 랜덤포레스트부터. 
                - 앙상블 모델의 장점은 뭐에요? 성능, 오버피팅 잘 안난다(여러개 합치니)
                - 이게 시스템화 하긴 어렵지만 트리의 단점을 보완해준답니다.

20. 디시전트리와 랜덤 포레스트
- from sklearn.tree import DecisionTreeClassifier
- tree= DecisionTreefier() # 크리터리언 보면(지니랑 엔트로피 뿐), 분류도 바이너리뿐
- tree.fit(X_train,y_train)

21. 디시젼 패스
- 갑을 어떻게 결정하느냐. 분류를 어떻게 하는가
- 설명 가능하게 해요
- tree.decision_path() 

22. 결과에 sparse matirx 나오면 ?
- toarray()
- tree.decision_path(X.test).toarray()
- 스파스 매트릭스: 희소행렬

23. 성능?
- tree.score(X_test, y_test)

24. 디시전트리와 랜덤 포레스트 해볼게요
- 얘네 어디서 사용
    - 피쳐 임폴턴스, 각 분류하는 기준이 얼마나 중요한지 체크할때
- from sklearn.tree import DecisionTreeClassifier
- tree= DecisionTreeClassifier() 
-tree= DecisionTreeClassifier() # 크리터리언 보면(지니랑 엔트로피 뿐), 분류도 바이너리뿐
- tree.fit(X_train,y_train)



#### 디시젼 패스
- 갑을 어떻게 결정하느냐. 분류를 어떻게 하는가
- 설명 가능하게 해요

-tree.decision_path(X_test) #값을 어떻게 결정하느냐 
- tree.decision_path(X_test)

#### 결과에 sparse matirx 나오면 toarray()
- 스파스 매트릭스: 희소행렬

- tree.decision_path(X_test).toarray()

# 성능
- tree.score(X_test, y_test)
1

25. 랜덤포레스트 어디에?
- 앙상블에
-from sklearn.ensemble import RandomForestClassifierfier
- rf=RandomForestClassifier()
- rf.fit(X_train, y_train)
- rf.feature_importances_# iris의 4가지 column의 각각의 중요도를 나타내주는 feature_importances_
>> 
array([0.08, 0.04, 0.48, 0.4 ])

26. 뉴럴네트워크
- 있지말 케라스로 할게요 그게 더 쉬움
- from sklearn.neural_network import MLPClassifier
 - from sklearn.linear_model import Perceptron

## 나이브 베이지
- 확률 기반
- 3가지 알고리즘(NB가 붙어요)
    - fit, prediction, score 이거만 딱 해주면 됩니다.★
    - 가우시안
    - 
    - 
from sklearn.naive_bayes import GaussianNB

### 엑스지 부스팅과 뉴럴네트워크는 따로 보여줄거에요
- 여기서 수집한 데이터가 성능 30프로 이상 나올리가 없을거야
- 전처리 엄청해야해
- 사이킷에서는 뭘지원하느냐


27. #### DummyClassifier
- 사람이 하는 행동처럼 분류
- from sklearn.dummy import DummyClassifier
- dummy=DummyClassifier()

#### 사이킷은 노가다 줄이게 도와줘
- 다시 KNN으로 돌아가보자. 
    - K가 어떨때 좋을까?

from sklearn.neighbors import KNeighborsClassifier
for i in range(1, 20):
    knn=KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    print(i,knn.score(X_train, y_train))






### 그리드 서치 시브이
- 아깐 정답이라고 가정하고, 테스트 셋으로 검증 했어
- 근데 데이터 적을때 '데이터 리퀴즈'문제가 생겨
- 따라서 데이터 적을때 무슨 방법 사용?
    - 크로스 벨리데이션
    
#### 크로스 벨리데이션
- mglearn.plot_cross_validation.plot_cross_validation()
- 데이터가 작을떄 사용하는 선응 예측 방법
- 단점) 학습하는데 시간이 많이 거렬
- 모든 데이터 트레이닝할떄 사용하니까 데이터 리퀴즈도 없어
- 결정저 단점) 최종 모델이 아니에요.
- 용법1) 대충 성능 얼마인지 알기위해
   - 다섯개 쪼개서 모델만들고 평균만들기.
   - 최종이 아니고 모델에 얼마나 정확성이 있는가 체크용
- 그렇기에 '오버피팅'인지 체크하기 위해서 이걸 사용.
   - 내 모델이 이거보다 성능이 좋다면 오버피팅

# - 데이터가 많으면 트레이닝 데이터 스플릿, 작으면 크로스 벨리데이션


28.  크로스벨리데이션 사용법
- 우린 모델 셀렉션 관점이랫죠
- from sklearn.model_selection import GridSearchCV


29. 모델 셀렉션
- from sklearn.model_selection import cross_val_score #스코어는 성능 알려주는거였ㅇ

30. #### estimator: 알고리즘 인스턴스화 하는 애
- 이제 쪼갠거말고 전체 데이터 다 넣어봐요
- 크로스에서는 5조각내는거 파이브..이고 관례상 템폴드 10개 써요
- cross_val_score(KNeighborsClassifier(), iris.iloc[:,:-1], iris.iloc[:,-1], cv=10, n_jobs=-2) #10 개 쪼개서 모델 만들어 #n잡슨는 내 코어? 

31. #### estimator: 알고리즘 인스턴스화 하는 애
- 이제 쪼갠거말고 전체 데이터 다 넣어봐요
- 크로스에서는 5조각내는거 파이브..이고 관례상 템폴드 10개 써요

32. 이 모델의 평균
- import numpy as np
- # 이 모델의 평균
np.mean(cross_val_score(KNeighborsClassifier(), iris.iloc[:,:-1], iris.iloc[:,-1], cv=10, n_jobs=-2)) #10 개 쪼개서 모델 만들어 #n잡슨는 내 코어? 

33. ## 에스브이엠
- svm 커널 트릭해서 차원을 증가시켜버려. 그럼 자를 수가 있겠죠. 그렇게 해서 판별 시키기도 함. 
- 수학적으로 미치는데 이뻐
- 수학적 관점이라서 깔끔하고 좋은데 속도가 좀 느려
    - 학습 및 예측 둘다 느려서 실무에 쓰기는 어려워요
- from sklearn.svm import SVC
- %time np.mean(cross_val_score(SVC(), iris.iloc[:,:-1], iris.iloc[:,-1], cv=10, n_jobs=-2)) 

#### 크로스벨리데이션 이용해서
- 데이터가 많으면 트레이닝 데이터 스플릿, 작으면 크로스 벨리데이션

34.#### 그리드 어떻게 쓰냐
- from sklearn.model_selection import GridSearchCV

35. - 이거 인스턴스 하면 에러나. 첫번째에 estimatie-알고리즘 인스턴스 넣어야해
- 파람 그리드: 점점.해서 그룹 만들어줘
        - knn은 하이퍼 파라미터(엔네이버즈가 파라미터_
        - 딕셔너리 형태로 작성
- param_grid={'n_neighbors':[1,2,3,4,5,6,7,8,9]}
- grid=GridSearchCV(KNeighborsClassifier(),param_grid) #이거 인스턴스 하면 에러나. 첫번째에 estimatie-알고리즘 인스턴스 넣어야해

36. ### 크로스 벨리데이션 하니까 전체로 핏해볼게
- grid.fit(iris.iloc[:,:-1], iris.iloc[:,-1])
- #### best index, best param 등등 나와요
- grid.best_params_
>> {'n_neighbors': 5}
- grid.best_score_
>>0.9866666666666667

---------------------
37. #### 파람 그리드 여러개 쓸 수 있어
- 하이퍼 파라미터 찾기 쉽다 @!!!@@@@@@@@@@@@@@@
- param_grid={'n_neighbors':[1,2,3,4,5,6,7,8,9]}



-------------
### 파이프라인으로 자동화하면 좋아요

## 그리드 정말 다 좋은데, 전처리가 그렇게 좋지 않을 거야
- 우리 데이터 안 좋을 이유: 웹에서 수집하면 문자라서 숫자로 전처리 해야할거에요. 
    - 도메인 별로 데이터를 숫자화 하느것 (벡터화라이제이션_교수님 인코딩이라 부르실듯)
    - 인코딩
        - 라벨 인코딩
        - 워.. 인코딩?

## 라벨 인코딩) 문자를 숫자로 바꾸는거
- import seaborn as sns
- iris=sns.load_dataset('iris')
- iris
- iris.species.map({'setosa':0, 'virginicia':1, 'versicolor':2})

## 문자열을 0,1,2 처럼 숫자값으로 바꿔주는 이러한 방식을 label encoding 이라 한다.
# 숫자값을 바꿔준다는 것은 즉 각각이 숫자 크기 비교가 가능해지므로 X 에는 이를 적용하면 안된다. y(target)에만 적용하자
# label encoding은 categorical인 target에 사용하기에 적합하다.

# map에서는 dictionary 또는 함수를 사용할 수 있다.

## 라벨 인코딩 조심
- 학습될 데이터에 들어가면 문제 발생 가능
- 라벨 인코딩은 숫자가 0,1,2로 넣으면 그 애가 우리는 숫자가 아닌 종류의 데이터인데.
- 크기로 바뀌어서 큰수가 영향력을 미치는 경우 발생가능
- 라벨 인코딩은 엑스에 있는 데이터 라벨링하는건 바보짓이야
- 대신 타겟에 가능. 단, 엑스에 쓰면 아주아주 조심해야해



## 맵은 내부 딕셔너리/함수 이용 가능
- 그래서 아마 맵으로도 전처리 가능할거에요

# r그리드 서치 시브이) 포 안해도 다 찾아서 좋아
- # cross_validation : '데이터가 작을 때 사용'
- from sklearn.model_selection import GridSearchCV # CV -> cross validation 방식을 사용한다는 의미
- param_grid = {'n_neighbors': [1,2,3,4,5,6,7,8,9]}
- grid = GridSearchCV(KNeighborsClassifier(), param_grid)
// param_grid : hyper parameter를 gridsearch 내부에서 어떤 값을 사용할 건지 dictionary 형태로 전달하여 정해줄 수 있다.
- grid.fit(iris.iloc[:,:-1], iris.iloc[:,-1])
- grid.best_params_
>>n_neighbors': 5}
- grid.best_score_
>>0.9866666666666667
- KNeighborsClassifier
>>klearn.neighbors.classification.KNeighborsClassifier

--
## 라벨 인코딩 다른 방법
- 사이킷에서
-from sklearn.preprocessing import LabelEncoder
- le=LabelEncoder()
- le.fit(iris.species) #이거 조금 혼동될 수 있어
- le.transform(iris.species)
###### 근데 인자가 다 같으니까 단축 포현으로
- le.fit_transform(iris.species)
>>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])

----
- #### 이제 이거 다시 백해서 판다스에 넣어야해
- 교수님은 라벨 인코딩을 판다스로 하셔
- 근덴 판다스보다 사이킷이 더 쉬움
    - 임포트-인스턴스-핏-프레딕트/트렌스폼. 이거 4개만 하면 돼

## 라벨 인코등
- 문자를 수자로 변경
- 1. 라벨인코딩
- 한번에 처리하는거 임배딩 방식
- 첫번쨰 옵션
    - 우리는 한 컬럼당 문자가 하나 들어갈때 카테고리컬을 할떄 라벨링을 합니다
    - 주의)학습용노. 
- 2.원한 인코딩(더미인코딩)

import pandas as pd
pd.get_dummies(iris.species)
----
## 원핫 인코딩
-투디 뭐시기 넣어야해
- 스펙시스는 일차원 시리즈

-from sklearn.preprocessing import OneHotEncoder
- ohe=OneHotEncoder()
- ohe.fit(iris.species)
-----
### 차원 2차원 만드는거- 팬시 인댁싱
- ohe.fit(iris[['species']])
>> OneHotEncoder(categorical_features=None, categories=None,
       dtype=<class 'numpy.float64'>, handle_unknown='error',
       n_values=None, sparse=True)
### 바꾸는거 트랜스폼
- 
ohe.fit_transform([iris['species']])
>><1x150 sparse matrix of type '<class 'numpy.float64'>'
   with 150 stored elements in Compressed Sparse Row format>

- ### 사이킷은 변환을 다시 원래로 돌리게도 해줌
- ohe.inverse_transform([[1,0.,0.]])
- le.inverse_transform([2])

-----------------
## 딥러닝 기반은 노말라이제이션해야함
- 사이킷에서는 이게 있어요.

from sklearn.preprocessing import MinMaxScaler, RobustScaler

### 아웃라이어있을떄 뭐 사용해요?
- RobustSclaer
    - 아웃라이어가 있을 경우에

#### 민맥스 스케일러가 뭔지 보여줄게요
- 예측모델은 스켈링하는것
- 오히려 좋아지는경우가 많을거에요
- 스탠다드스케일러는 뭐? ㅏㄴ글로 표준화. 표준화는 뭐하는 거였다. 
    - 표준화. (x-m)/표준편차
        - 이렇게 되면 모델이 더 예쁘게 될 수도 있어.
        - 그래서 아예 이거부터 다 해놓고 시작하는 사람도 있어

#### 예측에 특성은 안 바뀌죠
- 타겟은 뭘 안한다? 스케일링 안하고 학습할 거만 스케일링 ㅎ아면 문제 없어
- 리그레이션 할때도 스케일링 해야해요
- 가장 간단한전처리
- 그래서 같은 모델을 스케일링과 안 했을때 비교를 해봐야해ㅛ

### 도메인 지식 필요
- 아ㅣ무때나 스케일링하면 문제 생겨
- 아이리쉬는 자연현상이야. 그래서이 자연현상에 대해서 잘 모르고 바꾸면 안돼. 
- 즉 너희가 데이터 수집하고 나서 각 도메인에 대한 어트리뷰트별 수집을 다 해야해
    - 보고서 낼떄 그거에 대한 거 내야해
    - 근데 일반적인 상식은 할 필요 없지


#### 사이킷은 지원하는 것만 가능, 판다스는 내 맘대로 가능
- 내가 도메인 지식은 언ㅄ고 세팔 랭쓰의 크기 높이고 싶어
- 맵과 어플라이로 값 조금씩 변경 가능★
- iris.sepal_length.map(lambda x:x+1)

#### 예측할때도 같이 변화 시켜줘야해 ★

# 그래프 : 각각 보는 이유가 다 있어요
- 페어플랏
- 박스플랏
- 히트맵

# 그래프 : 각각 보는 이유가 다 있어요
- 페어플랏
- 박스플랏
- 히트맵

#### 최적 이전에 학습이 잘 되었는지 알 수 있따?
- 가정 자체
- 알 수 있어.
- 어떻게 쓰냐면 from sklearn.model_selection import learning_curve 
    - 학습에 따라 곡선을 통해 상황 파악 가능
   - from sklearn.model_selection import learning_curve
   - !pip install sklearn-evaluation
   ### 설치할떄는 -, 사용할때는 _
   - import sklearn_evaluation


#### 트레이닝 스코어는 갈 수록 좋아져
- 그러나 이게 좋다는건 거꾸로 학습이 어느정도 많이 되었다는 것
- 러닝커브는 즉 데이터가 어느정도 필요할지 여부 체크하려고 쓰는거고, 밸리데이션은 트레이닝과 테스트 에러가 어떻게 파라미터에 따라 변하는지 보는 것. 즉 그리드 씨브이와 같이 보는 것

## Confusion_matrix
- 실제 n개 중에 몇 개 맞췄는데 비쥬얼라이제이션하는거
#### 사이킷런 이벨류에이션을 쓰면
- 얘는 히트맵을 사용해서 그림



