{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jmkDay23_CNN_self",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minkyoJang/AI-BigDataStudy/blob/master/jmkDay23_CNN_self.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UnA6oJ0ERqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision #이미지 관련 처리, Pretrained Model 관련된 Package\n",
        "import torchvision.datasets as vision_dsets\n",
        "import torchvision.transforms as transforms #이미지 처리(vision)관련된 transformation\n",
        "import torch.optim as optim #pytorch에서 정의한 수 많은 optimization function들 불러옵니다.\n",
        "from torch.utils import data\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt #시각화를 위한 패키지"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5UGbPS7FDBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "lr=0.001\n",
        "batch_size= 128\n",
        "best_acc=0 #best test accuracy\n",
        "start_epoch=0 # start from epoch - or last checkpoint epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saJRYRj1FbuY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c60904ff-83c0-4919-8b37-ba38d517d8aa"
      },
      "source": [
        "# torch.view 연습\n",
        "\n",
        "##배치: 기존이 3차원이라고 하면, 배치를 사용하여 4차원으로 처리하게 함.\n",
        "sample= torch.randn(2000,3,64,64) #2000배치, 3채널, 64하이트, 64윗드 //텐서플로는 순서가 다름\n",
        "#코딩 시 B, C, H, W로 변수화 하여 사용하면 편리\n",
        "# sample=> (2000,64,64,3)\n",
        "\n",
        "B,C,H,W=sample.size()\n",
        "\n",
        "#sample => (2000,64,64,3)\n",
        "sample_1= sample.view(B,-1) #이 샘플을 (2000,)\n",
        "print(sample_1.size())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2000, 12288])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA5oSGIcJra9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e0a4a356-c88e-4716-a876-763032871833"
      },
      "source": [
        "sample_2= sample.view(B*C, H*W)\n",
        "print(sample_2.size())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6000, 4096])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iFbo0AVL1fk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "75357c13-7efd-41e2-cb2a-40c013eae4e5"
      },
      "source": [
        "# torch.view 다른 예제\n",
        "a= torch.FloatTensor(5,20,100) #1차원으로써 채널에 하이트만 있음\n",
        "a1= a.view(2,10,-1)\n",
        "print(a1.size())\n",
        "a2= a.view(-1,10,200)\n",
        "print(a2.size())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 10, 500])\n",
            "torch.Size([5, 10, 200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kMTanV9MRR9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a3611b42-c2cd-4c3b-97b6-22d2fedb84ed"
      },
      "source": [
        "# dimension간에 이동을 하고 싶다면? ex) B, C, H, W => B, C, W, H > permute 사용\n",
        "sample= torch.randn(2000,3,32,64)\n",
        "sample_1=sample.permute(0,1,3,2)\n",
        "print(sample.size())\n",
        "print(sample_1.size())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2000, 3, 32, 64])\n",
            "torch.Size([2000, 3, 64, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UddulkUsMvIX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsDPspZbMug3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed1d8fd6-ff76-4699-85ca-fc77390a6ca0"
      },
      "source": [
        "# sample_1 변수의 디맨션을 원래대로 돌리려면?\n",
        "sample_2= sample_1.permute(0,1,3,2)\n",
        "print(torch.equal(sample,sample_2))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6udbYiuNJNC",
        "colab_type": "text"
      },
      "source": [
        "### Convolution layer  가지고 놀기 \n",
        "- nn.Conv2d(in, out, filter_size, stride, padding)\n",
        "- 각 이미지를 만들때 몇 개의 레이어를 쌓아서 만드는 것\n",
        "- 커널=필터\n",
        "- batch: 고양이 사진을 판별한다고 한다면, 사용하는 사진의 갯수(우진님)\n",
        "    - 배치는 그대로\n",
        "- padding 사용이유: 만일 스트라이드를 하면서 필터를 하다보면 점점 이미지가 사이즈가 작아지게 됨. 그러면 다음번 이미지 컨부하면서 원래의 데이터보다 작아지게 되니 이를 방지하게 위해 패딩으로 원 크기를 유지시키는 것 (석진 조교님)\n",
        "- width, height는 그 공식 (input+2p-k/strids)+1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0VEmVcpM9Jz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4abc99df-408d-48e9-e3ce-5fc6cf835e1d"
      },
      "source": [
        "# torch.nn. conv 연습\n",
        "# 클래스 구성: nn.Conv2d(in, out, filter_size, stride, padding)\n",
        "# 아웃풋 데이터가 다음과 같이 되도록 해봅시다(16,128,32,32)\n",
        "\n",
        "input1 = torch.zeros(16,3,64,64) \n",
        "conv1 = nn.Conv2d(3,512, 3,1,1 ) #16,3,64,64> 16, 512, 64,64\n",
        "# 3,1,1 쓰면 아웃풋이 그대로 나온대. 커널로 5를 쓰면 패딩을 2로주고 스페셜 그대로 유지\n",
        "conv2= nn.Conv2d(512,128, 3,2,1) #16, 512, 64, 64 > 16,128,32,32\n",
        "print(conv2)\n",
        "\n",
        "out= conv1(input1)\n",
        "output=conv2(out)\n",
        "print(output.size())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Conv2d(512, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "torch.Size([16, 128, 32, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlhNW7EdVVuf",
        "colab_type": "text"
      },
      "source": [
        "# 데이터 로딩\n",
        "- Data Augmentation\n",
        "    - 이미지 하나를 트레이닝 시키려면 여러개의 소스가 필요한데 실질적으로 그렇게 많은 소스를 가져올 수 없음\n",
        "    - 따라서 하나의 이미지를 회전하는 식으로 데이터 소스를 얻습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRknIHi7RRl-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e3380028-19a3-40e6-84f7-c2e55ff05f4a"
      },
      "source": [
        "# 데이터 로딩: \n",
        "print('==> Preparing data..')\n",
        "\n",
        "#데이터 전처리를 위한 코드\n",
        "transform_train = transforms.Compose([\n",
        "    #transfomss 안에\n",
        "    ##randomCrop 랜덤하게 정해진 크리고 자름. 확률적으로 고양이 귀, 꼬리만 보고도 판단할 수 있게+ 밝기 조절도\n",
        "    ##RandomHorizontalFlip> 좌우를 우좌로 자름\n",
        "    \n",
        "    transforms.RandomCrop(32, padding=4), # 4만큼의 padding을 부여한 후 32*32 random cropping\n",
        "    transforms.RandomHorizontalFlip(), #0.5의 확률로 이미지 좌우 반전하여 넣어줌\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0,4914,0.4822,0.4465), (0.2023, 0.1994, 0.2010))\n",
        "     # cifar10(0~1)의 r,g,b 채널 별 평균 및 분산으로 normalization\n",
        "        ## => N(0,1)의 정규분포를 따르도록 만들어준후 input으로 넣어줍니다.    \n",
        "])\n",
        "\n",
        "# 데이터 전처리를 위한 코드\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    # cifar10(0~1)의 r,g,b 채널 별 평균 및 분산으로 normalization \n",
        "    ## =>N(0,1)의 정규분포를 따르도록 만들어준후 input으로 넣어줍니다.    \n",
        "    \n",
        "])\n",
        "\n",
        "\n",
        "# 데이터 로딩\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train= True, download= True, transform=transform_train)\n",
        "trainloader= torch.utils.data.DataLoader(trainset, batch_size= batch_size, shuffle= True, num_workers=1)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train= False, download= True, transform= transform_test)\n",
        "testloader= torch.utils.data.DataLoader(testset, batch_size=100, shuffle= False, num_workers=1)\n",
        "### @@질문- num_workers의 역할이 뭔가요?\n",
        "#### 재은 답: 데이터를 로딩하면서 사용되는 서브 프로세스의 갯수를 지정해줍니다. 지정 안 하면 0으로 메인 프로세스에서 진행을 합니다. \n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpiFQF9aXROl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "outputId": "12145632-c433-418c-b4f8-55c19e6c13df"
      },
      "source": [
        "# 데이터 시각화\n",
        "def showImaes(image, row):\n",
        "    for _ in range(row):\n",
        "        \n",
        "        idx = np.random.choice(batch_size, 6) #0~127의 정수 중 6개를 임의로 선택\n",
        "        imgaes = image.numpy()[idx].transpose(0,2,3,1).clip(0,1)  #선택된 index에 해당하는 이미지 가져옴\n",
        "                 #선택된 index에 해당되는 이미지를 가져움\n",
        "        plt.figure(figsize= (15,90)) #세로 길이: 15, 가로길이15*6의 화면 생성\n",
        "        \n",
        "        for i in range(161, 167):\n",
        "            plt.subplot(i)\n",
        "            plt.imshow(images[i - 161])\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "            \n",
        "        plt.show()\n",
        "for i , (image, labels) in enumerate(trainloader):\n",
        "    showImages(image.squeeze(), 3)\n",
        "    break"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-9a4837da27db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mshowImages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KeyError:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 99, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 99, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/cifar.py\", line 127, in __getitem__\n    img = self.transform(img)\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n    img = t(img)\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 164, in __call__\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\", line 208, in normalize\n    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\nRuntimeError: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBlxz_UYhy6U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "eb1999de-f657-460f-98d9-e5aa7a3b2fbe"
      },
      "source": [
        "fprint('[info] # of train batch :' , len(trainloader)) #트레인 로더 안에 배치 사이즈 있음.\n",
        "## 데이터 사이즈를 배치 사이즈로 자르면 미니 배치의 수가 나옴. 따라서 이런 미내 배치가 몇개인지 알려줌\n",
        "print('[info] # of test batch :' , len(testloader))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[info] # of train batch : 391\n",
            "[info] # of test batch : 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrada5mni6df",
        "colab_type": "text"
      },
      "source": [
        "## 딥러닝 줄기 2개\n",
        " - (1) 트레인 코드: 테스트는 일단 제외하고\n",
        "    - 인풋 받아서 포워딩을 함\n",
        "    - 주요내용: 정답과 스코어링 및 학습 \n",
        " - (2) 모델 코드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2zwzuCgqiWz",
        "colab_type": "text"
      },
      "source": [
        "### train_network(net, optimizer, tainloader, epoches)\n",
        "- net\n",
        "- optimizer, : 최적화 하기 위함\n",
        "- trainloader : 데이터 트레이닝 하기 위해서 사용하는 X값들 그리고 Y 값들(X는 넣은것, Y는 이렇지 않을까 예측한 것)\n",
        "- epochs : 반복 횟수\n",
        "\n",
        "#### 옵티마이저에서 제로 그레드를 하는 이유:\n",
        "    - 옵티마이저는 최적화를 하는것\n",
        "    - 전진방향으로 돌고나서 백프로파게이션을 할때, 해당 값이 누적이 되어있으면 가중치가 너무나도 늘기 때문임.\n",
        "    - 따라서 제로그레드를 통해 값들을 초기화 시켜야함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8IgTJUMi4Ye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29e08162-aeaf-4e81-f913-e0b4efbd9700"
      },
      "source": [
        "def train_network(net,optimizer, trainloader, epochs= 5):\n",
        "    for epoch in range(epochs): #loop over the dataset multiple times\n",
        "        \n",
        "        running_loss= 0.0 #runnin loss를 저장하기 위한 변수입니다.\n",
        "        for i, data in enumerate(trainloader): #한 Eopch만큼 돕니다. \n",
        "                                    ## 매 iteration마다 정해진 Batch sie만큼 데이터를 뱉습니다.\n",
        "            # get the inputs\n",
        "            inputs, labels = data #DataLoader iterator의 반환값은 input_data와 labesl의 튜플형식\n",
        "            # 데이터 로더에는 x 에 대한 (train/test) 그리고 y 에대 한 (trani/test)가 있습니다.\n",
        "            ## 여기서는 학습을 시킬 데이터 X를 inputs로 설정했고, 테스트용으로 쓸 Y를 labels로 표기\n",
        "            inputs = inputs.to(device) #Pytorch에서 nn.Module에 넣어 Backprop을 계산하기 위해 \n",
        "                                        ## gpu 연동을 이와 같이 해줘야합니다.\n",
        "            labels = labels.to(device) #gpu에 올려줘요\n",
        "            \n",
        "            #zero the parameter gradients\n",
        "            optimizer.zero_grad() #현재 기존의 backprop을 계산하기 위해 저장했던 activation buffer 비움\n",
        "            \n",
        "            \n",
        "            #from+ backward+ optimize\n",
        "            outputs = net(inputs) #인풋을 넣고 네트워크로부터 아웃풋 얻습니다. \n",
        "            ## **net이 모델 코드. 넷에서 인풋 받아서 아웃풋 뽑죠. 이 아웃풋과 정해진 로스를 구해요. \n",
        "            ###이 로스를 백프랍하고 옵티마이저 스탭\n",
        "            \n",
        "            loss = criterion(outputs, labels) #loss function 에 주어진 output을 얻어냅니다.\n",
        "            # outputs는 모델에 넣어서 나온 값이고, labels는 trainloader 에서 y값들에 대한것(예측값)\n",
        "            ##> 즉 이 2개에 대해서 loss가 얼마나 있는지를 criterion으로 체크게 하고\n",
        "            loss.backward() #백워드를 진행하여 loss를 구함. \n",
        "            optimizer.step() #최적화 마쳤다. 이런 느낌\n",
        "            \n",
        "        # @@@@ 러닝로스 아래 코드들이 무슨 의미이지\n",
        "            running_loss += loss.item() \n",
        "            if (i+1)%100 == 0:\n",
        "                print('[%d %5d] loss : %3f' % (epoch +1, i+1, running_loss/100))\n",
        "                running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaEmhiCnkr7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval() \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    for data, target in test_loader:\n",
        "        data= data.to(device)\n",
        "        target= target.to(device) #기존의 train function의 데이터 처리부분과 같습니다.\n",
        "        output= model(data)\n",
        "        pred= output.max(1, keepdim= True)[1] # get the index of the max\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item() #정답 데이터의 "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}