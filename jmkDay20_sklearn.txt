1. # 판다스에서 분석하게 바꾸기 (손에 익어야)
-from sklearn.datasets import load_iris
-import pandas as pd
-iris=pd.DataFrame(data.data, columns=data.feature_names)
-iris_target=pd.DataFrame(data.target, columns=['target'])
-iris=pd.concat([iris,iris_target],axis=1)
-iris


2. 트레이닝 데이터용 쪼개기
- from sklearn.model_selection import train_test_split
-iris.sample(frac=0.75) #75퍼센트 랜덤하게

3 샘플링방식. 
- 팝을 이용해요
- 팝은 리스트/셋/딕셔너리서 자신도 뮤터블이고 결과도 바꾸는애
- iris.pop(train) 

4. ### pop 단점?
-  중복에 약해
- 그래서 train_test_split을 교수님이 좋아하셔

5. train_Test_split단점이랄까?
- 근데 얘는 사용하는게 좀 귀찮아
    - 리턴은 4개 분리시켜서 해야하거든. 언패킹이 네개가 되야해서 다 외워야해
    - 근데 이건 무조건 기곅학습할떄 시작하는 행동이라 외우고 있어야해
        - 엑스 트레인 엑스 테스트,와이 트레인, 와이 테스트
                - test_size-0.33 33프로를 그렇게 쓰겠다
                - random_state=42) 지정하면 일정하게 짤려요
                    - 역시 파이썬에는 42가 많이 나오네요
6.X_train, X_test, y_train, y_test=train_test_split(iris.iloc[:,:-1], iris.iloc[:,-1]) #모든행엣어 모든 열까지 끝까지 하나ㅏ

7. #### 데이터 리키즈 문제
- 150개 뿐인데 테스트 스플릿 하면,데이터 리키즈 문제 생김
- 작은거에서 뽑았더니 트레이닝엔 없고 테스트에 있는 경우가 있을 수 있어

8. 로지스틱 리그레이션
- from sklearn.linear_model import LogisticRegression
- lr=LogisticRegression()
- lr.fit(X_train,y_train)
- lr.score(X_test, y_test) //정확도 체크

9. 검증 여러번 했는데 그 값이 들쭉날쭉하다면?
- 데이터를 새로 수집

10. 스코어
- 어느정도인지 검증
- np.mean(knn.predict(X_test)==y_test.values) 이렇게 하는것보다 쉽지?

11. y_test 하니까 뭐로나와?
- 어레이

12. 판다스의 값을 넌파이로 바꾸는거?
- 점 벨류즈

13. 로지스틱 리그레션 왜 쓴다?
- 선형과 비선형성 알고리즘을 비교했을떄 성능이 비슷하다면, 이건 ㅓㄴ형적으로 판별할 수 있는 확률이 많다
- 그때부터는 굳이 학습시간 많이 걸리는 비선형에 집중할 필요 없음ㄴ
-  선형으로 치우치는지 보고 싶어서 사용

14. 선형과 비선형 
- 둘이 성능 비슷하면? 선형
- 비선형이 훨씬 좋으면? 비선형

15. 시작을 어떻게 하느냐 따라 시간을 줄일 수 있는거야
- 재밌는건, 빅데이터 하는 사람들은 통계학자 빼고 선형 알고리즘 체크 안 해요
- 현실이 복잡할 수록 비선형으로 갑니다.
- 그래서 선형 알고리즘 하는건 설명가능한 여부떄문이지 대부분은 안 써요.
- CS는 결과/성능을 높이는거 좋아해ㅑ

16. 비선형알고리즘의 문제?
- 비선형은 데이터가 없으면 성능이 안 좋아해

#### 특히 그 비선형알고리즘 중에서 뉴럴네트워크는 데이터가 많을 수록 성능이 좋은알고리즘

17. 뉴럴네트워크
- 데이터가 많으면 많을수록 성능 높아짐
- 어떤애들은 많다고 좋은건 아닌데, 여기는 많으면 좋아요

18. 순서
- 1.로지스틱회귀모델) 선형/비선형 여부 알고 싶어서 사용. 선형인 경우 시간을 줄일 수 있어 
- 2.디시전트리_의사결정나무)
    - 우리가 사용하면 한심해
    - 하나의 룰로 분류할 수 있어요
        - 뭐가 뭐보다 클때, 작을떄. 이걸 '룰'로 바꿀 수 있다.
            - 즉, 시스템화 할 수 있어요. 데이터 마이닝 및 시스템 만드는 사람들은 정말 좋아해
    - if then만 하면 되서 쉽고, 성능이 막 나쁜 것도 아니야
    - 지니/엔트로피사용하는거 (데헷 자 복습)
    - 문제점) 오버피팅. 
    - 해결방안) 가지치기
    - ■ Cart (Classification and regression trees_레오레이만_엄청난 논문 냈어요)
        - 논문명) two cultures
        - 이전에 통계하는 사람은 파라미트릭 모델을 좋아했고(우린 몰라도 돼) 근데 실제 세상은 달라
        - 넌 파라미트릭이 많아요. 그래서 그 모델의 중요성을 설파한 논문

19.  트리모델로 만든거
        - 앙상블- 랜덤포레스트 : 이거 한때 왕자였음. 
            - 그러나 지금은 랜덤포레스트부터. 
                - 앙상블 모델의 장점은 뭐에요? 성능, 오버피팅 잘 안난다(여러개 합치니)
                - 이게 시스템화 하긴 어렵지만 트리의 단점을 보완해준답니다.

20. 디시전트리와 랜덤 포레스트
- from sklearn.tree import DecisionTreeClassifier
- tree= DecisionTreefier() # 크리터리언 보면(지니랑 엔트로피 뿐), 분류도 바이너리뿐
- tree.fit(X_train,y_train)

21. 디시젼 패스
- 갑을 어떻게 결정하느냐. 분류를 어떻게 하는가
- 설명 가능하게 해요
- tree.decision_path() 

22. 결과에 sparse matirx 나오면 ?
- toarray()
- tree.decision_path(X.test).toarray()
- 스파스 매트릭스: 희소행렬

23. 성능?
- tree.score(X_test, y_test)

24. 디시전트리와 랜덤 포레스트 해볼게요
- 얘네 어디서 사용
    - 피쳐 임폴턴스, 각 분류하는 기준이 얼마나 중요한지 체크할때
- from sklearn.tree import DecisionTreeClassifier
- tree= DecisionTreeClassifier() 
-tree= DecisionTreeClassifier() # 크리터리언 보면(지니랑 엔트로피 뿐), 분류도 바이너리뿐
- tree.fit(X_train,y_train)



#### 디시젼 패스
- 갑을 어떻게 결정하느냐. 분류를 어떻게 하는가
- 설명 가능하게 해요

-tree.decision_path(X_test) #값을 어떻게 결정하느냐 
- tree.decision_path(X_test)

#### 결과에 sparse matirx 나오면 toarray()
- 스파스 매트릭스: 희소행렬

- tree.decision_path(X_test).toarray()

# 성능
- tree.score(X_test, y_test)
1

25. 랜덤포레스트 어디에?
- 앙상블에
-from sklearn.ensemble import RandomForestClassifierfier
- rf=RandomForestClassifier()
- rf.fit(X_train, y_train)
- rf.feature_importances_# iris의 4가지 column의 각각의 중요도를 나타내주는 feature_importances_
>> 
array([0.08, 0.04, 0.48, 0.4 ])

26. 뉴럴네트워크
- 있지말 케라스로 할게요 그게 더 쉬움
- from sklearn.neural_network import MLPClassifier
 - from sklearn.linear_model import Perceptron

## 나이브 베이지
- 확률 기반
- 3가지 알고리즘(NB가 붙어요)
    - fit, prediction, score 이거만 딱 해주면 됩니다.★
    - 가우시안
    - 
    - 
from sklearn.naive_bayes import GaussianNB

### 엑스지 부스팅과 뉴럴네트워크는 따로 보여줄거에요
- 여기서 수집한 데이터가 성능 30프로 이상 나올리가 없을거야
- 전처리 엄청해야해
- 사이킷에서는 뭘지원하느냐


27. #### DummyClassifier
- 사람이 하는 행동처럼 분류
- from sklearn.dummy import DummyClassifier
- dummy=DummyClassifier()

#### 사이킷은 노가다 줄이게 도와줘
- 다시 KNN으로 돌아가보자. 
    - K가 어떨때 좋을까?

from sklearn.neighbors import KNeighborsClassifier
for i in range(1, 20):
    knn=KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    print(i,knn.score(X_train, y_train))






### 그리드 서치 시브이
- 아깐 정답이라고 가정하고, 테스트 셋으로 검증 했어
- 근데 데이터 적을때 '데이터 리퀴즈'문제가 생겨
- 따라서 데이터 적을때 무슨 방법 사용?
    - 크로스 벨리데이션
    
#### 크로스 벨리데이션
- mglearn.plot_cross_validation.plot_cross_validation()
- 데이터가 작을떄 사용하는 선응 예측 방법
- 단점) 학습하는데 시간이 많이 거렬
- 모든 데이터 트레이닝할떄 사용하니까 데이터 리퀴즈도 없어
- 결정저 단점) 최종 모델이 아니에요.
- 용법1) 대충 성능 얼마인지 알기위해
	- 다섯개 쪼개서 모델만들고 평균만들기.
	- 최종이 아니고 모델에 얼마나 정확성이 있는가 체크용
- 그렇기에 '오버피팅'인지 체크하기 위해서 이걸 사용.
	- 내 모델이 이거보다 성능이 좋다면 오버피팅

# - 데이터가 많으면 트레이닝 데이터 스플릿, 작으면 크로스 벨리데이션


28.  크로스벨리데이션 사용법
- 우린 모델 셀렉션 관점이랫죠
- from sklearn.model_selection import GridSearchCV


29. 모델 셀렉션
- from sklearn.model_selection import cross_val_score #스코어는 성능 알려주는거였ㅇ

30. #### estimator: 알고리즘 인스턴스화 하는 애
- 이제 쪼갠거말고 전체 데이터 다 넣어봐요
- 크로스에서는 5조각내는거 파이브..이고 관례상 템폴드 10개 써요
- cross_val_score(KNeighborsClassifier(), iris.iloc[:,:-1], iris.iloc[:,-1], cv=10, n_jobs=-2) #10 개 쪼개서 모델 만들어 #n잡슨는 내 코어? 

31. #### estimator: 알고리즘 인스턴스화 하는 애
- 이제 쪼갠거말고 전체 데이터 다 넣어봐요
- 크로스에서는 5조각내는거 파이브..이고 관례상 템폴드 10개 써요

32. 이 모델의 평균
- import numpy as np
- # 이 모델의 평균
np.mean(cross_val_score(KNeighborsClassifier(), iris.iloc[:,:-1], iris.iloc[:,-1], cv=10, n_jobs=-2)) #10 개 쪼개서 모델 만들어 #n잡슨는 내 코어? 

33. ## 에스브이엠
- svm 커널 트릭해서 차원을 증가시켜버려. 그럼 자를 수가 있겠죠. 그렇게 해서 판별 시키기도 함. 
- 수학적으로 미치는데 이뻐
- 수학적 관점이라서 깔끔하고 좋은데 속도가 좀 느려
    - 학습 및 예측 둘다 느려서 실무에 쓰기는 어려워요
- from sklearn.svm import SVC
- %time np.mean(cross_val_score(SVC(), iris.iloc[:,:-1], iris.iloc[:,-1], cv=10, n_jobs=-2)) 

#### 크로스벨리데이션 이용해서
- 데이터가 많으면 트레이닝 데이터 스플릿, 작으면 크로스 벨리데이션

34.#### 그리드 어떻게 쓰냐
- from sklearn.model_selection import GridSearchCV

35. - 이거 인스턴스 하면 에러나. 첫번째에 estimatie-알고리즘 인스턴스 넣어야해
- 파람 그리드: 점점.해서 그룹 만들어줘
        - knn은 하이퍼 파라미터(엔네이버즈가 파라미터_
        - 딕셔너리 형태로 작성
- param_grid={'n_neighbors':[1,2,3,4,5,6,7,8,9]}
- grid=GridSearchCV(KNeighborsClassifier(),param_grid) #이거 인스턴스 하면 에러나. 첫번째에 estimatie-알고리즘 인스턴스 넣어야해

36. ### 크로스 벨리데이션 하니까 전체로 핏해볼게
- grid.fit(iris.iloc[:,:-1], iris.iloc[:,-1])
- #### best index, best param 등등 나와요
- grid.best_params_
>> {'n_neighbors': 5}
- grid.best_score_
>>0.9866666666666667

---------------------
37. #### 파람 그리드 여러개 쓸 수 있어
- 하이퍼 파라미터 찾기 쉽다 @!!!@@@@@@@@@@@@@@@
- param_grid={'n_neighbors':[1,2,3,4,5,6,7,8,9]}



-------------
### 파이프라인으로 자동화하면 좋아요

## 그리드 정말 다 좋은데, 전처리가 그렇게 좋지 않을 거야
- 우리 데이터 안 좋을 이유: 웹에서 수집하면 문자라서 숫자로 전처리 해야할거에요. 
    - 도메인 별로 데이터를 숫자화 하느것 (벡터화라이제이션_교수님 인코딩이라 부르실듯)
    - 인코딩
        - 라벨 인코딩
        - 워.. 인코딩?

## 라벨 인코딩) 문자를 숫자로 바꾸는거
- import seaborn as sns
- iris=sns.load_dataset('iris')
- iris
- iris.species.map({'setosa':0, 'virginicia':1, 'versicolor':2})

## 문자열을 0,1,2 처럼 숫자값으로 바꿔주는 이러한 방식을 label encoding 이라 한다.
# 숫자값을 바꿔준다는 것은 즉 각각이 숫자 크기 비교가 가능해지므로 X 에는 이를 적용하면 안된다. y(target)에만 적용하자
# label encoding은 categorical인 target에 사용하기에 적합하다.

# map에서는 dictionary 또는 함수를 사용할 수 있다.

## 라벨 인코딩 조심
- 학습될 데이터에 들어가면 문제 발생 가능
- 라벨 인코딩은 숫자가 0,1,2로 넣으면 그 애가 우리는 숫자가 아닌 종류의 데이터인데.
- 크기로 바뀌어서 큰수가 영향력을 미치는 경우 발생가능
- 라벨 인코딩은 엑스에 있는 데이터 라벨링하는건 바보짓이야
- 대신 타겟에 가능. 단, 엑스에 쓰면 아주아주 조심해야해



## 맵은 내부 딕셔너리/함수 이용 가능
- 그래서 아마 맵으로도 전처리 가능할거에요

# r그리드 서치 시브이) 포 안해도 다 찾아서 좋아
- # cross_validation : '데이터가 작을 때 사용'
- from sklearn.model_selection import GridSearchCV # CV -> cross validation 방식을 사용한다는 의미
- param_grid = {'n_neighbors': [1,2,3,4,5,6,7,8,9]}
- grid = GridSearchCV(KNeighborsClassifier(), param_grid)
// param_grid : hyper parameter를 gridsearch 내부에서 어떤 값을 사용할 건지 dictionary 형태로 전달하여 정해줄 수 있다.
- grid.fit(iris.iloc[:,:-1], iris.iloc[:,-1])
- grid.best_params_
>>n_neighbors': 5}
- grid.best_score_
>>0.9866666666666667
- KNeighborsClassifier
>>klearn.neighbors.classification.KNeighborsClassifier

--
## 라벨 인코딩 다른 방법
- 사이킷에서
-from sklearn.preprocessing import LabelEncoder
- le=LabelEncoder()
- le.fit(iris.species) #이거 조금 혼동될 수 있어
- le.transform(iris.species)
###### 근데 인자가 다 같으니까 단축 포현으로
- le.fit_transform(iris.species)
>>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])

----
- #### 이제 이거 다시 백해서 판다스에 넣어야해
- 교수님은 라벨 인코딩을 판다스로 하셔
- 근덴 판다스보다 사이킷이 더 쉬움
    - 임포트-인스턴스-핏-프레딕트/트렌스폼. 이거 4개만 하면 돼

## 라벨 인코등
- 문자를 수자로 변경
- 1. 라벨인코딩
- 한번에 처리하는거 임배딩 방식
- 첫번쨰 옵션
    - 우리는 한 컬럼당 문자가 하나 들어갈때 카테고리컬을 할떄 라벨링을 합니다
    - 주의)학습용노. 
- 2.원한 인코딩(더미인코딩)

import pandas as pd
pd.get_dummies(iris.species)
----
## 원핫 인코딩
-투디 뭐시기 넣어야해
- 스펙시스는 일차원 시리즈

-from sklearn.preprocessing import OneHotEncoder
- ohe=OneHotEncoder()
- ohe.fit(iris.species)
-----
### 차원 2차원 만드는거- 팬시 인댁싱
- ohe.fit(iris[['species']])
>> OneHotEncoder(categorical_features=None, categories=None,
       dtype=<class 'numpy.float64'>, handle_unknown='error',
       n_values=None, sparse=True)
### 바꾸는거 트랜스폼
- 
ohe.fit_transform([iris['species']])
>><1x150 sparse matrix of type '<class 'numpy.float64'>'
	with 150 stored elements in Compressed Sparse Row format>

- ### 사이킷은 변환을 다시 원래로 돌리게도 해줌
- ohe.inverse_transform([[1,0.,0.]])
- le.inverse_transform([2])

-----------------
## 딥러닝 기반은 노말라이제이션해야함
- 사이킷에서는 이게 있어요.

from sklearn.preprocessing import MinMaxScaler, RobustScaler

### 아웃라이어있을떄 뭐 사용해요?
- RobustSclaer
    - 아웃라이어가 있을 경우에

#### 민맥스 스케일러가 뭔지 보여줄게요
- 예측모델은 스켈링하는것
- 오히려 좋아지는경우가 많을거에요
- 스탠다드스케일러는 뭐? ㅏㄴ글로 표준화. 표준화는 뭐하는 거였다. 
    - 표준화. (x-m)/표준편차
        - 이렇게 되면 모델이 더 예쁘게 될 수도 있어.
        - 그래서 아예 이거부터 다 해놓고 시작하는 사람도 있어

#### 예측에 특성은 안 바뀌죠
- 타겟은 뭘 안한다? 스케일링 안하고 학습할 거만 스케일링 ㅎ아면 문제 없어
- 리그레이션 할때도 스케일링 해야해요
- 가장 간단한전처리
- 그래서 같은 모델을 스케일링과 안 했을때 비교를 해봐야해ㅛ

### 도메인 지식 필요
- 아ㅣ무때나 스케일링하면 문제 생겨
- 아이리쉬는 자연현상이야. 그래서이 자연현상에 대해서 잘 모르고 바꾸면 안돼. 
- 즉 너희가 데이터 수집하고 나서 각 도메인에 대한 어트리뷰트별 수집을 다 해야해
    - 보고서 낼떄 그거에 대한 거 내야해
    - 근데 일반적인 상식은 할 필요 없지


#### 사이킷은 지원하는 것만 가능, 판다스는 내 맘대로 가능
- 내가 도메인 지식은 언ㅄ고 세팔 랭쓰의 크기 높이고 싶어
- 맵과 어플라이로 값 조금씩 변경 가능★
- iris.sepal_length.map(lambda x:x+1)

#### 예측할때도 같이 변화 시켜줘야해 ★

# 그래프 : 각각 보는 이유가 다 있어요
- 페어플랏
- 박스플랏
- 히트맵

# 그래프 : 각각 보는 이유가 다 있어요
- 페어플랏
- 박스플랏
- 히트맵

#### 최적 이전에 학습이 잘 되었는지 알 수 있따?
- 가정 자체
- 알 수 있어.
- 어떻게 쓰냐면 from sklearn.model_selection import learning_curve 
    - 학습에 따라 곡선을 통해 상황 파악 가능
	- from sklearn.model_selection import learning_curve
	- !pip install sklearn-evaluation
	### 설치할떄는 -, 사용할때는 _
	- import sklearn_evaluation


#### 트레이닝 스코어는 갈 수록 좋아져
- 그러나 이게 좋다는건 거꾸로 학습이 어느정도 많이 되었다는 것
- 러닝커브는 즉 데이터가 어느정도 필요할지 여부 체크하려고 쓰는거고, 밸리데이션은 트레이닝과 테스트 에러가 어떻게 파라미터에 따라 변하는지 보는 것. 즉 그리드 씨브이와 같이 보는 것

## Confusion_matrix
- 실제 n개 중에 몇 개 맞췄는데 비쥬얼라이제이션하는거
#### 사이킷런 이벨류에이션을 쓰면
- 얘는 히트맵을 사용해서 그림

